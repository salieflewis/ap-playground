import { Hex, Hash, Address, AccessList, PublicClient, RpcLog, RpcBlock, RpcTransaction, GetContractReturnType } from 'viem';
import { AbiEvent, Address as Address$1, Abi, ExtractAbiFunctionNames } from 'abitype';
import { LevelWithSilent } from 'pino';
import Emittery from 'emittery';
import { GraphQLObjectType, TypeNode, GraphQLScalarType, GraphQLEnumType, GraphQLInputObjectType, GraphQLSchema } from 'graphql';
import { Kysely, Migrator } from 'kysely';
import prometheus from 'prom-client';
import express from 'express';

type ResolvedConfig = {
    /** Database to use for storing blockchain & entity data. Default: `"postgres"` if `DATABASE_URL` env var is present, otherwise `"sqlite"`. */
    database?: {
        kind: "sqlite";
        /** Path to SQLite database file. Default: `"./.ponder/cache.db"`. */
        filename?: string;
    } | {
        kind: "postgres";
        /** PostgreSQL database connection string. Default: `process.env.DATABASE_URL`. */
        connectionString?: string;
    };
    /** List of blockchain networks. */
    networks: {
        /** Network name. Must be unique across all networks. */
        name: string;
        /** Chain ID of the network. */
        chainId: number;
        /** RPC URL. Default: if available, a public RPC provider. */
        rpcUrl?: string;
        /** Polling frequency (in ms). Default: `1_000`. */
        pollingInterval?: number;
        /** Maximum concurrency of RPC requests during the historical sync. Default: `10`. */
        maxRpcRequestConcurrency?: number;
    }[];
    /** List of contracts to fetch & handle events from. Contracts defined here will be present in `context.contracts`. */
    contracts?: {
        /** Contract name. Must be unique across `contracts` and `filters`. */
        name: string;
        /** Network that this contract is deployed to. Must match a network name in `networks`. */
        network: string;
        /** Contract ABI as a file path or an Array object. Accepts a single ABI or a list of ABIs to be merged. */
        abi: string | any[] | readonly any[] | (string | any[] | readonly any[])[];
        /** Contract address. */
        address: `0x${string}`;
        /** Block number at which to start processing events (inclusive). Default: `0`. */
        startBlock?: number;
        /** Block number at which to stop processing events (inclusive). If `undefined`, events will be processed in real-time. Default: `undefined`. */
        endBlock?: number;
        /** Maximum block range to use when calling `eth_getLogs`. Default: `10_000`. */
        maxBlockRange?: number;
        /** Whether to fetch & process event logs for this contract. If `false`, this contract will still be present in `context.contracts`. Default: `true`. */
        isLogEventSource?: boolean;
    }[];
    /** List of log filters from which to fetch & handle event logs. */
    filters?: {
        /** Filter name. Must be unique across `contracts` and `filters`. */
        name: string;
        /** Network that this filter is deployed to. Must match a network name in `networks`. */
        network: string;
        /** Log filter ABI as a file path or an Array object. Accepts a single ABI or a list of ABIs to be merged. */
        abi: string | any[] | readonly any[] | (string | any[] | readonly any[])[];
        /** Log filter options. */
        filter: {
            /** Contract addresses to include. If `undefined`, no filter will be applied. Default: `undefined`. */
            address?: `0x${string}` | `0x${string}`[];
        } & ({
            /** Event signature to include. If `undefined`, no filter will be applied. Default: `undefined`. */
            event?: AbiEvent;
            /** Event arguments to include. If `undefined`, no filter will be applied. Default: `undefined`. */
            args?: any[];
        } | {
            event?: never;
            args?: never;
        });
        /** Block number at which to start processing events (inclusive). Default: `0`. */
        startBlock?: number;
        /** Block number at which to stop processing events (inclusive). If `undefined`, events will be processed in real-time. Default: `undefined`. */
        endBlock?: number;
        /** Maximum block range to use when calling `eth_getLogs`. Default: `10_000`. */
        maxBlockRange?: number;
    }[];
    /** Configuration for Ponder internals. */
    options?: {
        /** Maximum number of seconds to wait for event processing to be complete before responding as healthy. If event processing exceeds this duration, the API may serve incomplete data. Default: `240` (4 minutes). */
        maxHealthcheckDuration?: number;
    };
};
type Config = ResolvedConfig | Promise<ResolvedConfig> | (() => ResolvedConfig) | (() => Promise<ResolvedConfig>);

type Options = {
    configFile: string;
    schemaFile: string;
    rootDir: string;
    srcDir: string;
    generatedDir: string;
    ponderDir: string;
    logDir: string;
    port: number;
    maxHealthcheckDuration: number;
    logLevel: LevelWithSilent;
    uiEnabled: boolean;
};

/**
 * A confirmed Ethereum block.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/introduction-to-smart-contracts.html#blocks
 */
type Block = {
    /** Base fee per gas */
    baseFeePerGas: bigint | null;
    /** Difficulty for this block */
    difficulty: bigint;
    /** "Extra data" field of this block */
    extraData: Hex;
    /** Maximum gas allowed in this block */
    gasLimit: bigint;
    /** Total used gas by all transactions in this block */
    gasUsed: bigint;
    /** Block hash */
    hash: Hash;
    /** Logs bloom filter */
    logsBloom: Hex;
    /** Address that received this block’s mining rewards */
    miner: Address;
    /** Unique identifier for the block. */
    mixHash: Hash;
    /** Proof-of-work hash */
    nonce: Hex;
    /** Block number */
    number: bigint;
    /** Parent block hash */
    parentHash: Hash;
    /** Root of the this block’s receipts trie */
    receiptsRoot: Hex;
    /** SHA3 of the uncles data in this block */
    sha3Uncles: Hash;
    /** Size of this block in bytes */
    size: bigint;
    /** Root of this block’s final state trie */
    stateRoot: Hash;
    /** Unix timestamp of when this block was collated */
    timestamp: bigint;
    /** Total difficulty of the chain until this block */
    totalDifficulty: bigint;
    /** Root of this block’s transaction trie */
    transactionsRoot: Hash;
};

/**
 * A confirmed Ethereum log.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/abi-spec.html#events
 */
type Log = {
    /** Globally unique identifier for this log. */
    id: string;
    /** The address from which this log originated */
    address: Address;
    /** Hash of block containing this log */
    blockHash: Hash;
    /** Number of block containing this log */
    blockNumber: bigint;
    /** Contains the non-indexed arguments of the log */
    data: Hex;
    /** Index of this log within its block */
    logIndex: number;
    /** `true` if this log has been removed in a chain reorganization */
    removed: boolean;
    /** List of order-dependent topics */
    topics: [Hex, ...Hex[]] | [];
    /** Hash of the transaction that created this log */
    transactionHash: Hash;
    /** Index of the transaction that created this log */
    transactionIndex: number;
};

/**
 * @description Combines members of an intersection into a readable type.
 *
 * @link https://twitter.com/mattpocockuk/status/1622730173446557697?s=20&t=NdpAcmEFXY01xkqU3KO0Mg
 * @example
 * Prettify<{ a: string } | { b: string } | { c: number, d: bigint }>
 * => { a: string, b: string, c: number, d: bigint }
 */
type Prettify<T> = {
    [K in keyof T]: T[K];
} & {};

/**
 * A confirmed Ethereum transaction. Contains `legacy`, `EIP-1559`, or `EIP-2930` fee values depending on the transaction `type`.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/introduction-to-smart-contracts.html#transactions
 */
type Transaction = Prettify<{
    /** Hash of block containing this transaction */
    blockHash: Hash;
    /** Number of block containing this transaction */
    blockNumber: bigint;
    /** Transaction sender */
    from: Address;
    /** Gas provided for transaction execution */
    gas: bigint;
    /** Hash of this transaction */
    hash: Hash;
    /** Contract code or a hashed method call */
    input: Hex;
    /** Unique number identifying this transaction */
    nonce: number;
    /** ECDSA signature r */
    r: Hex;
    /** ECDSA signature s */
    s: Hex;
    /** Transaction recipient or `null` if deploying a contract */
    to: Address | null;
    /** Index of this transaction in the block */
    transactionIndex: number;
    /** ECDSA recovery ID */
    v: bigint;
    /** Value in wei sent with this transaction */
    value: bigint;
} & ({
    /** Transaction type. */
    type: "legacy";
    accessList?: never;
    /** Base fee per gas. Only present in legacy and EIP-2930 transactions. */
    gasPrice: bigint;
    maxFeePerGas?: never;
    maxPriorityFeePerGas?: never;
} | {
    /** Transaction type. */
    type: "eip2930";
    /** List of addresses and storage keys the transaction will access. */
    accessList: AccessList;
    /** Base fee per gas. Only present in legacy and EIP-2930 transactions. */
    gasPrice: bigint;
    maxFeePerGas?: never;
    maxPriorityFeePerGas?: never;
} | {
    /** Transaction type. */
    type: "eip1559";
    accessList?: never;
    gasPrice?: never;
    /** Total fee per gas in wei (gasPrice/baseFeePerGas + maxPriorityFeePerGas). Only present in EIP-1559 transactions. */
    maxFeePerGas: bigint;
    /** Max priority fee per gas (in wei). Only present in EIP-1559 transactions. */
    maxPriorityFeePerGas: bigint;
} | {
    /** Transaction type. */
    type: "deposit";
    accessList?: never;
    gasPrice?: never;
    /** Total fee per gas in wei (gasPrice/baseFeePerGas + maxPriorityFeePerGas). Only present in EIP-1559 transactions. */
    maxFeePerGas: bigint;
    /** Max priority fee per gas (in wei). Only present in EIP-1559 transactions. */
    maxPriorityFeePerGas: bigint;
} | {
    /** Transaction type. */
    type: Hex;
    gasPrice?: never;
    accessList?: never;
    maxFeePerGas?: never;
    maxPriorityFeePerGas?: never;
})>;

interface LogEvent$1 {
    name: string;
    params: Record<string, any>;
    log: Log;
    block: Block;
    transaction: Transaction;
}
type SetupEventHandler = ({ context, }: {
    context: unknown;
}) => Promise<void> | void;
type LogEventHandler = ({ event, context, }: {
    event: LogEvent$1;
    context: unknown;
}) => Promise<void> | void;
type LogEventHandlers = Record<string, LogEventHandler | undefined>;
type Handlers = Record<string, LogEventHandlers | undefined> & {
    setup?: SetupEventHandler;
};
declare class PonderApp<EventHandlers = Record<string, LogEventHandler>> {
    private handlers;
    private errors;
    on<EventName extends Extract<keyof EventHandlers, string>>(name: EventName, handler: EventHandlers[EventName]): void;
}

type ScalarTypeName = "Boolean" | "Int" | "String" | "BigInt" | "Bytes" | "Float";
type ScalarField = {
    name: string;
    kind: "SCALAR";
    notNull: boolean;
    originalFieldType: TypeNode;
    scalarTypeName: ScalarTypeName;
    scalarGqlType: GraphQLScalarType;
};
type EnumField = {
    name: string;
    kind: "ENUM";
    notNull: boolean;
    originalFieldType: TypeNode;
    enumGqlType: GraphQLEnumType;
    enumValues: string[];
};
type ListField = {
    name: string;
    kind: "LIST";
    baseGqlType: GraphQLScalarType | GraphQLEnumType;
    originalFieldType: TypeNode;
    notNull: boolean;
    isListElementNotNull: boolean;
};
type RelationshipField = {
    name: string;
    kind: "RELATIONSHIP";
    baseGqlType: GraphQLInputObjectType;
    originalFieldType: TypeNode;
    notNull: boolean;
    relatedEntityName: string;
    relatedEntityIdType: GraphQLScalarType & {
        name: ScalarTypeName;
    };
};
type DerivedField = {
    name: string;
    kind: "DERIVED";
    baseGqlType: GraphQLInputObjectType;
    originalFieldType: TypeNode;
    notNull: boolean;
    derivedFromEntityName: string;
    derivedFromFieldName: string;
};
type Field = ScalarField | EnumField | ListField | RelationshipField | DerivedField;
type Entity = {
    name: string;
    gqlType: GraphQLObjectType;
    isImmutable: boolean;
    fields: Field[];
    fieldByName: {
        id: ScalarField;
    } & Record<string, Field>;
};
type Schema = {
    entities: Entity[];
};

type BuildServiceEvents = {
    newConfig: undefined;
    newHandlers: {
        handlers: Handlers;
    };
    newSchema: {
        schema: Schema;
        graphqlSchema: GraphQLSchema;
    };
};
declare class BuildService extends Emittery<BuildServiceEvents> {
    private resources;
    private closeWatcher?;
    private latestFileHashes;
    constructor({ resources }: {
        resources: Resources;
    });
    kill(): Promise<void>;
    watch(): void;
    buildHandlers(): Promise<void>;
    buildSchema(): {
        schema: Schema;
        graphqlSchema: GraphQLSchema;
    } | undefined;
    private isFileChanged;
}

type Network = {
    name: string;
    chainId: number;
    client: PublicClient;
    rpcUrl?: string;
    pollingInterval: number;
    defaultMaxBlockRange: number;
    maxRpcRequestConcurrency: number;
    finalityBlockCount: number;
};

type Contract = {
    name: string;
    address: Address$1;
    network: Network;
    abi: Abi;
};

type LogFilter = {
    name: string;
    abi: Abi;
    network: string;
    filter: {
        key: string;
        chainId: number;
        address?: `0x${string}` | `0x${string}`[];
        topics?: (`0x${string}` | `0x${string}`[] | null)[];
        startBlock: number;
        endBlock?: number;
    };
    maxBlockRange?: number;
};

declare class CodegenService extends Emittery {
    private resources;
    private contracts;
    private logFilters;
    constructor({ resources, contracts, logFilters, }: {
        resources: Resources;
        contracts: Contract[];
        logFilters: LogFilter[];
    });
    generateAppFile({ schema }?: {
        schema?: Schema;
    }): void;
    generateSchemaFile({ graphqlSchema }: {
        graphqlSchema: GraphQLSchema;
    }): void;
}

declare class UserError extends Error {
    name: string;
    meta?: string;
    constructor(message: string, options?: {
        stack?: string;
        meta?: string;
        cause?: unknown;
    });
}

type UserErrorEvents = {
    error: {
        error: UserError;
    };
};
declare class UserErrorService extends Emittery<UserErrorEvents> {
    hasUserError: boolean;
    submitUserError({ error }: {
        error: UserError;
    }): void;
}

/**
 * A record representing a range of blocks that have been added
 * to the event store for a given log filter.
 */
type LogFilterCachedRange = {
    filterKey: string;
    startBlock: bigint;
    endBlock: bigint;
    endBlockTimestamp: bigint;
};
/**
 * A record representing a call to a contract made at a specific block height.
 */
type ContractReadResult = {
    address: string;
    blockNumber: bigint;
    chainId: number;
    data: Hex;
    finalized: boolean;
    result: Hex;
};
interface EventStore {
    db: Kysely<any>;
    migrator: Migrator;
    migrateUp(): Promise<void>;
    migrateDown(): Promise<void>;
    insertFinalizedLogs(options: {
        chainId: number;
        logs: RpcLog[];
    }): Promise<void>;
    insertFinalizedBlock(options: {
        chainId: number;
        block: RpcBlock;
        transactions: RpcTransaction[];
        logFilterRange: {
            logFilterKey: string;
            blockNumberToCacheFrom: number;
        };
    }): Promise<void>;
    mergeLogFilterCachedRanges(options: {
        logFilterKey: string;
        logFilterStartBlockNumber: number;
    }): Promise<{
        startingRangeEndTimestamp: number;
    }>;
    getLogFilterCachedRanges(options: {
        filterKey: string;
    }): Promise<LogFilterCachedRange[]>;
    insertUnfinalizedBlock(options: {
        chainId: number;
        block: RpcBlock;
        transactions: RpcTransaction[];
        logs: RpcLog[];
    }): Promise<void>;
    deleteUnfinalizedData(options: {
        chainId: number;
        fromBlockNumber: number;
    }): Promise<void>;
    finalizeData(options: {
        chainId: number;
        toBlockNumber: number;
    }): Promise<void>;
    insertContractReadResult(options: {
        address: string;
        blockNumber: bigint;
        chainId: number;
        data: Hex;
        finalized: boolean;
        result: Hex;
    }): Promise<void>;
    getContractReadResult(options: {
        address: string;
        blockNumber: bigint;
        chainId: number;
        data: Hex;
    }): Promise<ContractReadResult | null>;
    getLogEvents(arg: {
        fromTimestamp: number;
        toTimestamp: number;
        filters?: {
            name: string;
            chainId: number;
            address?: Address | Address[];
            topics?: (Hex | Hex[] | null)[];
            fromBlock?: number;
            toBlock?: number;
            handledTopic0?: Hex[];
        }[];
    }): Promise<{
        totalEventCount: number;
        events: {
            filterName: string;
            log: Log;
            block: Block;
            transaction: Transaction;
        }[];
    }>;
}

type LogEvent = {
    logFilterName: string;
    eventName: string;
    params: Record<string, unknown>;
    log: Log;
    block: Block;
    transaction: Transaction;
};
type EventAggregatorEvents = {
    /**
     * Emitted when a new event checkpoint is reached. This is the minimum timestamp
     * at which events are available across all registered networks.
     */
    newCheckpoint: {
        timestamp: number;
    };
    /**
     * Emitted when a new finality checkpoint is reached. This is the minimum timestamp
     * at which events are finalized across all registered networks.
     */
    newFinalityCheckpoint: {
        timestamp: number;
    };
    /**
     * Emitted when a reorg has been detected on any registered network.
     */
    reorg: {
        commonAncestorTimestamp: number;
    };
};
type EventAggregatorMetrics = {};
declare class EventAggregatorService extends Emittery<EventAggregatorEvents> {
    private eventStore;
    private logFilters;
    private networks;
    checkpoint: number;
    finalityCheckpoint: number;
    historicalSyncCompletedAt?: number;
    private networkCheckpoints;
    metrics: EventAggregatorMetrics;
    constructor({ eventStore, networks, logFilters, }: {
        eventStore: EventStore;
        networks: Network[];
        logFilters: LogFilter[];
    });
    /** Fetches events for all registered log filters between the specified timestamps.
     *
     * @param options.fromTimestamp Timestamp to start including events (inclusive).
     * @param options.toTimestamp Timestamp to stop including events (inclusive).
     * @param options.handledLogFilters Subset of log filters that the user has provided a handler for.
     * @returns A promise resolving to an array of log events.
     */
    getEvents: ({ fromTimestamp, toTimestamp, handledLogFilters, }: {
        fromTimestamp: number;
        toTimestamp: number;
        handledLogFilters: Record<string, {
            eventName: string;
            topic0: Hex;
            abiItem: AbiEvent;
        }[]>;
    }) => Promise<{
        totalEventCount: number;
        events: LogEvent[];
    }>;
    handleNewHistoricalCheckpoint: ({ chainId, timestamp, }: {
        chainId: number;
        timestamp: number;
    }) => void;
    handleHistoricalSyncComplete: ({ chainId }: {
        chainId: number;
    }) => void;
    handleNewRealtimeCheckpoint: ({ chainId, timestamp, }: {
        chainId: number;
        timestamp: number;
    }) => void;
    handleNewFinalityCheckpoint: ({ chainId, timestamp, }: {
        chainId: number;
        timestamp: number;
    }) => void;
    handleReorg: ({ commonAncestorTimestamp, }: {
        commonAncestorTimestamp: number;
    }) => void;
    private recalculateCheckpoint;
    private recalculateFinalityCheckpoint;
}

type HistoricalSyncEvents = {
    /**
     * Emitted when the service has finished processing all historical sync tasks.
     */
    syncComplete: undefined;
    /**
     * Emitted when the minimum cached timestamp among all registered log filters moves forward.
     * This indicates to consumers that the connected event store now contains a complete history
     * of events for all registered log filters between their start block and this timestamp (inclusive).
     */
    historicalCheckpoint: {
        timestamp: number;
    };
    /**
     * Emitted when a historical sync task fails.
     */
    error: {
        error: Error;
    };
};
declare class HistoricalSyncService extends Emittery<HistoricalSyncEvents> {
    private resources;
    private eventStore;
    private logFilters;
    private network;
    private queue;
    private logFilterCheckpoints;
    private minimumLogFilterCheckpoint;
    private startTimestamp?;
    private killFunctions;
    constructor({ resources, eventStore, logFilters, network, }: {
        resources: Resources;
        eventStore: EventStore;
        logFilters: LogFilter[];
        network: Network;
    });
    setup({ finalizedBlockNumber }: {
        finalizedBlockNumber: number;
    }): Promise<void>;
    start(): void;
    kill: () => Promise<void>;
    onIdle: () => Promise<void>;
    private buildQueue;
    private logTaskWorker;
    private blockTaskWorker;
    private updateHistoricalCheckpoint;
    private getCompletionStats;
    private registerMetricCollectMethods;
}

type LogOptions = {
    msg?: string;
    service?: string;
} & {
    [key: string]: any;
};
declare class LoggerService {
    private logger;
    constructor({ level, dir, }?: {
        level?: LevelWithSilent;
        dir?: string;
    });
    fatal: (options: LogOptions & {
        error?: Error;
    }) => void;
    error: (options: LogOptions & {
        error: Error;
    }) => void;
    warn: (options: LogOptions & {
        msg: string;
    }) => void;
    info: (options: LogOptions & {
        msg: string;
    }) => void;
    debug: (options: LogOptions & {
        msg: string;
    }) => void;
    trace: (options: LogOptions & {
        msg: string;
    }) => void;
}

declare class MetricsService {
    private registry;
    ponder_historical_scheduled_tasks: prometheus.Counter<"network" | "kind">;
    ponder_historical_completed_tasks: prometheus.Counter<"network" | "kind" | "status">;
    ponder_historical_rpc_request_duration: prometheus.Histogram<"network" | "method">;
    ponder_historical_total_blocks: prometheus.Gauge<"network" | "logFilter">;
    ponder_historical_cached_blocks: prometheus.Gauge<"network" | "logFilter">;
    ponder_historical_completed_blocks: prometheus.Gauge<"network" | "logFilter">;
    ponder_historical_completion_rate: prometheus.Gauge<"network" | "logFilter">;
    ponder_historical_completion_eta: prometheus.Gauge<"network" | "logFilter">;
    ponder_realtime_is_connected: prometheus.Gauge<"network">;
    ponder_realtime_latest_block_number: prometheus.Gauge<"network">;
    ponder_realtime_latest_block_timestamp: prometheus.Gauge<"network">;
    ponder_realtime_rpc_request_duration: prometheus.Histogram<"network" | "method">;
    ponder_handlers_matched_events: prometheus.Gauge<"eventName">;
    ponder_handlers_handled_events: prometheus.Gauge<"eventName">;
    ponder_handlers_processed_events: prometheus.Gauge<"eventName">;
    ponder_handlers_has_error: prometheus.Gauge;
    ponder_handlers_latest_processed_timestamp: prometheus.Gauge;
    ponder_server_port: prometheus.Gauge;
    ponder_server_request_size: prometheus.Histogram<"method" | "path" | "status">;
    ponder_server_response_size: prometheus.Histogram<"method" | "path" | "status">;
    ponder_server_response_duration: prometheus.Histogram<"method" | "path" | "status">;
    constructor();
    /**
     * Get string representation for all metrics.
     * @returns Metrics encoded using Prometheus v0.0.4 format.
     */
    getMetrics(): Promise<string>;
}

type RealtimeSyncEvents = {
    realtimeCheckpoint: {
        timestamp: number;
    };
    finalityCheckpoint: {
        timestamp: number;
    };
    shallowReorg: {
        commonAncestorTimestamp: number;
    };
    deepReorg: {
        detectedAtBlockNumber: number;
        minimumDepth: number;
    };
    error: {
        error: Error;
    };
};
type RealtimeSyncStats = {
    blocks: Record<number, {
        bloom: {
            hit: boolean;
            falsePositive: boolean;
        };
        matchedLogCount: number;
    }>;
};
declare class RealtimeSyncService extends Emittery<RealtimeSyncEvents> {
    private resources;
    private eventStore;
    private logFilters;
    private network;
    stats: RealtimeSyncStats;
    private queue;
    private finalizedBlockNumber;
    private blocks;
    private unpoll?;
    constructor({ resources, eventStore, logFilters, network, }: {
        resources: Resources;
        eventStore: EventStore;
        logFilters: LogFilter[];
        network: Network;
    });
    setup: () => Promise<{
        finalizedBlockNumber: number;
    }>;
    start: () => Promise<void>;
    kill: () => Promise<void>;
    onIdle: () => Promise<void>;
    private getLatestBlock;
    addNewLatestBlock: () => Promise<void>;
    private buildQueue;
    private blockTaskWorker;
}

type WhereFieldValue = number | string | number[] | string[] | true | false | undefined | null;
type ModelFilter = {
    where?: {
        [key: `${string}_${FilterType}`]: WhereFieldValue;
    };
    first?: number;
    skip?: number;
    orderBy?: string;
    orderDirection?: "asc" | "desc";
};
type ModelInstance = {
    id: string | number | bigint;
    [key: string]: string | bigint | number | boolean | (string | bigint | number | boolean)[] | null;
};
interface UserStore {
    schema?: Schema;
    versionId?: string;
    reload(options?: {
        schema?: Schema;
    }): Promise<void>;
    teardown(): Promise<void>;
    revert(options: {
        safeTimestamp: number;
    }): Promise<void>;
    findUnique(options: {
        modelName: string;
        timestamp?: number;
        id: string | number | bigint;
    }): Promise<ModelInstance | null>;
    findMany(options: {
        modelName: string;
        timestamp?: number;
        filter?: ModelFilter;
    }): Promise<ModelInstance[]>;
    create(options: {
        modelName: string;
        timestamp: number;
        id: string | number | bigint;
        data?: Omit<ModelInstance, "id">;
    }): Promise<ModelInstance>;
    update(options: {
        modelName: string;
        timestamp: number;
        id: string | number | bigint;
        data: Partial<Omit<ModelInstance, "id">>;
    }): Promise<ModelInstance>;
    upsert(options: {
        modelName: string;
        timestamp: number;
        id: string | number | bigint;
        create?: Omit<ModelInstance, "id">;
        update?: Partial<Omit<ModelInstance, "id">>;
    }): Promise<ModelInstance>;
    delete(options: {
        modelName: string;
        timestamp: number;
        id: string | number | bigint;
    }): Promise<boolean>;
}

declare class ServerService {
    private resources;
    private userStore;
    private port;
    app?: express.Express;
    private terminate?;
    private graphqlMiddleware?;
    isHistoricalEventProcessingComplete: boolean;
    constructor({ resources, userStore, }: {
        resources: Resources;
        userStore: UserStore;
    });
    start(): Promise<void>;
    reload({ graphqlSchema }: {
        graphqlSchema: GraphQLSchema;
    }): void;
    kill(): Promise<void>;
    setIsHistoricalEventProcessingComplete(): void;
}

type UiState = {
    port: number;
    historicalSyncLogFilterStats: Record<string, {
        rate: number;
        eta?: number;
    }>;
    isHistoricalSyncComplete: boolean;
    handlerError: boolean;
    handlersCurrent: number;
    handlersTotal: number;
    handlersHandledTotal: number;
    handlersToTimestamp: number;
    networks: string[];
};

declare class UiService {
    private resources;
    private logFilters;
    ui: UiState;
    renderInterval: NodeJS.Timer;
    render: () => void;
    unmount: () => void;
    constructor({ resources, logFilters, }: {
        resources: Resources;
        logFilters: LogFilter[];
    });
    kill(): void;
}

type EventHandlerEvents = {
    eventsProcessed: {
        toTimestamp: number;
    };
};
declare class EventHandlerService extends Emittery<EventHandlerEvents> {
    private resources;
    private userStore;
    private eventAggregatorService;
    private logFilters;
    private readOnlyContracts;
    private schema?;
    private models;
    private handlers?;
    private handledLogFilters;
    private eventProcessingMutex;
    private queue?;
    private eventsProcessedToTimestamp;
    private hasError;
    private currentEventBlockNumber;
    private currentEventTimestamp;
    constructor({ resources, eventStore, userStore, eventAggregatorService, contracts, logFilters, }: {
        resources: Resources;
        eventStore: EventStore;
        userStore: UserStore;
        eventAggregatorService: EventAggregatorService;
        contracts: Contract[];
        logFilters: LogFilter[];
    });
    kill: () => void;
    /**
     * Registers a new set of handler functions and/or a new schema, cancels
     * the current event processing mutex & event queue, drops and re-creates
     * all tables from the user store, and resets eventsProcessedToTimestamp to zero.
     *
     * Note: Caller should (probably) immediately call processEvents after this method.
     */
    reset: ({ handlers: newHandlers, schema: newSchema, }?: {
        handlers?: Handlers;
        schema?: Schema;
    }) => Promise<void>;
    /**
     * This method is triggered by the realtime sync service detecting a reorg,
     * which can happen at any time. The event queue and the user store can be
     * in one of several different states that we need to keep in mind:
     *
     * 1) No events have been added to the queue yet.
     * 2) No unsafe events have been processed (eventsProcessedToTimestamp <= commonAncestorTimestamp).
     * 3) Unsafe events may have been processed (eventsProcessedToTimestamp > commonAncestorTimestamp).
     * 4) The queue has encountered a user error and is waiting for a reload.
     *
     * Note: It's crucial that we acquire a mutex lock while handling the reorg.
     * This will only ever run while the queue is idle, so we can be confident
     * that eventsProcessedToTimestamp matches the current state of the user store,
     * and that no unsafe events will get processed after handling the reorg.
     *
     * Note: Caller should (probably) immediately call processEvents after this method.
     */
    handleReorg: ({ commonAncestorTimestamp, }: {
        commonAncestorTimestamp: number;
    }) => Promise<void>;
    /**
     * Processes all newly available events.
     *
     * Acquires a lock on the event processing mutex, then gets the latest checkpoint
     * from the event aggregator service. Fetches events between previous checkpoint
     * and the new checkpoint, adds them to the queue, then processes them.
     */
    processEvents: () => Promise<void>;
    private createEventQueue;
}

type Resources = {
    options: Options;
    logger: LoggerService;
    errors: UserErrorService;
    metrics: MetricsService;
};
declare class Ponder {
    resources: Resources;
    logFilters: LogFilter[];
    eventStore: EventStore;
    userStore: UserStore;
    networkSyncServices: {
        network: Network;
        logFilters: LogFilter[];
        historicalSyncService: HistoricalSyncService;
        realtimeSyncService: RealtimeSyncService;
    }[];
    eventAggregatorService: EventAggregatorService;
    eventHandlerService: EventHandlerService;
    serverService: ServerService;
    buildService: BuildService;
    codegenService: CodegenService;
    uiService: UiService;
    constructor({ options, config, eventStore, userStore, }: {
        options: Options;
        config: ResolvedConfig;
        eventStore?: EventStore;
        userStore?: UserStore;
    });
    setup(): Promise<Error | undefined>;
    dev(): Promise<void>;
    start(): Promise<void>;
    codegen(): Promise<void>;
    kill(): Promise<void>;
    private registerServiceDependencies;
}

type ReadOnlyContract<TAbi extends Abi = Abi, _ReadFunctionNames extends string = TAbi extends Abi ? Abi extends TAbi ? string : ExtractAbiFunctionNames<TAbi, "pure" | "view"> : string> = GetContractReturnType<TAbi, PublicClient, unknown, Address$1, never, _ReadFunctionNames, never>;

type Model<T extends {
    id: string | number | bigint;
}> = {
    create: (options: {
        id: T["id"];
        data: Omit<T, "id">;
    }) => Promise<T>;
    update: (options: {
        id: T["id"];
        data: Omit<Partial<T>, "id">;
    }) => Promise<T>;
    upsert: (options: {
        id: T["id"];
        create: Omit<T, "id">;
        update: Omit<Partial<T>, "id">;
    }) => Promise<T>;
    findUnique: (options: {
        id: T["id"];
    }) => Promise<T | null>;
    delete: (options: {
        id: T["id"];
    }) => Promise<boolean>;
};

export { Block, Config, Log, Model, Options, Ponder, PonderApp, ReadOnlyContract, ResolvedConfig, Transaction };
