"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var src_exports = {};
__export(src_exports, {
  Ponder: () => Ponder,
  PonderApp: () => PonderApp
});
module.exports = __toCommonJS(src_exports);

// src/build/handlers.ts
var import_esbuild = require("esbuild");
var import_glob = __toESM(require("glob"));
var import_node_fs = require("fs");
var import_node_path = __toESM(require("path"));
var import_tsc_alias = require("tsc-alias");
var PonderApp = class {
  handlers = {};
  errors = [];
  on(name, handler) {
    if (name === "setup") {
      this.handlers.setup = handler;
      return;
    }
    const [contractName, eventName] = name.split(":");
    if (!contractName || !eventName) {
      this.errors.push(new Error(`Invalid event name: ${name}`));
      return;
    }
    this.handlers[contractName] ||= {};
    if (this.handlers[contractName][eventName]) {
      this.errors.push(
        new Error(`Cannot add multiple handlers for event: ${name}`)
      );
      return;
    }
    this.handlers[contractName][eventName] = handler;
  }
};
var readHandlers = async ({ options }) => {
  const entryAppFilename = import_node_path.default.join(options.generatedDir, "index.ts");
  if (!(0, import_node_fs.existsSync)(entryAppFilename)) {
    throw new Error(
      `generated/index.ts file not found, expected: ${entryAppFilename}`
    );
  }
  const entryGlob = options.srcDir + "/**/*.ts";
  const entryFilenames = [...import_glob.default.sync(entryGlob), entryAppFilename];
  const buildDir = import_node_path.default.join(options.ponderDir, "out");
  (0, import_node_fs.rmSync)(buildDir, { recursive: true, force: true });
  try {
    await (0, import_esbuild.build)({
      entryPoints: entryFilenames,
      outdir: buildDir,
      platform: "node",
      bundle: false,
      format: "cjs",
      logLevel: "silent",
      sourcemap: "inline"
    });
  } catch (err) {
    const error = err;
    const stackTraces = (0, import_esbuild.formatMessagesSync)(error.errors, {
      kind: "error",
      color: true
    });
    error.stack = stackTraces.join("\n");
    throw error;
  }
  const tsconfigPath = import_node_path.default.join(options.rootDir, "tsconfig.json");
  if ((0, import_node_fs.existsSync)(tsconfigPath)) {
    await (0, import_tsc_alias.replaceTscAliasPaths)({
      configFile: tsconfigPath,
      outDir: buildDir
    });
  } else {
    throw new Error(
      `tsconfig.json not found, unable to resolve "@/*" path aliases. Expected at: ${tsconfigPath}`
    );
  }
  const outGlob = buildDir + "/**/*.js";
  const outFilenames = import_glob.default.sync(outGlob);
  outFilenames.forEach((file) => delete require.cache[require.resolve(file)]);
  const outAppFilename = import_node_path.default.join(buildDir, "generated/index.js");
  const outUserFilenames = outFilenames.filter(
    (name) => name !== outAppFilename
  );
  const requireErrors = outUserFilenames.map((file) => {
    try {
      require(file);
    } catch (err) {
      return err;
    }
  }).filter((err) => err !== void 0);
  if (requireErrors.length > 0) {
    throw requireErrors[0];
  }
  const result = require(outAppFilename);
  const app = result.ponder;
  if (!app) {
    throw new Error(`ponder not exported from generated/index.ts`);
  }
  if (!(app.constructor.name === "PonderApp")) {
    throw new Error(`exported ponder not instanceof PonderApp`);
  }
  if (app["errors"].length > 0) {
    const error = app["errors"][0];
    throw error;
  }
  const handlers = app["handlers"];
  return handlers;
};

// src/Ponder.ts
var import_node_path9 = __toESM(require("path"));

// src/build/service.ts
var import_chokidar = __toESM(require("chokidar"));
var import_emittery = __toESM(require("emittery"));
var import_node_crypto = require("crypto");
var import_node_fs3 = require("fs");
var import_node_path2 = __toESM(require("path"));

// src/errors/user.ts
var UserError = class extends Error {
  name = "UserError";
  meta;
  constructor(message, options = {}) {
    super(message, options.cause ? { cause: options.cause } : void 0);
    this.stack = options.stack;
    this.meta = options.meta;
  }
};

// src/schema/schema.ts
var import_graphql = require("graphql");
var GraphQLBigInt = new import_graphql.GraphQLScalarType({
  name: "BigInt",
  serialize: (value) => String(value),
  parseValue: (value) => BigInt(value),
  parseLiteral: (value) => {
    if (value.kind === "StringValue") {
      return BigInt(value.value);
    } else {
      throw new Error(
        `Invalid value kind provided for field of type BigInt: ${value.kind}. Expected: StringValue`
      );
    }
  }
});
var gqlScalarTypeByName = {
  ID: import_graphql.GraphQLID,
  Int: import_graphql.GraphQLInt,
  Float: import_graphql.GraphQLFloat,
  String: import_graphql.GraphQLString,
  Boolean: import_graphql.GraphQLBoolean,
  BigInt: GraphQLBigInt,
  Bytes: import_graphql.GraphQLString
};
var buildSchema = (graphqlSchema) => {
  const gqlEntityTypes = getEntityTypes(graphqlSchema);
  const gqlEnumTypes = getEnumTypes(graphqlSchema);
  const userDefinedScalars = getUserDefinedScalarTypes(graphqlSchema);
  if (userDefinedScalars.length > 0) {
    throw new Error(
      `Custom scalars are not supported: ${userDefinedScalars[0]}`
    );
  }
  const entities = gqlEntityTypes.map((entity) => {
    const entityName = entity.name;
    const entityIsImmutable = !!entity.astNode?.directives?.find((directive) => directive.name.value === "entity")?.arguments?.find(
      (arg) => arg.name.value === "immutable" && arg.value.kind === "BooleanValue" && arg.value.value
    );
    const gqlFields = entity.astNode?.fields || [];
    const fields = gqlFields.map((field) => {
      const originalFieldType = field.type;
      const {
        fieldName,
        fieldTypeName,
        isNotNull,
        isList,
        isListElementNotNull
      } = unwrapFieldDefinition(field);
      const scalarBaseType = gqlScalarTypeByName[fieldTypeName];
      const enumBaseType = gqlEnumTypes.find((t) => t.name === fieldTypeName);
      const entityBaseType = gqlEntityTypes.find(
        (t) => t.name === fieldTypeName
      );
      const derivedFromDirective = field.directives?.find(
        (directive) => directive.name.value === "derivedFrom"
      );
      if (derivedFromDirective) {
        if (!entityBaseType || !isList) {
          throw new Error(
            `Resolved type of a @derivedFrom field must be a list of entities`
          );
        }
        const derivedFromFieldArgument = derivedFromDirective.arguments?.find(
          (arg) => arg.name.value === "field" && arg.value.kind === "StringValue"
        );
        if (!derivedFromFieldArgument) {
          throw new Error(
            `The @derivedFrom directive requires an argument: field`
          );
        }
        const derivedFromFieldName = derivedFromFieldArgument.value.value;
        const baseTypeAsInputType = entityBaseType;
        return {
          name: fieldName,
          kind: "DERIVED",
          baseGqlType: baseTypeAsInputType,
          originalFieldType,
          notNull: isNotNull,
          derivedFromEntityName: entityBaseType.name,
          derivedFromFieldName
        };
      }
      if (entityBaseType) {
        if (isList) {
          throw new Error(
            `Invalid field: ${entityName}.${fieldName}. Lists of entities must use the @derivedFrom directive.`
          );
        }
        const relatedEntityIdField = entityBaseType.getFields()["id"]?.astNode;
        if (!relatedEntityIdField) {
          throw new Error(
            `Related entity is missing an id field: ${entityBaseType.name}`
          );
        }
        const { fieldTypeName: fieldTypeName2 } = unwrapFieldDefinition(relatedEntityIdField);
        const relatedEntityIdType = gqlScalarTypeByName[fieldTypeName2];
        if (!relatedEntityIdType) {
          throw new Error(
            `Related entity id field is not a scalar: ${entityBaseType.name}`
          );
        }
        const entityBaseTypeAsInputType = entityBaseType;
        return {
          name: fieldName,
          kind: "RELATIONSHIP",
          baseGqlType: entityBaseTypeAsInputType,
          originalFieldType,
          notNull: isNotNull,
          relatedEntityName: entityBaseType.name,
          relatedEntityIdType
        };
      }
      if (isList) {
        if (scalarBaseType) {
          return {
            name: fieldName,
            kind: "LIST",
            baseGqlType: scalarBaseType,
            originalFieldType,
            notNull: isNotNull,
            isListElementNotNull
          };
        }
        if (enumBaseType) {
          return {
            name: fieldName,
            kind: "LIST",
            baseGqlType: enumBaseType,
            originalFieldType,
            notNull: isNotNull,
            isListElementNotNull
          };
        }
      }
      if (scalarBaseType) {
        const baseType = scalarBaseType;
        if (fieldName === "id") {
          if (!isNotNull) {
            throw new Error(`${entityName}.id field must be non-null`);
          }
          if (isList) {
            throw new Error(`${entityName}.id field must not be a list`);
          }
          if (!["BigInt", "String", "Int", "Bytes"].includes(baseType.name)) {
            throw new Error(
              `${entityName}.id field must be a String, BigInt, Int, or Bytes.`
            );
          }
        }
        return {
          name: fieldName,
          kind: "SCALAR",
          notNull: isNotNull,
          originalFieldType,
          scalarTypeName: fieldTypeName,
          scalarGqlType: baseType
        };
      }
      if (enumBaseType) {
        const enumValues = (enumBaseType.astNode?.values || []).map(
          (v) => v.name.value
        );
        return {
          name: fieldName,
          kind: "ENUM",
          enumGqlType: enumBaseType,
          originalFieldType,
          notNull: isNotNull,
          enumValues
        };
      }
      throw new Error(`Unhandled field type: ${fieldTypeName}`);
    });
    const fieldByName = {};
    fields.forEach((field) => {
      fieldByName[field.name] = field;
    });
    return {
      name: entityName,
      gqlType: entity,
      isImmutable: entityIsImmutable,
      fields,
      fieldByName
    };
  });
  const schema = {
    entities
  };
  return schema;
};
var unwrapFieldDefinition = (field) => {
  const fieldName = field.name.value;
  let fieldType = field.type;
  let isNotNull = false;
  let isList = false;
  let isListElementNotNull = false;
  if (fieldType.kind === import_graphql.Kind.NON_NULL_TYPE) {
    isNotNull = true;
    fieldType = fieldType.type;
  }
  if (fieldType.kind === import_graphql.Kind.LIST_TYPE) {
    isList = true;
    fieldType = fieldType.type;
    if (fieldType.kind === import_graphql.Kind.NON_NULL_TYPE) {
      isListElementNotNull = true;
      fieldType = fieldType.type;
    }
  }
  if (fieldType.kind === import_graphql.Kind.LIST_TYPE) {
    throw new Error(
      `Invalid field "${fieldName}": nested lists are not supported`
    );
  }
  return {
    fieldName,
    fieldTypeName: fieldType.name.value,
    isNotNull,
    isList,
    isListElementNotNull
  };
};
var getEntityTypes = (schema) => {
  const entities = Object.values(schema.getTypeMap()).filter((type) => {
    return type.astNode?.kind === import_graphql.Kind.OBJECT_TYPE_DEFINITION;
  }).filter((type) => {
    return !!type.astNode?.directives?.find(
      (directive) => directive.name.value === "entity"
    );
  });
  return entities;
};
var getUserDefinedScalarTypes = (schema) => {
  return Object.values(schema.getTypeMap()).filter(
    (type) => !!type.astNode && type.astNode.kind === import_graphql.Kind.SCALAR_TYPE_DEFINITION && !["BigInt", "Bytes"].includes(type.name)
  );
};
var getEnumTypes = (schema) => {
  return Object.values(schema.getTypeMap()).filter(
    (type) => !!type.astNode && type.astNode.kind === import_graphql.Kind.ENUM_TYPE_DEFINITION
  );
};

// src/server/graphql/schema.ts
var import_graphql5 = require("graphql");

// src/server/graphql/entity.ts
var import_graphql2 = require("graphql");
var buildEntityType = ({
  entity,
  entityGqlTypes
}) => {
  return new import_graphql2.GraphQLObjectType({
    name: entity.name,
    fields: () => {
      const fieldConfigMap = {};
      entity.fields.forEach((field) => {
        switch (field.kind) {
          case "SCALAR": {
            fieldConfigMap[field.name] = {
              type: field.notNull ? new import_graphql2.GraphQLNonNull(field.scalarGqlType) : field.scalarGqlType,
              // Convert bigints to strings for GraphQL responses.
              resolve: field.scalarTypeName === "BigInt" ? (
                // eslint-disable-next-line @typescript-eslint/ban-ts-comment
                // @ts-ignore
                (parent) => parent[field.name].toString()
              ) : void 0
            };
            break;
          }
          case "ENUM": {
            fieldConfigMap[field.name] = {
              type: field.notNull ? new import_graphql2.GraphQLNonNull(field.enumGqlType) : field.enumGqlType
            };
            break;
          }
          case "RELATIONSHIP": {
            const resolver = async (parent, args, context) => {
              const { store } = context;
              const relatedInstanceId = parent[field.name];
              return await store.findUnique({
                modelName: field.relatedEntityName,
                id: relatedInstanceId
              });
            };
            fieldConfigMap[field.name] = {
              type: entityGqlTypes[field.baseGqlType.name],
              resolve: resolver
            };
            break;
          }
          case "DERIVED": {
            const resolver = async (parent, args, context) => {
              const { store } = context;
              const entityId = parent.id;
              return await store.findMany({
                modelName: field.derivedFromEntityName,
                filter: {
                  where: {
                    [field.derivedFromFieldName]: entityId
                  }
                }
              });
            };
            fieldConfigMap[field.name] = {
              type: new import_graphql2.GraphQLNonNull(
                new import_graphql2.GraphQLList(
                  new import_graphql2.GraphQLNonNull(entityGqlTypes[field.baseGqlType.name])
                )
              ),
              resolve: resolver
            };
            break;
          }
          case "LIST": {
            const listType = new import_graphql2.GraphQLList(
              field.isListElementNotNull ? new import_graphql2.GraphQLNonNull(field.baseGqlType) : field.baseGqlType
            );
            fieldConfigMap[field.name] = {
              type: field.notNull ? new import_graphql2.GraphQLNonNull(listType) : listType
            };
            break;
          }
        }
      });
      return fieldConfigMap;
    }
  });
};

// src/server/graphql/plural.ts
var import_graphql3 = require("graphql");
var operators = {
  universal: ["", "_not"],
  singular: ["_in", "_not_in"],
  plural: ["_contains", "_not_contains"],
  numeric: ["_gt", "_lt", "_gte", "_lte"],
  string: [
    "_contains",
    "_not_contains",
    "_starts_with",
    "_ends_with",
    "_not_starts_with",
    "_not_ends_with"
  ]
};
var buildPluralField = ({
  entity,
  entityGqlType
}) => {
  const filterFields = {};
  entity.fields.forEach((field) => {
    switch (field.kind) {
      case "SCALAR": {
        operators.universal.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: field.scalarGqlType
          };
        });
        operators.singular.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: new import_graphql3.GraphQLList(field.scalarGqlType)
          };
        });
        if (["Int", "BigInt", "Float"].includes(field.scalarTypeName)) {
          operators.numeric.forEach((suffix) => {
            filterFields[`${field.name}${suffix}`] = {
              type: field.scalarGqlType
            };
          });
        }
        if (["String", "Bytes"].includes(field.scalarTypeName)) {
          operators.string.forEach((suffix) => {
            filterFields[`${field.name}${suffix}`] = {
              type: field.scalarGqlType
            };
          });
        }
        break;
      }
      case "ENUM": {
        operators.universal.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = { type: field.enumGqlType };
        });
        operators.singular.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: new import_graphql3.GraphQLList(field.enumGqlType)
          };
        });
        break;
      }
      case "LIST": {
        operators.universal.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: new import_graphql3.GraphQLList(field.baseGqlType)
          };
        });
        operators.plural.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = { type: field.baseGqlType };
        });
        break;
      }
      case "RELATIONSHIP": {
        operators.universal.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: field.relatedEntityIdType
          };
        });
        operators.singular.forEach((suffix) => {
          filterFields[`${field.name}${suffix}`] = {
            type: new import_graphql3.GraphQLList(field.relatedEntityIdType)
          };
        });
        if (["Int", "BigInt", "Float"].includes(field.relatedEntityIdType.name)) {
          operators.numeric.forEach((suffix) => {
            filterFields[`${field.name}${suffix}`] = {
              type: field.relatedEntityIdType
            };
          });
        }
        if (["String", "Bytes"].includes(field.relatedEntityIdType.name)) {
          operators.string.forEach((suffix) => {
            filterFields[`${field.name}${suffix}`] = {
              type: field.relatedEntityIdType
            };
          });
        }
        break;
      }
      case "DERIVED": {
        break;
      }
    }
  });
  const filterType = new import_graphql3.GraphQLInputObjectType({
    name: `${entity.name}Filter`,
    fields: filterFields
  });
  const resolver = async (_, args, context) => {
    const { store } = context;
    const filter = args;
    return await store.findMany({ modelName: entity.name, filter });
  };
  return {
    type: new import_graphql3.GraphQLNonNull(
      new import_graphql3.GraphQLList(new import_graphql3.GraphQLNonNull(entityGqlType))
    ),
    args: {
      where: { type: filterType },
      first: { type: import_graphql3.GraphQLInt },
      skip: { type: import_graphql3.GraphQLInt },
      orderBy: { type: import_graphql3.GraphQLString },
      orderDirection: { type: import_graphql3.GraphQLString }
    },
    resolve: resolver
  };
};

// src/server/graphql/singular.ts
var import_graphql4 = require("graphql");
var buildSingularField = ({
  entity,
  entityGqlType
}) => {
  const resolver = async (_, args, context) => {
    const { store } = context;
    const { id } = args;
    if (!id)
      return null;
    const entityInstance = await store.findUnique({
      modelName: entity.name,
      id
    });
    return entityInstance;
  };
  return {
    type: entityGqlType,
    args: {
      id: { type: new import_graphql4.GraphQLNonNull(entity.fieldByName.id.scalarGqlType) }
    },
    resolve: resolver
  };
};

// src/server/graphql/schema.ts
var buildGqlSchema = (schema) => {
  const queryFields = {};
  const entityGqlTypes = {};
  for (const entity of schema.entities) {
    entityGqlTypes[entity.name] = buildEntityType({ entity, entityGqlTypes });
  }
  for (const entity of schema.entities) {
    const entityGqlType = entityGqlTypes[entity.name];
    const singularFieldName = entity.name.charAt(0).toLowerCase() + entity.name.slice(1);
    queryFields[singularFieldName] = buildSingularField({
      entity,
      entityGqlType
    });
    const pluralFieldName = singularFieldName + "s";
    queryFields[pluralFieldName] = buildPluralField({ entity, entityGqlType });
  }
  const queryType = new import_graphql5.GraphQLObjectType({
    name: "Query",
    fields: queryFields
  });
  const gqlSchema = new import_graphql5.GraphQLSchema({
    query: queryType
  });
  return gqlSchema;
};

// src/build/schema.ts
var import_graphql6 = require("graphql");
var import_node_fs2 = require("fs");
var schemaHeader = `
"Directs the executor to process this type as a Ponder entity."
directive @entity(immutable: Boolean = false) on OBJECT

"Creates a virtual field on the entity that may be queried but cannot be set manually through the mappings API."
directive @derivedFrom(field: String!) on FIELD_DEFINITION

scalar Bytes
scalar BigInt
`;
var readGraphqlSchema = ({ options }) => {
  const schemaBody = (0, import_node_fs2.readFileSync)(options.schemaFile);
  const schemaSource = schemaHeader + schemaBody.toString();
  const schema = (0, import_graphql6.buildSchema)(schemaSource);
  return schema;
};

// src/build/service.ts
var BuildService = class extends import_emittery.default {
  resources;
  closeWatcher;
  latestFileHashes = {};
  constructor({ resources }) {
    super();
    this.resources = resources;
  }
  async kill() {
    this.closeWatcher?.();
    this.resources.logger.debug({
      service: "build",
      msg: `Killed build service`
    });
  }
  watch() {
    const watchFiles = [
      this.resources.options.configFile,
      this.resources.options.schemaFile,
      this.resources.options.srcDir
    ];
    const watcher = import_chokidar.default.watch(watchFiles);
    this.closeWatcher = async () => {
      await watcher.close();
    };
    watcher.on("change", async (filePath) => {
      if (filePath === this.resources.options.configFile) {
        this.emit("newConfig");
        return;
      }
      if (this.isFileChanged(filePath)) {
        const fileName = import_node_path2.default.basename(filePath);
        this.resources.logger.info({
          service: "build",
          msg: `Detected change in ${fileName}`
        });
        this.resources.errors.hasUserError = false;
        if (filePath === this.resources.options.schemaFile) {
          this.buildSchema();
        } else {
          await this.buildHandlers();
        }
      }
    });
  }
  async buildHandlers() {
    try {
      const handlers = await readHandlers({ options: this.resources.options });
      this.emit("newHandlers", { handlers });
    } catch (error_) {
      const error = error_;
      const message = `Error while building handlers: ${error.message}`;
      const userError = new UserError(message, {
        stack: error.stack
      });
      this.resources.logger.error({
        service: "build",
        error: userError
      });
      this.resources.errors.submitUserError({ error: userError });
    }
  }
  buildSchema() {
    try {
      const userGraphqlSchema = readGraphqlSchema({
        options: this.resources.options
      });
      const schema = buildSchema(userGraphqlSchema);
      const graphqlSchema = buildGqlSchema(schema);
      this.emit("newSchema", { schema, graphqlSchema });
      return { schema, graphqlSchema };
    } catch (error_) {
      const error = error_;
      const message = `Error while building schema.graphql: ${error.message}`;
      const userError = new UserError(message, {
        stack: error.stack
      });
      this.resources.logger.error({
        service: "build",
        error: userError
      });
      this.resources.errors.submitUserError({ error: userError });
    }
  }
  isFileChanged(filePath) {
    try {
      const content = (0, import_node_fs3.readFileSync)(filePath, "utf-8");
      const hash = (0, import_node_crypto.createHash)("md5").update(content).digest("hex");
      const prevHash = this.latestFileHashes[filePath];
      this.latestFileHashes[filePath] = hash;
      if (!prevHash) {
        return true;
      } else {
        return prevHash !== hash;
      }
    } catch (e) {
      return true;
    }
  }
};

// src/codegen/service.ts
var import_emittery2 = __toESM(require("emittery"));
var import_graphql8 = require("graphql");
var import_node_fs5 = require("fs");
var import_node_path4 = __toESM(require("path"));

// src/utils/exists.ts
var import_node_fs4 = require("fs");
var import_node_path3 = __toESM(require("path"));
var ensureDirExists = (filePath) => {
  const dirname = import_node_path3.default.dirname(filePath);
  if ((0, import_node_fs4.existsSync)(dirname)) {
    return;
  }
  (0, import_node_fs4.mkdirSync)(dirname, { recursive: true });
};

// src/codegen/contract.ts
var buildContractTypes = (contracts) => {
  return contracts.map((contract) => {
    const abiReadOrViewFunctions = contract.abi.filter(
      (item) => item.type === "function" && (item.stateMutability === "pure" || item.stateMutability === "view")
    );
    return `
      const ${contract.name}Abi = ${JSON.stringify(
      abiReadOrViewFunctions
    )} as const;

      export type ${contract.name} = ReadOnlyContract<typeof ${contract.name}Abi>;
      `;
  }).join("\n");
};

// src/codegen/entity.ts
var import_graphql7 = require("graphql");
var gqlScalarToTsType = {
  String: "string",
  Boolean: "boolean",
  Int: "number",
  Float: "number",
  BigInt: "bigint",
  Bytes: "string"
};
var buildEntityTypes = (entities) => {
  const entityModelTypes = entities.map((entity) => {
    return `export type ${entity.name} = {
        ${entity.fields.map((field) => {
      switch (field.kind) {
        case "SCALAR": {
          const scalarTsType = gqlScalarToTsType[field.scalarTypeName];
          if (!scalarTsType) {
            throw new Error(
              `TypeScript type not found for scalar: ${field.scalarTypeName}`
            );
          }
          return `${field.name}${field.notNull ? "" : "?"}: ${scalarTsType};`;
        }
        case "ENUM": {
          return `${field.name}${field.notNull ? "" : "?"}: ${field.enumValues.map((val) => `"${val}"`).join(" | ")};`;
        }
        case "LIST": {
          let tsBaseType;
          if (Object.keys(gqlScalarToTsType).includes(
            field.baseGqlType.toString()
          )) {
            const scalarTypeName = field.baseGqlType.toString();
            const scalarTsType = gqlScalarToTsType[scalarTypeName];
            if (!scalarTsType) {
              throw new Error(
                `TypeScript type not found for scalar: ${scalarTypeName}`
              );
            }
            tsBaseType = scalarTsType;
          } else if (field.baseGqlType.astNode?.kind === import_graphql7.Kind.ENUM_TYPE_DEFINITION) {
            const enumValues = (field.baseGqlType.astNode?.values || []).map((v) => v.name.value);
            tsBaseType = `(${enumValues.map((v) => `"${v}"`).join(" | ")})`;
          } else {
            throw new Error(
              `Unable to generate type for field: ${field.name}`
            );
          }
          if (!field.isListElementNotNull) {
            tsBaseType = `(${tsBaseType} | null)`;
          }
          return `${field.name}${field.notNull ? "" : "?"}: ${tsBaseType}[];`;
        }
        case "RELATIONSHIP": {
          return `${field.name}${field.notNull ? "" : "?"}: string;`;
        }
      }
    }).join("")}
        };`;
  }).join("");
  return entityModelTypes;
};

// src/codegen/event.ts
var buildEventTypes = (logFilters) => {
  const allHandlers = logFilters.map((logFilter) => {
    const abiEvents = logFilter.abi.filter(
      (item) => item.type === "event"
    );
    return abiEvents.map(({ name, inputs }) => {
      const paramsType = `{${inputs.map((input, index) => {
        const inputName = input.name ? input.name : `param_${index}`;
        return `${inputName}:
              AbiParameterToPrimitiveType<${JSON.stringify(input)}>`;
      }).join(";")}}`;
      return `["${logFilter.name}:${name}"]: ({
            event, context
            }: {
              event: {
                name: "${name}";
                params: ${paramsType};
                log: Log;
                block: Block;
                transaction: Transaction;
              };
              context: Context;
            }) => Promise<any> | any;`;
    }).join("");
  });
  allHandlers.unshift(
    `["setup"]: ({ context }: { context: Context; }) => Promise<any> | any;`
  );
  const final = `
    export type AppType = {
      ${allHandlers.join("")}
    }
  `;
  return final;
};

// src/codegen/prettier.ts
var import_prettier = __toESM(require("prettier"));
var prettierConfig = { parser: "typescript" };
var loadPrettierConfig = async () => {
  if (prettierConfig)
    return prettierConfig;
  const configFile = await import_prettier.default.resolveConfigFile();
  if (configFile) {
    const foundConfig = await import_prettier.default.resolveConfig(configFile);
    if (foundConfig) {
      prettierConfig = foundConfig;
    }
  }
};
loadPrettierConfig();
var formatPrettier = (source, configOverrides) => {
  return import_prettier.default.format(source, { ...prettierConfig, ...configOverrides });
};

// src/codegen/service.ts
var CodegenService = class extends import_emittery2.default {
  resources;
  contracts;
  logFilters;
  constructor({
    resources,
    contracts,
    logFilters
  }) {
    super();
    this.resources = resources;
    this.contracts = contracts;
    this.logFilters = logFilters;
  }
  generateAppFile({ schema } = {}) {
    const entities = schema?.entities || [];
    const raw = `
      /* Autogenerated file. Do not edit manually. */
  
      import { PonderApp } from "@ponder/core";
      import type { Block, Log, Transaction, Model, ReadOnlyContract } from "@ponder/core";
      import type { AbiParameterToPrimitiveType } from "abitype";
      import type { BlockTag, Hash } from "viem";

      /* ENTITY TYPES */

      ${buildEntityTypes(entities)}
  
      /* CONTRACT TYPES */

      ${buildContractTypes(this.contracts)}

      /* CONTEXT TYPES */

      export type Context = {
        contracts: {
          ${this.contracts.map((contract) => `${contract.name}: ${contract.name};`).join("")}
        },
        entities: {
          ${entities.map((entity) => `${entity.name}: Model<${entity.name}>;`).join("")}
        },
      }

  
      /* HANDLER TYPES */
    
      ${buildEventTypes(this.logFilters)}

      export const ponder = new PonderApp<AppType>();
    `;
    const final = formatPrettier(raw);
    const filePath = import_node_path4.default.join(this.resources.options.generatedDir, "index.ts");
    ensureDirExists(filePath);
    (0, import_node_fs5.writeFileSync)(filePath, final, "utf8");
    this.resources.logger.debug({
      service: "codegen",
      msg: `Wrote new file at generated/index.ts`
    });
  }
  generateSchemaFile({ graphqlSchema }) {
    const header = `
      """ Autogenerated file. Do not edit manually. """
    `;
    const body = (0, import_graphql8.printSchema)(graphqlSchema);
    const final = header + body;
    const filePath = import_node_path4.default.join(
      this.resources.options.generatedDir,
      "schema.graphql"
    );
    ensureDirExists(filePath);
    (0, import_node_fs5.writeFileSync)(filePath, final, "utf8");
    this.resources.logger.debug({
      service: "codegen",
      msg: `Wrote new file at generated/schema.graphql`
    });
  }
};

// src/config/abi.ts
var import_node_fs6 = require("fs");
var import_node_path5 = __toESM(require("path"));
var buildAbi = ({
  abiConfig,
  configFilePath
}) => {
  let resolvedAbi;
  const filePaths = [];
  if (typeof abiConfig === "string" || Array.isArray(abiConfig) && (abiConfig.length === 0 || typeof abiConfig[0] === "object")) {
    const { abi, filePath } = buildSingleAbi({ abiConfig, configFilePath });
    resolvedAbi = abi;
    if (filePath)
      filePaths.push(filePath);
  } else {
    const results = abiConfig.map(
      (a) => buildSingleAbi({ abiConfig: a, configFilePath })
    );
    const mergedAbi = results.map(({ abi }) => abi.filter((item) => item.type !== "constructor")).flat().flat();
    const mergedUniqueAbi = [
      ...new Map(
        mergedAbi.map((item) => [JSON.stringify(item), item])
      ).values()
    ];
    filePaths.push(
      ...results.map((r) => r.filePath).filter((f) => !!f)
    );
    resolvedAbi = mergedUniqueAbi;
  }
  return {
    abi: resolvedAbi,
    filePaths
  };
};
var buildSingleAbi = ({
  abiConfig,
  configFilePath
}) => {
  let filePath = void 0;
  let abi;
  if (typeof abiConfig === "string") {
    filePath = import_node_path5.default.isAbsolute(abiConfig) ? abiConfig : import_node_path5.default.join(import_node_path5.default.dirname(configFilePath), abiConfig);
    const abiString = (0, import_node_fs6.readFileSync)(filePath, "utf-8");
    abi = JSON.parse(abiString);
  } else {
    abi = abiConfig;
  }
  return { abi, filePath };
};

// src/config/networks.ts
var import_viem = require("viem");
var import_chains = require("viem/chains");
var clients = {};
function buildNetwork({
  network
}) {
  let client = clients[network.chainId];
  if (!client) {
    client = (0, import_viem.createPublicClient)({
      transport: (0, import_viem.http)(network.rpcUrl),
      chain: {
        ...import_chains.mainnet,
        name: network.name,
        id: network.chainId,
        network: network.name
      }
    });
    clients[network.chainId] = client;
  }
  const resolvedNetwork = {
    name: network.name,
    chainId: network.chainId,
    client,
    rpcUrl: network.rpcUrl,
    pollingInterval: network.pollingInterval ?? 1e3,
    defaultMaxBlockRange: getDefaultMaxBlockRange(network),
    maxRpcRequestConcurrency: network.maxRpcRequestConcurrency ?? 10,
    finalityBlockCount: getFinalityBlockCount(network)
  };
  return resolvedNetwork;
}
function getDefaultMaxBlockRange(network) {
  if (network.rpcUrl !== void 0 && network.rpcUrl.includes("quiknode.pro")) {
    return 1e4;
  }
  let maxBlockRange;
  switch (network.chainId) {
    case 1:
    case 3:
    case 4:
    case 5:
    case 42:
    case 11155111:
      maxBlockRange = 2e3;
      break;
    case 10:
    case 420:
      maxBlockRange = 5e4;
      break;
    case 137:
    case 80001:
      maxBlockRange = 5e4;
      break;
    case 42161:
    case 421613:
      maxBlockRange = 5e4;
      break;
    default:
      maxBlockRange = 5e4;
  }
  return maxBlockRange;
}
function getFinalityBlockCount(network) {
  let finalityBlockCount;
  switch (network.chainId) {
    case 1:
    case 3:
    case 4:
    case 5:
    case 42:
    case 11155111:
      finalityBlockCount = 32;
      break;
    case 10:
    case 420:
      finalityBlockCount = 5;
      break;
    case 137:
    case 80001:
      finalityBlockCount = 100;
      break;
    case 42161:
    case 421613:
      finalityBlockCount = 40;
      break;
    case 7777777:
      finalityBlockCount = 5;
      break;
    default:
      finalityBlockCount = 5;
  }
  return finalityBlockCount;
}

// src/config/contracts.ts
function buildContracts({
  config,
  options
}) {
  return (config.contracts ?? []).map((contract) => {
    const address = contract.address.toLowerCase();
    const { abi } = buildAbi({
      abiConfig: contract.abi,
      configFilePath: options.configFile
    });
    const rawNetwork = config.networks.find((n) => n.name === contract.network);
    if (!rawNetwork) {
      throw new Error(
        `Network [${contract.network}] not found for contract: ${contract.name}`
      );
    }
    const network = buildNetwork({ network: rawNetwork });
    return {
      name: contract.name,
      address,
      network,
      abi
    };
  });
}

// src/config/database.ts
var import_better_sqlite3 = __toESM(require("better-sqlite3"));
var import_node_path6 = __toESM(require("path"));
var import_pg = __toESM(require("pg"));

// src/utils/print.ts
function prettyPrint(args) {
  const entries = Object.entries(args).map(([key, value]) => {
    if (value === void 0 || value === false)
      return null;
    const trimmedValue = typeof value === "string" && value.length > 80 ? value.slice(0, 80).concat("...") : value;
    return [key, trimmedValue];
  }).filter(Boolean);
  const maxLength = entries.reduce(
    (acc, [key]) => Math.max(acc, key.length),
    0
  );
  return entries.map(([key, value]) => `  ${`${key}:`.padEnd(maxLength + 1)}  ${value}`).join("\n");
}

// src/errors/base.ts
var BaseError = class extends Error {
  details;
  metaMessages;
  shortMessage;
  name = "PonderError";
  constructor(shortMessage, args = {}) {
    const details = args.cause instanceof BaseError ? args.cause.details : args.cause?.message ? args.cause.message : args.details;
    const message = [
      shortMessage || "An error occurred.",
      // "",
      ...args.metaMessages ? [...args.metaMessages, ""] : [],
      ...details ? [`Details: ${details}`] : []
    ].join("\n");
    super(message);
    if (args.cause)
      this.cause = args.cause;
    if (!details)
      this.details = message;
    this.metaMessages = args.metaMessages;
    this.shortMessage = shortMessage;
  }
};

// src/errors/postgres.ts
var PostgresError = class extends BaseError {
  name = "PostgresError";
  constructor({
    statement,
    parameters,
    pgError
  }) {
    const params = parameters.reduce(
      (acc, parameter, idx) => {
        acc[idx + 1] = parameter;
        return acc;
      },
      {}
    );
    const metaMessages = [];
    if (pgError.detail)
      metaMessages.push(`Detail:
  ${pgError.detail}`);
    metaMessages.push(`Statement:
  ${statement}`);
    metaMessages.push(`Parameters:
${prettyPrint(params)}`);
    const shortMessage = `PostgreSQL error: ${pgError.message}`;
    super(shortMessage, {
      metaMessages
    });
  }
};

// src/errors/sqlite.ts
var SqliteError = class extends BaseError {
  name = "SqliteError";
  constructor({
    statement,
    parameters,
    sqliteError
  }) {
    const params = parameters.reduce(
      (acc, parameter, idx) => {
        acc[idx + 1] = parameter;
        return acc;
      },
      {}
    );
    const metaMessages = [];
    metaMessages.push(`Statement:
  ${statement}`);
    metaMessages.push(`Parameters:
${prettyPrint(params)}`);
    const shortMessage = `SQLite error: ${sqliteError.message}`;
    super(shortMessage, {
      metaMessages
    });
  }
};

// src/config/database.ts
import_pg.default.types.setTypeParser(20, BigInt);
var originalClientQuery = import_pg.Client.prototype.query;
import_pg.Client.prototype.query = async function query(...args) {
  try {
    return await originalClientQuery.apply(this, args);
  } catch (error) {
    const [statement, parameters] = args;
    if (error instanceof import_pg.DatabaseError) {
      const parameters_ = parameters ?? [];
      throw new PostgresError({
        statement,
        parameters: parameters_.length <= 25 ? parameters_ : parameters_.slice(0, 26).concat(["..."]),
        pgError: error
      });
    }
    throw error;
  }
};
var patchSqliteDatabase = ({ db }) => {
  const oldPrepare = db.prepare;
  db.prepare = (source) => {
    const statement = oldPrepare.apply(db, [source]);
    const wrapper = (fn) => (...args) => {
      try {
        return fn.apply(statement, args);
      } catch (error) {
        throw new SqliteError({
          statement: source,
          parameters: args[0],
          sqliteError: error
        });
      }
    };
    for (const method of ["run", "get", "all"]) {
      statement[method] = wrapper(statement[method]);
    }
    return statement;
  };
  return db;
};
var buildDatabase = ({
  options,
  config
}) => {
  let resolvedDatabaseConfig;
  const defaultSqliteFilename = import_node_path6.default.join(options.ponderDir, "cache.db");
  if (config.database) {
    if (config.database.kind === "postgres") {
      resolvedDatabaseConfig = {
        kind: "postgres",
        connectionString: config.database.connectionString
      };
    } else {
      resolvedDatabaseConfig = {
        kind: "sqlite",
        filename: config.database.filename ?? defaultSqliteFilename
      };
    }
  } else {
    if (process.env.DATABASE_URL) {
      resolvedDatabaseConfig = {
        kind: "postgres",
        connectionString: process.env.DATABASE_URL
      };
    } else {
      resolvedDatabaseConfig = {
        kind: "sqlite",
        filename: defaultSqliteFilename
      };
    }
  }
  if (resolvedDatabaseConfig.kind === "sqlite") {
    ensureDirExists(resolvedDatabaseConfig.filename);
    const rawDb = (0, import_better_sqlite3.default)(resolvedDatabaseConfig.filename);
    rawDb.pragma("journal_mode = WAL");
    const db = patchSqliteDatabase({ db: rawDb });
    return { kind: "sqlite", db };
  } else {
    const pool = new import_pg.Pool({
      connectionString: resolvedDatabaseConfig.connectionString
    });
    return { kind: "postgres", pool };
  }
};

// src/config/logFilters.ts
var import_viem2 = require("viem");

// src/config/logFilterKey.ts
function encodeLogFilterKey({
  chainId,
  address,
  topics
}) {
  return `${chainId}-${JSON.stringify(address ?? null)}-${JSON.stringify(
    topics ?? null
  )}`;
}

// src/config/logFilters.ts
function buildLogFilters({
  config,
  options
}) {
  const contractLogFilters = (config.contracts ?? []).filter((contract) => contract.isLogEventSource ?? true).map((contract) => {
    const { abi } = buildAbi({
      abiConfig: contract.abi,
      configFilePath: options.configFile
    });
    const network = config.networks.find((n) => n.name === contract.network);
    if (!network) {
      throw new Error(
        `Network [${contract.network}] not found for contract: ${contract.name}`
      );
    }
    const address = contract.address.toLowerCase();
    const topics = void 0;
    const key = encodeLogFilterKey({
      chainId: network.chainId,
      address,
      topics
    });
    const logFilter = {
      name: contract.name,
      abi,
      network: network.name,
      filter: {
        key,
        chainId: network.chainId,
        address,
        topics,
        startBlock: contract.startBlock ?? 0,
        endBlock: contract.endBlock
      },
      maxBlockRange: contract.maxBlockRange
    };
    return logFilter;
  });
  const filterLogFilters = (config.filters ?? []).map((filter) => {
    const { abi } = buildAbi({
      abiConfig: filter.abi,
      configFilePath: options.configFile
    });
    const network = config.networks.find((n) => n.name === filter.network);
    if (!network) {
      throw new Error(
        `Network [${filter.network}] not found for filter: ${filter.name}`
      );
    }
    const address = Array.isArray(filter.filter.address) ? filter.filter.address.map((a) => a.toLowerCase()) : typeof filter.filter.address === "string" ? filter.filter.address.toLowerCase() : void 0;
    const topics = filter.filter.event ? (0, import_viem2.encodeEventTopics)({
      abi: [filter.filter.event],
      eventName: filter.filter.event.name,
      args: filter.filter.args
    }) : void 0;
    const key = encodeLogFilterKey({
      chainId: network.chainId,
      address,
      topics
    });
    const logFilter = {
      name: filter.name,
      abi,
      network: network.name,
      filter: {
        key,
        chainId: network.chainId,
        address,
        topics,
        startBlock: filter.startBlock ?? 0,
        endBlock: filter.endBlock
      },
      maxBlockRange: filter.maxBlockRange
    };
    return logFilter;
  });
  const logFilters = contractLogFilters.concat(filterLogFilters);
  return logFilters;
}

// src/errors/service.ts
var import_emittery3 = __toESM(require("emittery"));
var UserErrorService = class extends import_emittery3.default {
  hasUserError = false;
  submitUserError({ error }) {
    this.hasUserError = true;
    this.emit("error", { error });
  }
};

// src/event-aggregator/service.ts
var import_emittery4 = __toESM(require("emittery"));
var import_viem3 = require("viem");
var EventAggregatorService = class extends import_emittery4.default {
  eventStore;
  logFilters;
  networks;
  // Minimum timestamp at which events are available (across all networks).
  checkpoint;
  // Minimum finalized timestamp (across all networks).
  finalityCheckpoint;
  // Timestamp at which the historical sync was completed (across all networks).
  historicalSyncCompletedAt;
  // Per-network event timestamp checkpoints.
  networkCheckpoints;
  metrics;
  constructor({
    eventStore,
    networks,
    logFilters
  }) {
    super();
    this.eventStore = eventStore;
    this.logFilters = logFilters;
    this.networks = networks;
    this.metrics = {};
    this.checkpoint = 0;
    this.finalityCheckpoint = 0;
    this.networkCheckpoints = {};
    this.networks.forEach((network) => {
      this.networkCheckpoints[network.chainId] = {
        isHistoricalSyncComplete: false,
        historicalCheckpoint: 0,
        realtimeCheckpoint: 0,
        finalityCheckpoint: 0
      };
    });
  }
  /** Fetches events for all registered log filters between the specified timestamps.
   *
   * @param options.fromTimestamp Timestamp to start including events (inclusive).
   * @param options.toTimestamp Timestamp to stop including events (inclusive).
   * @param options.handledLogFilters Subset of log filters that the user has provided a handler for.
   * @returns A promise resolving to an array of log events.
   */
  getEvents = async ({
    fromTimestamp,
    toTimestamp,
    handledLogFilters
  }) => {
    const { events, totalEventCount } = await this.eventStore.getLogEvents({
      fromTimestamp,
      toTimestamp,
      filters: this.logFilters.map((logFilter) => ({
        name: logFilter.name,
        chainId: logFilter.filter.chainId,
        address: logFilter.filter.address,
        topics: logFilter.filter.topics,
        fromBlock: logFilter.filter.startBlock,
        toBlock: logFilter.filter.endBlock,
        handledTopic0: handledLogFilters[logFilter.name].map((i) => i.topic0)
      }))
    });
    const decodedEvents = events.reduce((acc, event) => {
      const logFilterData = handledLogFilters[event.filterName].find(
        (i) => i.topic0 === event.log.topics[0]
      );
      try {
        const decodedLog = (0, import_viem3.decodeEventLog)({
          abi: [logFilterData?.abiItem],
          data: event.log.data,
          topics: event.log.topics
        });
        acc.push({
          logFilterName: event.filterName,
          eventName: decodedLog.eventName,
          params: decodedLog.args || {},
          log: event.log,
          block: event.block,
          transaction: event.transaction
        });
      } catch (err) {
      }
      return acc;
    }, []);
    return {
      totalEventCount,
      events: decodedEvents
    };
  };
  handleNewHistoricalCheckpoint = ({
    chainId,
    timestamp
  }) => {
    this.networkCheckpoints[chainId].historicalCheckpoint = timestamp;
    this.recalculateCheckpoint();
  };
  handleHistoricalSyncComplete = ({ chainId }) => {
    this.networkCheckpoints[chainId].isHistoricalSyncComplete = true;
    this.recalculateCheckpoint();
    const networkCheckpoints = Object.values(this.networkCheckpoints);
    if (networkCheckpoints.every((n) => n.isHistoricalSyncComplete)) {
      const maxHistoricalCheckpoint = Math.max(
        ...networkCheckpoints.map((n) => n.historicalCheckpoint)
      );
      this.historicalSyncCompletedAt = maxHistoricalCheckpoint;
    }
  };
  handleNewRealtimeCheckpoint = ({
    chainId,
    timestamp
  }) => {
    this.networkCheckpoints[chainId].realtimeCheckpoint = timestamp;
    this.recalculateCheckpoint();
  };
  handleNewFinalityCheckpoint = ({
    chainId,
    timestamp
  }) => {
    this.networkCheckpoints[chainId].finalityCheckpoint = timestamp;
    this.recalculateFinalityCheckpoint();
  };
  handleReorg = ({
    commonAncestorTimestamp
  }) => {
    this.emit("reorg", { commonAncestorTimestamp });
  };
  recalculateCheckpoint = () => {
    const checkpoints = Object.values(this.networkCheckpoints).map(
      (n) => n.isHistoricalSyncComplete ? Math.max(n.historicalCheckpoint, n.realtimeCheckpoint) : n.historicalCheckpoint
    );
    const newCheckpoint = Math.min(...checkpoints);
    if (newCheckpoint > this.checkpoint) {
      this.checkpoint = newCheckpoint;
      this.emit("newCheckpoint", { timestamp: this.checkpoint });
    }
  };
  recalculateFinalityCheckpoint = () => {
    const newFinalityCheckpoint = Math.min(
      ...Object.values(this.networkCheckpoints).map((n) => n.finalityCheckpoint)
    );
    if (newFinalityCheckpoint > this.finalityCheckpoint) {
      this.finalityCheckpoint = newFinalityCheckpoint;
      this.emit("newFinalityCheckpoint", {
        timestamp: this.finalityCheckpoint
      });
    }
  };
};

// src/event-store/postgres/store.ts
var import_kysely2 = require("kysely");

// src/utils/decode.ts
var EVM_MAX_UINT = 115792089237316195423570985008687907853269984665640564039457584007913129639935n;
function blobToBigInt(buffer) {
  const signByte = buffer.at(0);
  const hexString = buffer.subarray(1).toString("hex").replace(/^0+/, "");
  if (hexString.length === 0)
    return 0n;
  let value = BigInt("0x" + hexString);
  if (signByte === 0) {
    value = value - EVM_MAX_UINT;
  }
  return value;
}

// src/utils/encode.ts
var EVM_MAX_UINT2 = 115792089237316195423570985008687907853269984665640564039457584007913129639935n;
function intToBlob(value) {
  if (typeof value === "string" || typeof value === "number")
    value = BigInt(value);
  const signByte = value >= 0n ? "ff" : "00";
  if (value < 0n)
    value = EVM_MAX_UINT2 + value;
  let hexString = value.toString(16);
  if (hexString.length > 64) {
    throw new Error(
      `Value exceeds the EVM_MAX_UINT size (32 byte unsigned integer): ${value}`
    );
  }
  hexString = signByte + hexString.padStart(64, "0");
  return Buffer.from(hexString, "hex");
}

// src/utils/intervals.ts
function mergeIntervals(intervals) {
  intervals.sort((a, b) => a[0] - b[0]);
  const result = [];
  let last;
  intervals.forEach((interval) => {
    if (interval[1] < interval[0])
      throw new Error(`Cannot merge invalid interval: ${interval}`);
    interval = [interval[0], interval[1] + 1];
    if (!last || interval[0] > last[1]) {
      result.push(last = interval);
    } else if (interval[1] > last[1]) {
      last[1] = interval[1];
    }
  });
  return result.map((r) => [r[0], r[1] - 1]);
}

// src/event-store/postgres/format.ts
var import_viem4 = require("viem");
function rpcToPostgresBlock(block) {
  return {
    baseFeePerGas: block.baseFeePerGas ? intToBlob(block.baseFeePerGas) : null,
    difficulty: intToBlob(block.difficulty),
    extraData: block.extraData,
    gasLimit: intToBlob(block.gasLimit),
    gasUsed: intToBlob(block.gasUsed),
    hash: block.hash,
    logsBloom: block.logsBloom,
    miner: block.miner,
    mixHash: block.mixHash,
    nonce: block.nonce,
    number: intToBlob(block.number),
    parentHash: block.parentHash,
    receiptsRoot: block.receiptsRoot,
    sha3Uncles: block.sha3Uncles,
    size: intToBlob(block.size),
    stateRoot: block.stateRoot,
    timestamp: intToBlob(block.timestamp),
    totalDifficulty: intToBlob(block.totalDifficulty),
    transactionsRoot: block.transactionsRoot
  };
}
function rpcToPostgresTransaction(transaction) {
  return {
    accessList: transaction.accessList ? JSON.stringify(transaction.accessList) : void 0,
    blockHash: transaction.blockHash,
    blockNumber: intToBlob(transaction.blockNumber),
    from: transaction.from,
    gas: intToBlob(transaction.gas),
    gasPrice: transaction.gasPrice ? intToBlob(transaction.gasPrice) : null,
    hash: transaction.hash,
    input: transaction.input,
    maxFeePerGas: transaction.maxFeePerGas ? intToBlob(transaction.maxFeePerGas) : null,
    maxPriorityFeePerGas: transaction.maxPriorityFeePerGas ? intToBlob(transaction.maxPriorityFeePerGas) : null,
    nonce: (0, import_viem4.hexToNumber)(transaction.nonce),
    r: transaction.r,
    s: transaction.s,
    to: transaction.to ? transaction.to : null,
    transactionIndex: Number(transaction.transactionIndex),
    type: transaction.type ?? "0x0",
    value: intToBlob(transaction.value),
    v: intToBlob(transaction.v)
  };
}
function rpcToPostgresLog({
  log
}) {
  return {
    address: log.address,
    blockHash: log.blockHash,
    blockNumber: intToBlob(log.blockNumber),
    data: log.data,
    id: `${log.blockHash}-${log.logIndex}`,
    logIndex: Number(log.logIndex),
    topic0: log.topics[0] ? log.topics[0] : null,
    topic1: log.topics[1] ? log.topics[1] : null,
    topic2: log.topics[2] ? log.topics[2] : null,
    topic3: log.topics[3] ? log.topics[3] : null,
    transactionHash: log.transactionHash,
    transactionIndex: Number(log.transactionIndex)
  };
}

// src/event-store/postgres/migrations.ts
var import_kysely = require("kysely");
var migrations = {
  ["2023_05_15_0_initial"]: {
    async up(db) {
      await db.schema.createTable("blocks").addColumn("baseFeePerGas", import_kysely.sql`bytea`).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("difficulty", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("extraData", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("gasLimit", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("gasUsed", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("logsBloom", "text", (col) => col.notNull()).addColumn("miner", "text", (col) => col.notNull()).addColumn("mixHash", "text", (col) => col.notNull()).addColumn("nonce", "text", (col) => col.notNull()).addColumn("number", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("parentHash", "text", (col) => col.notNull()).addColumn("receiptsRoot", "text", (col) => col.notNull()).addColumn("sha3Uncles", "text", (col) => col.notNull()).addColumn("size", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("stateRoot", "text", (col) => col.notNull()).addColumn("timestamp", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("totalDifficulty", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("transactionsRoot", "text", (col) => col.notNull()).execute();
      await db.schema.createTable("transactions").addColumn("accessList", "text").addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("from", "text", (col) => col.notNull()).addColumn("gas", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("gasPrice", import_kysely.sql`bytea`).addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("input", "text", (col) => col.notNull()).addColumn("maxFeePerGas", import_kysely.sql`bytea`).addColumn("maxPriorityFeePerGas", import_kysely.sql`bytea`).addColumn("nonce", "integer", (col) => col.notNull()).addColumn("r", "text", (col) => col.notNull()).addColumn("s", "text", (col) => col.notNull()).addColumn("to", "text").addColumn("transactionIndex", "integer", (col) => col.notNull()).addColumn("type", "text", (col) => col.notNull()).addColumn("value", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("v", import_kysely.sql`bytea`, (col) => col.notNull()).execute();
      await db.schema.createTable("logs").addColumn("address", "text", (col) => col.notNull()).addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("logIndex", "integer", (col) => col.notNull()).addColumn("topic0", "text").addColumn("topic1", "text").addColumn("topic2", "text").addColumn("topic3", "text").addColumn("transactionHash", "text", (col) => col.notNull()).addColumn("transactionIndex", "integer", (col) => col.notNull()).execute();
      await db.schema.createTable("contractReadResults").addColumn("address", "text", (col) => col.notNull()).addColumn("blockNumber", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("result", "text", (col) => col.notNull()).addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
        "chainId",
        "blockNumber",
        "address",
        "data"
      ]).execute();
      await db.schema.createTable("logFilterCachedRanges").addColumn("endBlock", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("endBlockTimestamp", import_kysely.sql`bytea`, (col) => col.notNull()).addColumn("filterKey", "text", (col) => col.notNull()).addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn("startBlock", import_kysely.sql`bytea`, (col) => col.notNull()).execute();
    },
    async down(db) {
      await db.schema.dropTable("blocks").execute();
      await db.schema.dropTable("logs").execute();
      await db.schema.dropTable("transactions").execute();
      await db.schema.dropTable("contractReadResults").execute();
      await db.schema.dropTable("logFilterCachedRanges").execute();
    }
  },
  ["2023_06_20_0_indices"]: {
    async up(db) {
      await db.schema.createIndex("log_events_index").on("logs").columns(["address", "chainId", "blockHash"]).execute();
      await db.schema.createIndex("blocks_index").on("blocks").columns(["timestamp", "number"]).execute();
      await db.schema.createIndex("logFilterCachedRanges_index").on("logFilterCachedRanges").columns(["filterKey"]).execute();
    },
    async down(db) {
      await db.schema.dropIndex("log_events_index").execute();
      await db.schema.dropIndex("logFilterCachedRanges_index").execute();
      await db.schema.dropIndex("blocks_index").execute();
    }
  }
};
var StaticMigrationProvider = class {
  async getMigrations() {
    return migrations;
  }
};
var migrationProvider = new StaticMigrationProvider();

// src/event-store/postgres/store.ts
var PostgresEventStore = class {
  db;
  migrator;
  constructor({
    pool,
    databaseSchema
  }) {
    this.db = new import_kysely2.Kysely({
      dialect: new import_kysely2.PostgresDialect({
        pool,
        onCreateConnection: databaseSchema ? async (connection) => {
          await connection.executeQuery(
            import_kysely2.CompiledQuery.raw(
              `CREATE SCHEMA IF NOT EXISTS ${databaseSchema}`
            )
          );
          await connection.executeQuery(
            import_kysely2.CompiledQuery.raw(`SET search_path = ${databaseSchema}`)
          );
        } : void 0
      })
    });
    this.migrator = new import_kysely2.Migrator({
      db: this.db,
      provider: migrationProvider,
      migrationTableSchema: databaseSchema ?? "public"
    });
  }
  migrateUp = async () => {
    const { error } = await this.migrator.migrateToLatest();
    if (error)
      throw error;
  };
  migrateDown = async () => {
    const { error } = await this.migrator.migrateTo(import_kysely2.NO_MIGRATIONS);
    if (error)
      throw error;
  };
  insertUnfinalizedBlock = async ({
    chainId,
    block: rpcBlock,
    transactions: rpcTransactions,
    logs: rpcLogs
  }) => {
    const block = {
      ...rpcToPostgresBlock(rpcBlock),
      chainId,
      finalized: 0
    };
    const transactions = rpcTransactions.map(
      (transaction) => ({
        ...rpcToPostgresTransaction(transaction),
        chainId,
        finalized: 0
      })
    );
    const logs = rpcLogs.map((log) => ({
      ...rpcToPostgresLog({ log }),
      chainId,
      finalized: 0
    }));
    await this.db.transaction().execute(async (tx) => {
      await tx.insertInto("blocks").values(block).onConflict((oc) => oc.column("hash").doNothing()).execute();
      if (transactions.length > 0) {
        await tx.insertInto("transactions").values(transactions).onConflict((oc) => oc.column("hash").doNothing()).execute();
      }
      if (logs.length > 0) {
        await tx.insertInto("logs").values(logs).onConflict((oc) => oc.column("id").doNothing()).execute();
      }
    });
  };
  deleteUnfinalizedData = async ({
    chainId,
    fromBlockNumber
  }) => {
    await this.db.transaction().execute(async (tx) => {
      await tx.deleteFrom("blocks").where("number", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("transactions").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("logs").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("contractReadResults").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
    });
  };
  finalizeData = async ({
    chainId,
    toBlockNumber
  }) => {
    await this.db.transaction().execute(async (tx) => {
      await tx.updateTable("blocks").set({ finalized: 1 }).where("number", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("transactions").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("logs").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("contractReadResults").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
    });
  };
  insertFinalizedLogs = async ({
    chainId,
    logs: rpcLogs
  }) => {
    const logBatches = rpcLogs.reduce((acc, log, index) => {
      const batchIndex = Math.floor(index / 1e3);
      acc[batchIndex] = acc[batchIndex] ?? [];
      acc[batchIndex].push({
        ...rpcToPostgresLog({ log }),
        chainId,
        finalized: 1
      });
      return acc;
    }, []);
    await Promise.all(
      logBatches.map(async (batch) => {
        await this.db.insertInto("logs").values(batch).onConflict((oc) => oc.column("id").doNothing()).execute();
      })
    );
  };
  insertFinalizedBlock = async ({
    chainId,
    block: rpcBlock,
    transactions: rpcTransactions,
    logFilterRange: { logFilterKey, blockNumberToCacheFrom }
  }) => {
    const block = {
      ...rpcToPostgresBlock(rpcBlock),
      chainId,
      finalized: 1
    };
    const transactions = rpcTransactions.map(
      (transaction) => ({
        ...rpcToPostgresTransaction(transaction),
        chainId,
        finalized: 1
      })
    );
    const logFilterCachedRange = {
      filterKey: logFilterKey,
      startBlock: intToBlob(blockNumberToCacheFrom),
      endBlock: block.number,
      endBlockTimestamp: block.timestamp
    };
    await this.db.transaction().execute(async (tx) => {
      await tx.insertInto("blocks").values(block).onConflict((oc) => oc.column("hash").doNothing()).execute();
      if (transactions.length > 0) {
        await tx.insertInto("transactions").values(transactions).onConflict((oc) => oc.column("hash").doNothing()).execute();
      }
      await tx.insertInto("logFilterCachedRanges").values(logFilterCachedRange).execute();
    });
  };
  mergeLogFilterCachedRanges = async ({
    logFilterKey,
    logFilterStartBlockNumber
  }) => {
    const startingRangeEndTimestamp = await this.db.transaction().execute(async (tx) => {
      const existingRanges = await tx.deleteFrom("logFilterCachedRanges").where("filterKey", "=", logFilterKey).returningAll().execute();
      const mergedIntervals = mergeIntervals(
        existingRanges.map((r) => [
          Number(blobToBigInt(r.startBlock)),
          Number(blobToBigInt(r.endBlock))
        ])
      );
      const mergedRanges = mergedIntervals.map((interval) => {
        const [startBlock, endBlock] = interval;
        const endBlockTimestamp = existingRanges.find(
          (r) => Number(blobToBigInt(r.endBlock)) === endBlock
        ).endBlockTimestamp;
        return {
          filterKey: logFilterKey,
          startBlock: intToBlob(startBlock),
          endBlock: intToBlob(endBlock),
          endBlockTimestamp
        };
      });
      if (mergedRanges.length > 0) {
        await tx.insertInto("logFilterCachedRanges").values(mergedRanges).execute();
      }
      const startingRange = mergedRanges.find(
        (range2) => Number(blobToBigInt(range2.startBlock)) <= logFilterStartBlockNumber && Number(blobToBigInt(range2.endBlock)) >= logFilterStartBlockNumber
      );
      if (!startingRange) {
        return 0;
      } else {
        return Number(blobToBigInt(startingRange.endBlockTimestamp));
      }
    });
    return { startingRangeEndTimestamp };
  };
  getLogFilterCachedRanges = async ({ filterKey }) => {
    const results = await this.db.selectFrom("logFilterCachedRanges").select(["filterKey", "startBlock", "endBlock", "endBlockTimestamp"]).where("filterKey", "=", filterKey).execute();
    return results.map((range2) => ({
      ...range2,
      startBlock: blobToBigInt(range2.startBlock),
      endBlock: blobToBigInt(range2.endBlock),
      endBlockTimestamp: blobToBigInt(range2.endBlockTimestamp)
    }));
  };
  insertContractReadResult = async ({
    address,
    blockNumber,
    chainId,
    data,
    finalized,
    result
  }) => {
    await this.db.insertInto("contractReadResults").values({
      address,
      blockNumber: intToBlob(blockNumber),
      chainId,
      data,
      finalized: finalized ? 1 : 0,
      result
    }).onConflict(
      (oc) => oc.constraint("contractReadResultPrimaryKey").doUpdateSet({ result })
    ).execute();
  };
  getContractReadResult = async ({
    address,
    blockNumber,
    chainId,
    data
  }) => {
    const contractReadResult = await this.db.selectFrom("contractReadResults").selectAll().where("address", "=", address).where("blockNumber", "=", intToBlob(blockNumber)).where("chainId", "=", chainId).where("data", "=", data).executeTakeFirst();
    return contractReadResult ? {
      ...contractReadResult,
      blockNumber: blobToBigInt(contractReadResult.blockNumber),
      finalized: contractReadResult.finalized === 1
    } : null;
  };
  getLogEvents = async ({
    fromTimestamp,
    toTimestamp,
    filters = []
  }) => {
    const handledLogQuery = this.db.with(
      "logFilters(logFilter_name, logFilter_chainId, logFilter_address, logFilter_topic0, logFilter_topic1, logFilter_topic2, logFilter_topic3, logFilter_fromBlock, logFilter_toBlock, logFilter_handledTopic0)",
      () => import_kysely2.sql`( values ${import_kysely2.sql.join(filters.map(buildLogFilterValues))} )`
    ).selectFrom("logs").leftJoin("blocks", "blocks.hash", "logs.blockHash").leftJoin("transactions", "transactions.hash", "logs.transactionHash").innerJoin("logFilters", (join) => join.onTrue()).select([
      "logFilter_name",
      "logs.address as log_address",
      "logs.blockHash as log_blockHash",
      "logs.blockNumber as log_blockNumber",
      // "logs.chainId as log_chainId",
      "logs.data as log_data",
      // "logs.finalized as log_finalized",
      "logs.id as log_id",
      "logs.logIndex as log_logIndex",
      "logs.topic0 as log_topic0",
      "logs.topic1 as log_topic1",
      "logs.topic2 as log_topic2",
      "logs.topic3 as log_topic3",
      "logs.transactionHash as log_transactionHash",
      "logs.transactionIndex as log_transactionIndex",
      "blocks.baseFeePerGas as block_baseFeePerGas",
      // "blocks.chainId as block_chainId",
      "blocks.difficulty as block_difficulty",
      "blocks.extraData as block_extraData",
      // "blocks.finalized as block_finalized",
      "blocks.gasLimit as block_gasLimit",
      "blocks.gasUsed as block_gasUsed",
      "blocks.hash as block_hash",
      "blocks.logsBloom as block_logsBloom",
      "blocks.miner as block_miner",
      "blocks.mixHash as block_mixHash",
      "blocks.nonce as block_nonce",
      "blocks.number as block_number",
      "blocks.parentHash as block_parentHash",
      "blocks.receiptsRoot as block_receiptsRoot",
      "blocks.sha3Uncles as block_sha3Uncles",
      "blocks.size as block_size",
      "blocks.stateRoot as block_stateRoot",
      "blocks.timestamp as block_timestamp",
      "blocks.totalDifficulty as block_totalDifficulty",
      "blocks.transactionsRoot as block_transactionsRoot",
      "transactions.accessList as tx_accessList",
      "transactions.blockHash as tx_blockHash",
      "transactions.blockNumber as tx_blockNumber",
      // "transactions.chainId as tx_chainId",
      // "transactions.finalized as tx_finalized",
      "transactions.from as tx_from",
      "transactions.gas as tx_gas",
      "transactions.gasPrice as tx_gasPrice",
      "transactions.hash as tx_hash",
      "transactions.input as tx_input",
      "transactions.maxFeePerGas as tx_maxFeePerGas",
      "transactions.maxPriorityFeePerGas as tx_maxPriorityFeePerGas",
      "transactions.nonce as tx_nonce",
      "transactions.r as tx_r",
      "transactions.s as tx_s",
      "transactions.to as tx_to",
      "transactions.transactionIndex as tx_transactionIndex",
      "transactions.type as tx_type",
      "transactions.value as tx_value",
      "transactions.v as tx_v"
    ]).where(
      ({ and, or, cmpr, ref }) => and([
        cmpr("logs.chainId", "=", ref("logFilter_chainId")),
        or([
          cmpr("logFilter_address", "is", null),
          cmpr("logFilter_address", "like", import_kysely2.sql`'%' || logs.address || '%'`)
        ]),
        and([
          or([
            cmpr("logFilter_topic0", "is", null),
            cmpr("logFilter_topic0", "like", import_kysely2.sql`'%' || logs.topic0 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic1", "is", null),
            cmpr("logFilter_topic1", "like", import_kysely2.sql`'%' || logs.topic1 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic2", "is", null),
            cmpr("logFilter_topic2", "like", import_kysely2.sql`'%' || logs.topic2 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic3", "is", null),
            cmpr("logFilter_topic3", "like", import_kysely2.sql`'%' || logs.topic3 || '%'`)
          ])
        ]),
        or([
          cmpr("logFilter_fromBlock", "is", null),
          cmpr("blocks.number", ">=", ref("logFilter_fromBlock"))
        ]),
        or([
          cmpr("logFilter_toBlock", "is", null),
          cmpr("blocks.number", "<=", ref("logFilter_toBlock"))
        ]),
        or([
          cmpr("logFilter_handledTopic0", "is", null),
          cmpr(
            "logFilter_handledTopic0",
            "like",
            import_kysely2.sql`'%' || logs.topic0 || '%'`
          )
        ])
      ])
    ).where("blocks.timestamp", ">=", intToBlob(fromTimestamp)).where("blocks.timestamp", "<=", intToBlob(toTimestamp)).orderBy("blocks.timestamp", "asc").orderBy("logs.chainId", "asc").orderBy("logs.logIndex", "asc").orderBy("logFilter_name", "asc");
    const totalLogCountQuery = this.db.with(
      "logFilters(logFilter_name, logFilter_chainId, logFilter_address, logFilter_topic0, logFilter_topic1, logFilter_topic2, logFilter_topic3, logFilter_fromBlock, logFilter_toBlock, logFilter_handledTopic0)",
      () => import_kysely2.sql`( values ${import_kysely2.sql.join(filters.map(buildLogFilterValues))} )`
    ).selectFrom("logs").leftJoin("blocks", "blocks.hash", "logs.blockHash").innerJoin("logFilters", (join) => join.onTrue()).select(this.db.fn.count("logs.id").as("log_count")).where(
      ({ and, or, cmpr, ref }) => and([
        cmpr("logs.chainId", "=", ref("logFilter_chainId")),
        or([
          cmpr("logFilter_address", "is", null),
          cmpr("logFilter_address", "like", import_kysely2.sql`'%' || logs.address || '%'`)
        ]),
        and([
          or([
            cmpr("logFilter_topic0", "is", null),
            cmpr("logFilter_topic0", "like", import_kysely2.sql`'%' || logs.topic0 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic1", "is", null),
            cmpr("logFilter_topic1", "like", import_kysely2.sql`'%' || logs.topic1 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic2", "is", null),
            cmpr("logFilter_topic2", "like", import_kysely2.sql`'%' || logs.topic2 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic3", "is", null),
            cmpr("logFilter_topic3", "like", import_kysely2.sql`'%' || logs.topic3 || '%'`)
          ])
        ]),
        or([
          cmpr("logFilter_fromBlock", "is", null),
          cmpr("blocks.number", ">=", ref("logFilter_fromBlock"))
        ]),
        or([
          cmpr("logFilter_toBlock", "is", null),
          cmpr("blocks.number", "<=", ref("logFilter_toBlock"))
        ])
      ])
    ).where("blocks.timestamp", ">=", intToBlob(fromTimestamp)).where("blocks.timestamp", "<=", intToBlob(toTimestamp));
    const handledLogs = await handledLogQuery.execute();
    const totalLogCount = await totalLogCountQuery.execute();
    const totalEventCount = Number(totalLogCount[0].log_count);
    const events = handledLogs.map((result_) => {
      const result = result_;
      const event = {
        filterName: result.logFilter_name,
        log: {
          address: result.log_address,
          blockHash: result.log_blockHash,
          blockNumber: blobToBigInt(result.log_blockNumber),
          data: result.log_data,
          id: result.log_id,
          logIndex: Number(result.log_logIndex),
          removed: false,
          topics: [
            result.log_topic0,
            result.log_topic1,
            result.log_topic2,
            result.log_topic3
          ].filter((t) => t !== null),
          transactionHash: result.log_transactionHash,
          transactionIndex: Number(result.log_transactionIndex)
        },
        block: {
          baseFeePerGas: result.block_baseFeePerGas ? blobToBigInt(result.block_baseFeePerGas) : null,
          difficulty: blobToBigInt(result.block_difficulty),
          extraData: result.block_extraData,
          gasLimit: blobToBigInt(result.block_gasLimit),
          gasUsed: blobToBigInt(result.block_gasUsed),
          hash: result.block_hash,
          logsBloom: result.block_logsBloom,
          miner: result.block_miner,
          mixHash: result.block_mixHash,
          nonce: result.block_nonce,
          number: blobToBigInt(result.block_number),
          parentHash: result.block_parentHash,
          receiptsRoot: result.block_receiptsRoot,
          sha3Uncles: result.block_sha3Uncles,
          size: blobToBigInt(result.block_size),
          stateRoot: result.block_stateRoot,
          timestamp: blobToBigInt(result.block_timestamp),
          totalDifficulty: blobToBigInt(result.block_totalDifficulty),
          transactionsRoot: result.block_transactionsRoot
        },
        transaction: {
          blockHash: result.tx_blockHash,
          blockNumber: blobToBigInt(result.tx_blockNumber),
          from: result.tx_from,
          gas: blobToBigInt(result.tx_gas),
          hash: result.tx_hash,
          input: result.tx_input,
          nonce: Number(result.tx_nonce),
          r: result.tx_r,
          s: result.tx_s,
          to: result.tx_to,
          transactionIndex: Number(result.tx_transactionIndex),
          value: blobToBigInt(result.tx_value),
          v: blobToBigInt(result.tx_v),
          ...result.tx_type === "0x0" ? {
            type: "legacy",
            gasPrice: blobToBigInt(result.tx_gasPrice)
          } : result.tx_type === "0x1" ? {
            type: "eip2930",
            gasPrice: blobToBigInt(result.tx_gasPrice),
            accessList: JSON.parse(result.tx_accessList)
          } : result.tx_type === "0x2" ? {
            type: "eip1559",
            maxFeePerGas: blobToBigInt(result.tx_maxFeePerGas),
            maxPriorityFeePerGas: blobToBigInt(
              result.tx_maxPriorityFeePerGas
            )
          } : result.tx_type === "0x7e" ? {
            type: "deposit",
            maxFeePerGas: blobToBigInt(result.tx_maxFeePerGas),
            maxPriorityFeePerGas: blobToBigInt(
              result.tx_maxPriorityFeePerGas
            )
          } : {
            type: result.tx_type
          }
        }
      };
      return event;
    });
    return {
      events,
      totalEventCount
    };
  };
};
function getLogFilterAddressOrTopic(value) {
  if (value === void 0 || value === null)
    return null;
  if (typeof value === "string")
    return value;
  return value.join(",");
}
function getLogFilterTopics(topics) {
  if (!topics)
    return [null, null, null, null];
  const topic0 = getLogFilterAddressOrTopic(topics[0]);
  const topic1 = getLogFilterAddressOrTopic(topics[1]);
  const topic2 = getLogFilterAddressOrTopic(topics[2]);
  const topic3 = getLogFilterAddressOrTopic(topics[3]);
  return [topic0, topic1, topic2, topic3];
}
function buildLogFilterValues(filter) {
  const { name, chainId, address, topics, fromBlock, toBlock, handledTopic0 } = filter;
  const address_ = getLogFilterAddressOrTopic(address);
  const [topic0, topic1, topic2, topic3] = getLogFilterTopics(topics);
  const handledTopic0_ = getLogFilterAddressOrTopic(handledTopic0);
  return import_kysely2.sql`(${import_kysely2.sql.join([
    import_kysely2.sql.val(name),
    import_kysely2.sql`${import_kysely2.sql.val(chainId)}::integer`,
    import_kysely2.sql.val(address_),
    import_kysely2.sql.val(topic0),
    import_kysely2.sql.val(topic1),
    import_kysely2.sql.val(topic2),
    import_kysely2.sql.val(topic3),
    import_kysely2.sql`${import_kysely2.sql.val(fromBlock ? intToBlob(fromBlock) : null)}::bytea`,
    import_kysely2.sql`${import_kysely2.sql.val(toBlock ? intToBlob(toBlock) : null)}::bytea`,
    import_kysely2.sql.val(handledTopic0_)
  ])})`;
}

// src/event-store/sqlite/store.ts
var import_kysely3 = require("kysely");

// src/event-store/sqlite/format.ts
var import_viem5 = require("viem");
function rpcToSqliteBlock(block) {
  return {
    baseFeePerGas: block.baseFeePerGas ? intToBlob(block.baseFeePerGas) : null,
    difficulty: intToBlob(block.difficulty),
    extraData: block.extraData,
    gasLimit: intToBlob(block.gasLimit),
    gasUsed: intToBlob(block.gasUsed),
    hash: block.hash,
    logsBloom: block.logsBloom,
    miner: block.miner,
    mixHash: block.mixHash,
    nonce: block.nonce,
    number: intToBlob(block.number),
    parentHash: block.parentHash,
    receiptsRoot: block.receiptsRoot,
    sha3Uncles: block.sha3Uncles,
    size: intToBlob(block.size),
    stateRoot: block.stateRoot,
    timestamp: intToBlob(block.timestamp),
    totalDifficulty: intToBlob(block.totalDifficulty),
    transactionsRoot: block.transactionsRoot
  };
}
function rpcToSqliteTransaction(transaction) {
  return {
    accessList: transaction.accessList ? JSON.stringify(transaction.accessList) : void 0,
    blockHash: transaction.blockHash,
    blockNumber: intToBlob(transaction.blockNumber),
    from: transaction.from,
    gas: intToBlob(transaction.gas),
    gasPrice: transaction.gasPrice ? intToBlob(transaction.gasPrice) : null,
    hash: transaction.hash,
    input: transaction.input,
    maxFeePerGas: transaction.maxFeePerGas ? intToBlob(transaction.maxFeePerGas) : null,
    maxPriorityFeePerGas: transaction.maxPriorityFeePerGas ? intToBlob(transaction.maxPriorityFeePerGas) : null,
    nonce: (0, import_viem5.hexToNumber)(transaction.nonce),
    r: transaction.r,
    s: transaction.s,
    to: transaction.to ? transaction.to : null,
    transactionIndex: Number(transaction.transactionIndex),
    type: transaction.type ?? "0x0",
    value: intToBlob(transaction.value),
    v: intToBlob(transaction.v)
  };
}
function rpcToSqliteLog({
  log
}) {
  return {
    address: log.address,
    blockHash: log.blockHash,
    blockNumber: intToBlob(log.blockNumber),
    data: log.data,
    id: `${log.blockHash}-${log.logIndex}`,
    logIndex: Number(log.logIndex),
    topic0: log.topics[0] ? log.topics[0] : null,
    topic1: log.topics[1] ? log.topics[1] : null,
    topic2: log.topics[2] ? log.topics[2] : null,
    topic3: log.topics[3] ? log.topics[3] : null,
    transactionHash: log.transactionHash,
    transactionIndex: Number(log.transactionIndex)
  };
}

// src/event-store/sqlite/migrations.ts
var migrations2 = {
  ["2023_05_15_0_initial"]: {
    async up(db) {
      await db.schema.createTable("blocks").addColumn("baseFeePerGas", "blob").addColumn("chainId", "integer", (col) => col.notNull()).addColumn("difficulty", "blob", (col) => col.notNull()).addColumn("extraData", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("gasLimit", "blob", (col) => col.notNull()).addColumn("gasUsed", "blob", (col) => col.notNull()).addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("logsBloom", "text", (col) => col.notNull()).addColumn("miner", "text", (col) => col.notNull()).addColumn("mixHash", "text", (col) => col.notNull()).addColumn("nonce", "text", (col) => col.notNull()).addColumn("number", "blob", (col) => col.notNull()).addColumn("parentHash", "text", (col) => col.notNull()).addColumn("receiptsRoot", "text", (col) => col.notNull()).addColumn("sha3Uncles", "text", (col) => col.notNull()).addColumn("size", "blob", (col) => col.notNull()).addColumn("stateRoot", "text", (col) => col.notNull()).addColumn("timestamp", "blob", (col) => col.notNull()).addColumn("totalDifficulty", "blob", (col) => col.notNull()).addColumn("transactionsRoot", "text", (col) => col.notNull()).execute();
      await db.schema.createTable("transactions").addColumn("accessList", "text").addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", "blob", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("from", "text", (col) => col.notNull()).addColumn("gas", "blob", (col) => col.notNull()).addColumn("gasPrice", "blob").addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("input", "text", (col) => col.notNull()).addColumn("maxFeePerGas", "blob").addColumn("maxPriorityFeePerGas", "blob").addColumn("nonce", "integer", (col) => col.notNull()).addColumn("r", "text", (col) => col.notNull()).addColumn("s", "text", (col) => col.notNull()).addColumn("to", "text").addColumn("transactionIndex", "integer", (col) => col.notNull()).addColumn("type", "text", (col) => col.notNull()).addColumn("value", "blob", (col) => col.notNull()).addColumn("v", "blob", (col) => col.notNull()).execute();
      await db.schema.createTable("logs").addColumn("address", "text", (col) => col.notNull()).addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", "blob", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("logIndex", "integer", (col) => col.notNull()).addColumn("topic0", "text").addColumn("topic1", "text").addColumn("topic2", "text").addColumn("topic3", "text").addColumn("transactionHash", "text", (col) => col.notNull()).addColumn("transactionIndex", "integer", (col) => col.notNull()).execute();
      await db.schema.createTable("contractReadResults").addColumn("address", "text", (col) => col.notNull()).addColumn("blockNumber", "blob", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("result", "text", (col) => col.notNull()).addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
        "chainId",
        "blockNumber",
        "address",
        "data"
      ]).execute();
      await db.schema.createTable("logFilterCachedRanges").addColumn("endBlock", "blob", (col) => col.notNull()).addColumn("endBlockTimestamp", "blob", (col) => col.notNull()).addColumn("filterKey", "text", (col) => col.notNull()).addColumn("id", "integer", (col) => col.notNull().primaryKey()).addColumn("startBlock", "blob", (col) => col.notNull()).execute();
    },
    async down(db) {
      await db.schema.dropTable("blocks").execute();
      await db.schema.dropTable("logs").execute();
      await db.schema.dropTable("transactions").execute();
      await db.schema.dropTable("contractReadResults").execute();
      await db.schema.dropTable("logFilterCachedRanges").execute();
    }
  },
  ["2023_06_20_0_indices"]: {
    async up(db) {
      await db.schema.createIndex("log_events_index").on("logs").columns(["address", "chainId", "blockHash"]).execute();
      await db.schema.createIndex("blocks_index").on("blocks").columns(["timestamp", "number"]).execute();
      await db.schema.createIndex("logFilterCachedRanges_index").on("logFilterCachedRanges").columns(["filterKey"]).execute();
    },
    async down(db) {
      await db.schema.dropIndex("log_events_index").execute();
      await db.schema.dropIndex("logFilterCachedRanges_index").execute();
      await db.schema.dropIndex("blocks_index").execute();
    }
  }
};
var StaticMigrationProvider2 = class {
  async getMigrations() {
    return migrations2;
  }
};
var migrationProvider2 = new StaticMigrationProvider2();

// src/event-store/sqlite/store.ts
var SqliteEventStore = class {
  db;
  migrator;
  constructor({ db }) {
    this.db = new import_kysely3.Kysely({
      dialect: new import_kysely3.SqliteDialect({ database: db })
    });
    this.migrator = new import_kysely3.Migrator({
      db: this.db,
      provider: migrationProvider2
    });
  }
  migrateUp = async () => {
    const { error } = await this.migrator.migrateToLatest();
    if (error)
      throw error;
  };
  migrateDown = async () => {
    const { error } = await this.migrator.migrateTo(import_kysely3.NO_MIGRATIONS);
    if (error)
      throw error;
  };
  insertUnfinalizedBlock = async ({
    chainId,
    block: rpcBlock,
    transactions: rpcTransactions,
    logs: rpcLogs
  }) => {
    const block = {
      ...rpcToSqliteBlock(rpcBlock),
      chainId,
      finalized: 0
    };
    const transactions = rpcTransactions.map(
      (transaction) => ({
        ...rpcToSqliteTransaction(transaction),
        chainId,
        finalized: 0
      })
    );
    const logs = rpcLogs.map((log) => ({
      ...rpcToSqliteLog({ log }),
      chainId,
      finalized: 0
    }));
    await this.db.transaction().execute(async (tx) => {
      await Promise.all([
        tx.insertInto("blocks").values(block).onConflict((oc) => oc.column("hash").doNothing()).execute(),
        ...transactions.map(
          (transaction) => tx.insertInto("transactions").values(transaction).onConflict((oc) => oc.column("hash").doNothing()).execute()
        ),
        ...logs.map(
          (log) => tx.insertInto("logs").values(log).onConflict((oc) => oc.column("id").doNothing()).execute()
        )
      ]);
    });
  };
  deleteUnfinalizedData = async ({
    chainId,
    fromBlockNumber
  }) => {
    await this.db.transaction().execute(async (tx) => {
      await tx.deleteFrom("blocks").where("number", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("transactions").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("logs").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
      await tx.deleteFrom("contractReadResults").where("blockNumber", ">=", intToBlob(fromBlockNumber)).where("finalized", "=", 0).where("chainId", "=", chainId).execute();
    });
  };
  finalizeData = async ({
    chainId,
    toBlockNumber
  }) => {
    await this.db.transaction().execute(async (tx) => {
      await tx.updateTable("blocks").set({ finalized: 1 }).where("number", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("transactions").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("logs").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
      await tx.updateTable("contractReadResults").set({ finalized: 1 }).where("blockNumber", "<=", intToBlob(toBlockNumber)).where("chainId", "=", chainId).execute();
    });
  };
  insertFinalizedLogs = async ({
    chainId,
    logs: rpcLogs
  }) => {
    const logs = rpcLogs.map((log) => ({
      ...rpcToSqliteLog({ log }),
      chainId,
      finalized: 1
    }));
    await Promise.all(
      logs.map(
        (log) => this.db.insertInto("logs").values(log).onConflict((oc) => oc.column("id").doNothing()).execute()
      )
    );
  };
  insertFinalizedBlock = async ({
    chainId,
    block: rpcBlock,
    transactions: rpcTransactions,
    logFilterRange: { logFilterKey, blockNumberToCacheFrom }
  }) => {
    const block = {
      ...rpcToSqliteBlock(rpcBlock),
      chainId,
      finalized: 1
    };
    const transactions = rpcTransactions.map(
      (transaction) => ({
        ...rpcToSqliteTransaction(transaction),
        chainId,
        finalized: 1
      })
    );
    const logFilterCachedRange = {
      filterKey: logFilterKey,
      startBlock: intToBlob(blockNumberToCacheFrom),
      endBlock: block.number,
      endBlockTimestamp: block.timestamp
    };
    await this.db.transaction().execute(async (tx) => {
      await Promise.all([
        tx.insertInto("blocks").values(block).onConflict((oc) => oc.column("hash").doNothing()).execute(),
        ...transactions.map(
          (transaction) => tx.insertInto("transactions").values(transaction).onConflict((oc) => oc.column("hash").doNothing()).execute()
        ),
        tx.insertInto("logFilterCachedRanges").values(logFilterCachedRange).execute()
      ]);
    });
  };
  mergeLogFilterCachedRanges = async ({
    logFilterKey,
    logFilterStartBlockNumber
  }) => {
    const startingRangeEndTimestamp = await this.db.transaction().execute(async (tx) => {
      const existingRanges = await tx.deleteFrom("logFilterCachedRanges").where("filterKey", "=", logFilterKey).returningAll().execute();
      const mergedIntervals = mergeIntervals(
        existingRanges.map((r) => [
          Number(blobToBigInt(r.startBlock)),
          Number(blobToBigInt(r.endBlock))
        ])
      );
      const mergedRanges = mergedIntervals.map((interval) => {
        const [startBlock, endBlock] = interval;
        const endBlockTimestamp = existingRanges.find(
          (r) => Number(blobToBigInt(r.endBlock)) === endBlock
        ).endBlockTimestamp;
        return {
          filterKey: logFilterKey,
          startBlock: intToBlob(startBlock),
          endBlock: intToBlob(endBlock),
          endBlockTimestamp
        };
      });
      await Promise.all(
        mergedRanges.map(
          (range2) => tx.insertInto("logFilterCachedRanges").values(range2).execute()
        )
      );
      const startingRange = mergedRanges.find(
        (range2) => Number(blobToBigInt(range2.startBlock)) <= logFilterStartBlockNumber && Number(blobToBigInt(range2.endBlock)) >= logFilterStartBlockNumber
      );
      if (!startingRange) {
        return 0;
      } else {
        return Number(blobToBigInt(startingRange.endBlockTimestamp));
      }
    });
    return { startingRangeEndTimestamp };
  };
  getLogFilterCachedRanges = async ({ filterKey }) => {
    const results = await this.db.selectFrom("logFilterCachedRanges").select(["filterKey", "startBlock", "endBlock", "endBlockTimestamp"]).where("filterKey", "=", filterKey).execute();
    return results.map((range2) => ({
      ...range2,
      startBlock: blobToBigInt(range2.startBlock),
      endBlock: blobToBigInt(range2.endBlock),
      endBlockTimestamp: blobToBigInt(range2.endBlockTimestamp)
    }));
  };
  insertContractReadResult = async ({
    address,
    blockNumber,
    chainId,
    data,
    finalized,
    result
  }) => {
    await this.db.insertInto("contractReadResults").values({
      address,
      blockNumber: intToBlob(blockNumber),
      chainId,
      data,
      finalized: finalized ? 1 : 0,
      result
    }).onConflict((oc) => oc.doUpdateSet({ result })).execute();
  };
  getContractReadResult = async ({
    address,
    blockNumber,
    chainId,
    data
  }) => {
    const contractReadResult = await this.db.selectFrom("contractReadResults").selectAll().where("address", "=", address).where("blockNumber", "=", intToBlob(blockNumber)).where("chainId", "=", chainId).where("data", "=", data).executeTakeFirst();
    return contractReadResult ? {
      ...contractReadResult,
      blockNumber: blobToBigInt(contractReadResult.blockNumber),
      finalized: contractReadResult.finalized === 1
    } : null;
  };
  getLogEvents = async ({
    fromTimestamp,
    toTimestamp,
    filters = []
  }) => {
    const handledLogQuery = this.db.with(
      "logFilters(logFilter_name, logFilter_chainId, logFilter_address, logFilter_topic0, logFilter_topic1, logFilter_topic2, logFilter_topic3, logFilter_fromBlock, logFilter_toBlock, logFilter_handledTopic0)",
      () => import_kysely3.sql`( values ${import_kysely3.sql.join(filters.map(buildLogFilterValues2))} )`
    ).selectFrom("logs").leftJoin("blocks", "blocks.hash", "logs.blockHash").leftJoin("transactions", "transactions.hash", "logs.transactionHash").innerJoin("logFilters", (join) => join.onTrue()).select([
      "logFilter_name",
      "logs.address as log_address",
      "logs.blockHash as log_blockHash",
      "logs.blockNumber as log_blockNumber",
      // "logs.chainId as log_chainId",
      "logs.data as log_data",
      // "logs.finalized as log_finalized",
      "logs.id as log_id",
      "logs.logIndex as log_logIndex",
      "logs.topic0 as log_topic0",
      "logs.topic1 as log_topic1",
      "logs.topic2 as log_topic2",
      "logs.topic3 as log_topic3",
      "logs.transactionHash as log_transactionHash",
      "logs.transactionIndex as log_transactionIndex",
      "blocks.baseFeePerGas as block_baseFeePerGas",
      // "blocks.chainId as block_chainId",
      "blocks.difficulty as block_difficulty",
      "blocks.extraData as block_extraData",
      // "blocks.finalized as block_finalized",
      "blocks.gasLimit as block_gasLimit",
      "blocks.gasUsed as block_gasUsed",
      "blocks.hash as block_hash",
      "blocks.logsBloom as block_logsBloom",
      "blocks.miner as block_miner",
      "blocks.mixHash as block_mixHash",
      "blocks.nonce as block_nonce",
      "blocks.number as block_number",
      "blocks.parentHash as block_parentHash",
      "blocks.receiptsRoot as block_receiptsRoot",
      "blocks.sha3Uncles as block_sha3Uncles",
      "blocks.size as block_size",
      "blocks.stateRoot as block_stateRoot",
      "blocks.timestamp as block_timestamp",
      "blocks.totalDifficulty as block_totalDifficulty",
      "blocks.transactionsRoot as block_transactionsRoot",
      "transactions.accessList as tx_accessList",
      "transactions.blockHash as tx_blockHash",
      "transactions.blockNumber as tx_blockNumber",
      // "transactions.chainId as tx_chainId",
      // "transactions.finalized as tx_finalized",
      "transactions.from as tx_from",
      "transactions.gas as tx_gas",
      "transactions.gasPrice as tx_gasPrice",
      "transactions.hash as tx_hash",
      "transactions.input as tx_input",
      "transactions.maxFeePerGas as tx_maxFeePerGas",
      "transactions.maxPriorityFeePerGas as tx_maxPriorityFeePerGas",
      "transactions.nonce as tx_nonce",
      "transactions.r as tx_r",
      "transactions.s as tx_s",
      "transactions.to as tx_to",
      "transactions.transactionIndex as tx_transactionIndex",
      "transactions.type as tx_type",
      "transactions.value as tx_value",
      "transactions.v as tx_v"
    ]).where(
      ({ and, or, cmpr, ref }) => and([
        cmpr("logs.chainId", "=", ref("logFilter_chainId")),
        or([
          cmpr("logFilter_address", "is", null),
          cmpr("logFilter_address", "like", import_kysely3.sql`'%' || logs.address || '%'`)
        ]),
        and([
          or([
            cmpr("logFilter_topic0", "is", null),
            cmpr("logFilter_topic0", "like", import_kysely3.sql`'%' || logs.topic0 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic1", "is", null),
            cmpr("logFilter_topic1", "like", import_kysely3.sql`'%' || logs.topic1 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic2", "is", null),
            cmpr("logFilter_topic2", "like", import_kysely3.sql`'%' || logs.topic2 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic3", "is", null),
            cmpr("logFilter_topic3", "like", import_kysely3.sql`'%' || logs.topic3 || '%'`)
          ])
        ]),
        or([
          cmpr("logFilter_fromBlock", "is", null),
          cmpr("blocks.number", ">=", ref("logFilter_fromBlock"))
        ]),
        or([
          cmpr("logFilter_toBlock", "is", null),
          cmpr("blocks.number", "<=", ref("logFilter_toBlock"))
        ]),
        or([
          cmpr("logFilter_handledTopic0", "is", null),
          cmpr(
            "logFilter_handledTopic0",
            "like",
            import_kysely3.sql`'%' || logs.topic0 || '%'`
          )
        ])
      ])
    ).where("blocks.timestamp", ">=", intToBlob(fromTimestamp)).where("blocks.timestamp", "<=", intToBlob(toTimestamp)).orderBy("blocks.timestamp", "asc").orderBy("logs.chainId", "asc").orderBy("logs.logIndex", "asc").orderBy("logFilter_name", "asc");
    const totalLogCountQuery = this.db.with(
      "logFilters(logFilter_name, logFilter_chainId, logFilter_address, logFilter_topic0, logFilter_topic1, logFilter_topic2, logFilter_topic3, logFilter_fromBlock, logFilter_toBlock, logFilter_handledTopic0)",
      () => import_kysely3.sql`( values ${import_kysely3.sql.join(filters.map(buildLogFilterValues2))} )`
    ).selectFrom("logs").leftJoin("blocks", "blocks.hash", "logs.blockHash").innerJoin("logFilters", (join) => join.onTrue()).select(this.db.fn.count("logs.id").as("log_count")).where(
      ({ and, or, cmpr, ref }) => and([
        cmpr("logs.chainId", "=", ref("logFilter_chainId")),
        or([
          cmpr("logFilter_address", "is", null),
          cmpr("logFilter_address", "like", import_kysely3.sql`'%' || logs.address || '%'`)
        ]),
        and([
          or([
            cmpr("logFilter_topic0", "is", null),
            cmpr("logFilter_topic0", "like", import_kysely3.sql`'%' || logs.topic0 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic1", "is", null),
            cmpr("logFilter_topic1", "like", import_kysely3.sql`'%' || logs.topic1 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic2", "is", null),
            cmpr("logFilter_topic2", "like", import_kysely3.sql`'%' || logs.topic2 || '%'`)
          ]),
          or([
            cmpr("logFilter_topic3", "is", null),
            cmpr("logFilter_topic3", "like", import_kysely3.sql`'%' || logs.topic3 || '%'`)
          ])
        ]),
        or([
          cmpr("logFilter_fromBlock", "is", null),
          cmpr("blocks.number", ">=", ref("logFilter_fromBlock"))
        ]),
        or([
          cmpr("logFilter_toBlock", "is", null),
          cmpr("blocks.number", "<=", ref("logFilter_toBlock"))
        ])
      ])
    ).where("blocks.timestamp", ">=", intToBlob(fromTimestamp)).where("blocks.timestamp", "<=", intToBlob(toTimestamp));
    const handledLogs = await handledLogQuery.execute();
    const totalLogCount = await totalLogCountQuery.execute();
    const totalEventCount = Number(totalLogCount[0].log_count);
    const events = handledLogs.map((result_) => {
      const result = result_;
      const event = {
        filterName: result.logFilter_name,
        log: {
          address: result.log_address,
          blockHash: result.log_blockHash,
          blockNumber: blobToBigInt(result.log_blockNumber),
          data: result.log_data,
          id: result.log_id,
          logIndex: Number(result.log_logIndex),
          removed: false,
          topics: [
            result.log_topic0,
            result.log_topic1,
            result.log_topic2,
            result.log_topic3
          ].filter((t) => t !== null),
          transactionHash: result.log_transactionHash,
          transactionIndex: Number(result.log_transactionIndex)
        },
        block: {
          baseFeePerGas: result.block_baseFeePerGas ? blobToBigInt(result.block_baseFeePerGas) : null,
          difficulty: blobToBigInt(result.block_difficulty),
          extraData: result.block_extraData,
          gasLimit: blobToBigInt(result.block_gasLimit),
          gasUsed: blobToBigInt(result.block_gasUsed),
          hash: result.block_hash,
          logsBloom: result.block_logsBloom,
          miner: result.block_miner,
          mixHash: result.block_mixHash,
          nonce: result.block_nonce,
          number: blobToBigInt(result.block_number),
          parentHash: result.block_parentHash,
          receiptsRoot: result.block_receiptsRoot,
          sha3Uncles: result.block_sha3Uncles,
          size: blobToBigInt(result.block_size),
          stateRoot: result.block_stateRoot,
          timestamp: blobToBigInt(result.block_timestamp),
          totalDifficulty: blobToBigInt(result.block_totalDifficulty),
          transactionsRoot: result.block_transactionsRoot
        },
        transaction: {
          blockHash: result.tx_blockHash,
          blockNumber: blobToBigInt(result.tx_blockNumber),
          from: result.tx_from,
          gas: blobToBigInt(result.tx_gas),
          hash: result.tx_hash,
          input: result.tx_input,
          nonce: Number(result.tx_nonce),
          r: result.tx_r,
          s: result.tx_s,
          to: result.tx_to,
          transactionIndex: Number(result.tx_transactionIndex),
          value: blobToBigInt(result.tx_value),
          v: blobToBigInt(result.tx_v),
          ...result.tx_type === "0x0" ? {
            type: "legacy",
            gasPrice: blobToBigInt(result.tx_gasPrice)
          } : result.tx_type === "0x1" ? {
            type: "eip2930",
            gasPrice: blobToBigInt(result.tx_gasPrice),
            accessList: JSON.parse(result.tx_accessList)
          } : result.tx_type === "0x2" ? {
            type: "eip1559",
            maxFeePerGas: blobToBigInt(result.tx_maxFeePerGas),
            maxPriorityFeePerGas: blobToBigInt(
              result.tx_maxPriorityFeePerGas
            )
          } : result.tx_type === "0x7e" ? {
            type: "deposit",
            maxFeePerGas: blobToBigInt(result.tx_maxFeePerGas),
            maxPriorityFeePerGas: blobToBigInt(
              result.tx_maxPriorityFeePerGas
            )
          } : {
            type: result.tx_type
          }
        }
      };
      return event;
    });
    return {
      events,
      totalEventCount
    };
  };
};
function getLogFilterAddressOrTopic2(value) {
  if (value === void 0 || value === null)
    return null;
  if (typeof value === "string")
    return value;
  return value.join(",");
}
function getLogFilterTopics2(topics) {
  if (!topics)
    return [null, null, null, null];
  const topic0 = getLogFilterAddressOrTopic2(topics[0]);
  const topic1 = getLogFilterAddressOrTopic2(topics[1]);
  const topic2 = getLogFilterAddressOrTopic2(topics[2]);
  const topic3 = getLogFilterAddressOrTopic2(topics[3]);
  return [topic0, topic1, topic2, topic3];
}
function buildLogFilterValues2(filter) {
  const { name, chainId, address, topics, fromBlock, toBlock, handledTopic0 } = filter;
  const address_ = getLogFilterAddressOrTopic2(address);
  const [topic0, topic1, topic2, topic3] = getLogFilterTopics2(topics);
  const handledTopic0_ = getLogFilterAddressOrTopic2(handledTopic0);
  return import_kysely3.sql`(${import_kysely3.sql.join([
    import_kysely3.sql.val(name),
    import_kysely3.sql`cast (${import_kysely3.sql.val(chainId)} as integer)`,
    import_kysely3.sql.val(address_),
    import_kysely3.sql.val(topic0),
    import_kysely3.sql.val(topic1),
    import_kysely3.sql.val(topic2),
    import_kysely3.sql.val(topic3),
    import_kysely3.sql`cast (${import_kysely3.sql.val(fromBlock ? intToBlob(fromBlock) : null)} as blob)`,
    import_kysely3.sql`cast (${import_kysely3.sql.val(toBlock ? intToBlob(toBlock) : null)} as blob)`,
    import_kysely3.sql.val(handledTopic0_)
  ])})`;
}

// src/historical-sync/service.ts
var import_emittery5 = __toESM(require("emittery"));
var import_viem6 = require("viem");

// src/errors/queue.ts
var QueueError = class extends BaseError {
  name = "QueueError";
  constructor({
    queueName,
    task,
    cause
  }) {
    const metaMessages = [];
    metaMessages.push(`Task:
${prettyPrint(task)}`);
    if (cause.stack)
      metaMessages.push(`Stack: ${cause.stack}`);
    const shortMessage = `${queueName} error`;
    super(shortMessage, {
      metaMessages
    });
  }
};

// src/utils/format.ts
var formatEta = (ms) => {
  if (ms < 1e3)
    return `${Math.round(ms)}ms`;
  const seconds = Math.floor(ms / 1e3);
  const h = Math.floor(seconds / 3600);
  const m = Math.floor((seconds - h * 3600) / 60);
  const s = seconds - h * 3600 - m * 60;
  const hstr = h > 0 ? `${h}h ` : "";
  const mstr = m > 0 || h > 0 ? `${m}m ` : "";
  const sstr = s > 0 || m > 0 ? `${s}s` : "";
  return `${hstr}${mstr}${sstr}`;
};
var formatPercentage = (cacheRate) => {
  const decimal = Math.round(cacheRate * 1e3) / 10;
  return Number.isInteger(decimal) && decimal < 100 ? `${decimal}.0%` : `${decimal}%`;
};

// src/utils/queue.ts
var import_p_queue = __toESM(require("p-queue"));
var import_retry = __toESM(require("retry"));
var import_promises = require("timers/promises");
function createQueue({
  worker,
  context,
  options,
  onAdd,
  onComplete,
  onError,
  onIdle
}) {
  const queue = new import_p_queue.default(options);
  if (onIdle) {
    queue.on("idle", () => onIdle());
  }
  const controller = new AbortController();
  const signal = controller.signal;
  const superClear = queue.clear.bind(queue);
  queue.clear = () => {
    controller.abort();
    superClear();
  };
  const retryTimeouts = import_retry.default.timeouts(
    options?.retryTimeoutOptions ?? {
      retries: 3,
      factor: 2,
      minTimeout: 100
      // 100 ms
    }
  );
  queue.addTask = async (task, taskOptions) => {
    const priority = taskOptions?.priority ?? 0;
    let retryTimeout = void 0;
    if (taskOptions?.retry) {
      task._retryCount ||= 0;
      task._retryCount += 1;
      if (task._retryCount > retryTimeouts.length) {
        console.log("too many retries!!!");
        return;
      } else {
        retryTimeout = retryTimeouts[task._retryCount];
      }
    }
    onAdd?.({ task, context, queue });
    await queue.add(
      async () => {
        let result;
        if (retryTimeout)
          await (0, import_promises.setTimeout)(retryTimeout, false, { signal });
        try {
          result = await worker({ task, context, queue });
        } catch (error_) {
          await onError?.({ error: error_, task, context, queue });
          return;
        }
        await onComplete?.({ result, task, context, queue });
      },
      { priority }
    );
  };
  queue.addTasks = async (tasks, taskOptions) => {
    await Promise.all(
      tasks.map(async (task) => {
        const priority = taskOptions?.priority ?? 0;
        let retryTimeout = void 0;
        if (taskOptions?.retry) {
          task._retryCount ||= 0;
          task._retryCount += 1;
          if (task._retryCount > retryTimeouts.length) {
            console.log("too many retries!!!");
            return;
          } else {
            retryTimeout = retryTimeouts[task._retryCount];
          }
        }
        onAdd?.({ task, context, queue });
        await queue.add(
          async () => {
            let result;
            if (retryTimeout)
              await (0, import_promises.setTimeout)(retryTimeout, false, { signal });
            try {
              result = await worker({ task, context, queue });
            } catch (error_) {
              await onError?.({ error: error_, task, context, queue });
              return;
            }
            await onComplete?.({ result, task, context, queue });
          },
          { priority }
        );
      })
    );
  };
  return queue;
}

// src/utils/timer.ts
function startClock() {
  const start = process.hrtime();
  return () => hrTimeToMs(process.hrtime(start));
}
function hrTimeToMs(diff) {
  return Math.round(diff[0] * 1e3 + diff[1] / 1e6);
}

// src/historical-sync/intervals.ts
function remove_duplicates(arr) {
  const lookup = {};
  const results = [];
  for (let i = 0; i < arr.length; i++) {
    const el = arr[i];
    const key = el.toString();
    if (lookup[key])
      continue;
    lookup[key] = 1;
    results.push(el);
  }
  return results;
}
function p1_excluding_p2(p1, p2) {
  if (p1[1] < p2[0])
    return [p1];
  if (p1[0] > p2[1])
    return [p1];
  const lines = [];
  const line1 = [p1[0], Math.min(p1[1], p2[0] - 1)];
  if (line1[0] <= line1[1])
    lines.push(line1);
  const line2 = [p2[1] + 1, p1[1]];
  if (line2[0] <= line2[1])
    lines.push(line2);
  return lines;
}
function points_excluding_p2(points, p2) {
  const results = [];
  for (let i = 0; i < points.length; i++) {
    const lines = p1_excluding_p2(points[i], p2);
    results.push(...lines);
  }
  return results;
}
function findMissingIntervals(p1, exclude) {
  let checking = [p1];
  for (let i = 0; i < exclude.length; i++) {
    checking = points_excluding_p2(checking, exclude[i]);
  }
  return remove_duplicates(checking);
}

// src/historical-sync/service.ts
var HistoricalSyncService = class extends import_emittery5.default {
  resources;
  eventStore;
  logFilters;
  network;
  queue;
  logFilterCheckpoints = {};
  minimumLogFilterCheckpoint = 0;
  startTimestamp;
  killFunctions = [];
  constructor({
    resources,
    eventStore,
    logFilters,
    network
  }) {
    super();
    this.resources = resources;
    this.eventStore = eventStore;
    this.logFilters = logFilters;
    this.network = network;
    this.queue = this.buildQueue();
    logFilters.forEach((logFilter) => {
      this.logFilterCheckpoints[logFilter.name] = 0;
    });
    this.registerMetricCollectMethods();
  }
  async setup({ finalizedBlockNumber }) {
    await Promise.all(
      this.logFilters.map(async (logFilter) => {
        const { startBlock, endBlock: userDefinedEndBlock } = logFilter.filter;
        const endBlock = userDefinedEndBlock ?? finalizedBlockNumber;
        const maxBlockRange = logFilter.maxBlockRange ?? this.network.defaultMaxBlockRange;
        if (startBlock > endBlock) {
          throw new Error(
            `Start block number (${startBlock}) is greater than end block number (${endBlock}).
             Are you sure the RPC endpoint is for the correct network?`
          );
        }
        const cachedRanges = await this.eventStore.getLogFilterCachedRanges({
          filterKey: logFilter.filter.key
        });
        const requiredBlockRanges = findMissingIntervals(
          [startBlock, endBlock],
          cachedRanges.map((r) => [Number(r.startBlock), Number(r.endBlock)])
        );
        const totalBlockCount = endBlock - startBlock + 1;
        const requiredBlockCount = requiredBlockRanges.reduce(
          (acc, range2) => acc + range2[1] + 1 - range2[0],
          0
        );
        const cachedBlockCount = totalBlockCount - requiredBlockCount;
        const cacheRate = Math.min(
          1,
          cachedBlockCount / (totalBlockCount || 1)
        );
        this.resources.metrics.ponder_historical_total_blocks.set(
          {
            network: this.network.name,
            logFilter: logFilter.name
          },
          totalBlockCount
        );
        this.resources.metrics.ponder_historical_cached_blocks.set(
          {
            network: this.network.name,
            logFilter: logFilter.name
          },
          cachedBlockCount
        );
        this.resources.logger.info({
          service: "historical",
          msg: `Started sync with ${formatPercentage(
            cacheRate
          )} cached (network=${this.network.name})`,
          network: this.network.name,
          logFilter: logFilter.name,
          totalBlockCount,
          cacheRate
        });
        for (const blockRange of requiredBlockRanges) {
          const [startBlock2, endBlock2] = blockRange;
          let fromBlock = startBlock2;
          let toBlock = Math.min(fromBlock + maxBlockRange - 1, endBlock2);
          while (fromBlock <= endBlock2) {
            this.queue.addTask(
              {
                kind: "LOG_SYNC",
                logFilter,
                fromBlock,
                toBlock
              },
              {
                priority: Number.MAX_SAFE_INTEGER - fromBlock
              }
            );
            this.resources.metrics.ponder_historical_scheduled_tasks.inc({
              network: this.network.name,
              kind: "log"
            });
            fromBlock = toBlock + 1;
            toBlock = Math.min(fromBlock + maxBlockRange - 1, endBlock2);
          }
        }
      })
    );
  }
  start() {
    this.startTimestamp = process.hrtime();
    const updateLogInterval = setInterval(async () => {
      const completionStats = await this.getCompletionStats();
      completionStats.forEach(({ logFilter, rate, eta }) => {
        if (rate === 1)
          return;
        this.resources.logger.info({
          service: "historical",
          msg: `Sync is ${formatPercentage(rate)} complete${eta !== void 0 ? ` with ~${formatEta(eta)} remaining` : ""} (logFilter=${logFilter})`,
          network: this.network.name
        });
      });
    }, 1e4);
    this.killFunctions.push(() => {
      clearInterval(updateLogInterval);
    });
    if (this.queue.size === 0) {
      const now = Math.round(Date.now() / 1e3);
      this.emit("historicalCheckpoint", { timestamp: now });
      this.emit("syncComplete");
      this.resources.logger.info({
        service: "historical",
        msg: `Completed sync (network=${this.network.name})`,
        network: this.network.name
      });
    }
    this.queue.start();
  }
  kill = async () => {
    for (const fn of this.killFunctions) {
      await fn();
    }
    this.queue.pause();
    this.queue.clear();
    this.resources.logger.debug({
      service: "historical",
      msg: `Killed historical sync service (network=${this.network.name})`
    });
  };
  onIdle = async () => {
    await this.queue.onIdle();
  };
  buildQueue = () => {
    const worker = async ({
      task,
      queue: queue2
    }) => {
      if (task.kind === "LOG_SYNC") {
        await this.logTaskWorker({ task });
      } else {
        await this.blockTaskWorker({ task });
      }
      if (queue2.size > 0 || queue2.pending > 1)
        return;
      await Promise.all(
        this.logFilters.map(
          (logFilter) => this.updateHistoricalCheckpoint({ logFilter })
        )
      );
      this.emit("syncComplete");
      const duration = hrTimeToMs(process.hrtime(this.startTimestamp));
      this.resources.logger.info({
        service: "historical",
        msg: `Completed sync in ${formatEta(duration)} (network=${this.network.name})`,
        network: this.network.name,
        duration
      });
    };
    const queue = createQueue({
      worker,
      options: {
        concurrency: this.network.maxRpcRequestConcurrency,
        autoStart: false
      },
      onComplete: ({ task }) => {
        const { logFilter } = task;
        if (task.kind === "BLOCK_SYNC") {
          this.resources.metrics.ponder_historical_completed_tasks.inc({
            network: this.network.name,
            kind: "block",
            status: "success"
          });
          this.resources.logger.trace({
            service: "historical",
            msg: `Completed block sync task`,
            network: this.network.name,
            logFilter: logFilter.name,
            blockNumberToCacheFrom: task.blockNumberToCacheFrom,
            blockNumber: task.blockNumber,
            requiredTransactionCount: task.requiredTxHashes.size
          });
          this.resources.metrics.ponder_historical_completed_blocks.inc(
            {
              network: this.network.name,
              logFilter: logFilter.name
            },
            task.blockNumber - task.blockNumberToCacheFrom + 1
          );
        }
        if (task.kind === "LOG_SYNC") {
          this.resources.metrics.ponder_historical_completed_tasks.inc({
            network: this.network.name,
            kind: "log",
            status: "success"
          });
          this.resources.logger.trace({
            service: "historical",
            msg: `Completed log sync task`,
            network: this.network.name,
            logFilter: logFilter.name,
            fromBlock: task.fromBlock,
            toBlock: task.toBlock
          });
        }
      },
      onError: ({ error, task, queue: queue2 }) => {
        const { logFilter } = task;
        this.resources.metrics.ponder_historical_completed_tasks.inc({
          network: this.network.name,
          kind: task.kind === "LOG_SYNC" ? "log" : "block",
          status: "failure"
        });
        if (task.kind === "LOG_SYNC" && error instanceof import_viem6.InvalidParamsRpcError && error.details.startsWith("Log response size exceeded.")) {
          const safe = error.details.split("this block range should work: ")[1];
          const safeStart = Number(safe.split(", ")[0].slice(1));
          const safeEnd = Number(safe.split(", ")[1].slice(0, -1));
          queue2.addTask(
            { ...task, fromBlock: safeStart, toBlock: safeEnd },
            {
              priority: Number.MAX_SAFE_INTEGER - safeStart
            }
          );
          queue2.addTask(
            { ...task, fromBlock: safeEnd + 1 },
            { priority: Number.MAX_SAFE_INTEGER - safeEnd + 1 }
          );
          this.resources.metrics.ponder_historical_scheduled_tasks.inc({
            network: this.network.name,
            kind: "log"
          });
          return;
        }
        if (task.kind === "LOG_SYNC" && error instanceof import_viem6.InvalidParamsRpcError && error.details.includes("block range less than 20000")) {
          const midpoint = Math.floor(
            (task.toBlock - task.fromBlock) / 2 + task.fromBlock
          );
          queue2.addTask(
            { ...task, toBlock: midpoint },
            { priority: Number.MAX_SAFE_INTEGER - task.fromBlock }
          );
          queue2.addTask(
            { ...task, fromBlock: midpoint + 1 },
            { priority: Number.MAX_SAFE_INTEGER - midpoint + 1 }
          );
          this.resources.metrics.ponder_historical_scheduled_tasks.inc({
            network: this.network.name,
            kind: "log"
          });
          return;
        }
        if (task.kind === "LOG_SYNC" && error instanceof import_viem6.HttpRequestError && error.details.includes(
          "eth_getLogs and eth_newFilter are limited to a 10,000 blocks range"
        )) {
          const midpoint = Math.floor(
            (task.toBlock - task.fromBlock) / 2 + task.fromBlock
          );
          queue2.addTask(
            { ...task, toBlock: midpoint },
            { priority: Number.MAX_SAFE_INTEGER - task.fromBlock }
          );
          queue2.addTask(
            { ...task, fromBlock: midpoint + 1 },
            { priority: Number.MAX_SAFE_INTEGER - midpoint + 1 }
          );
          this.resources.metrics.ponder_historical_scheduled_tasks.inc({
            network: this.network.name,
            kind: "log"
          });
          return;
        }
        const queueError = new QueueError({
          queueName: "Historical sync queue",
          task: {
            logFilterName: task.logFilter.name,
            ...task,
            logFilter: void 0
          },
          cause: error
        });
        this.emit("error", { error: queueError });
        if (task.kind === "LOG_SYNC") {
          this.resources.logger.error({
            service: "historical",
            msg: `Log sync task failed (network=${this.network.name}, logFilter=${logFilter.name})`,
            error,
            network: this.network.name,
            logFilter: logFilter.name,
            fromBlock: task.fromBlock,
            toBlock: task.toBlock
          });
        }
        if (task.kind === "BLOCK_SYNC") {
          this.resources.logger.error({
            service: "historical",
            msg: `Block sync task failed (network=${this.network.name}, logFilter=${logFilter.name})`,
            error,
            network: this.network.name,
            logFilter: logFilter.name,
            blockNumberToCacheFrom: task.blockNumberToCacheFrom,
            blockNumber: task.blockNumber,
            requiredTransactionCount: task.requiredTxHashes.size
          });
        }
        const priority = Number.MAX_SAFE_INTEGER - (task.kind === "LOG_SYNC" ? task.fromBlock : task.blockNumberToCacheFrom);
        queue2.addTask(task, { priority, retry: true });
      }
    });
    return queue;
  };
  logTaskWorker = async ({ task }) => {
    const { logFilter, fromBlock, toBlock } = task;
    const stopClock = startClock();
    const logs = await this.network.client.request({
      method: "eth_getLogs",
      params: [
        {
          address: logFilter.filter.address,
          topics: logFilter.filter.topics,
          fromBlock: (0, import_viem6.toHex)(fromBlock),
          toBlock: (0, import_viem6.toHex)(toBlock)
        }
      ]
    });
    this.resources.metrics.ponder_historical_rpc_request_duration.observe(
      {
        method: "eth_getLogs",
        network: this.network.name
      },
      stopClock()
    );
    await this.eventStore.insertFinalizedLogs({
      chainId: this.network.chainId,
      logs
    });
    const txHashesByBlockNumber = logs.reduce(
      (acc, log) => {
        const blockNumber = Number(log.blockNumber);
        acc[blockNumber] ||= /* @__PURE__ */ new Set();
        acc[blockNumber].add(log.transactionHash);
        return acc;
      },
      {}
    );
    const requiredBlockNumbers = Object.keys(txHashesByBlockNumber).map(Number).sort((a, b) => a - b);
    let blockNumberToCacheFrom = fromBlock;
    const blockTasks = [];
    for (const blockNumber of requiredBlockNumbers) {
      blockTasks.push({
        kind: "BLOCK_SYNC",
        logFilter,
        blockNumberToCacheFrom,
        blockNumber,
        requiredTxHashes: txHashesByBlockNumber[blockNumber]
      });
      blockNumberToCacheFrom = blockNumber + 1;
    }
    if (blockNumberToCacheFrom <= toBlock) {
      blockTasks.push({
        kind: "BLOCK_SYNC",
        logFilter,
        blockNumberToCacheFrom,
        blockNumber: toBlock,
        requiredTxHashes: /* @__PURE__ */ new Set()
      });
    }
    for (const blockTask of blockTasks) {
      const priority = Number.MAX_SAFE_INTEGER - blockTask.blockNumberToCacheFrom;
      this.queue.addTask(blockTask, { priority });
    }
    this.resources.metrics.ponder_historical_scheduled_tasks.inc(
      {
        network: this.network.name,
        kind: "block"
      },
      blockTasks.length
    );
  };
  blockTaskWorker = async ({ task }) => {
    const { logFilter, blockNumber, blockNumberToCacheFrom, requiredTxHashes } = task;
    const stopClock = startClock();
    const block = await this.network.client.request({
      method: "eth_getBlockByNumber",
      params: [(0, import_viem6.toHex)(blockNumber), true]
    });
    this.resources.metrics.ponder_historical_rpc_request_duration.observe(
      {
        method: "eth_getBlockByNumber",
        network: this.network.name
      },
      stopClock()
    );
    if (!block)
      throw new Error(`Block not found: ${blockNumber}`);
    const transactions = block.transactions.filter(
      (tx) => requiredTxHashes.has(tx.hash)
    );
    await this.eventStore.insertFinalizedBlock({
      chainId: this.network.chainId,
      block,
      transactions,
      logFilterRange: {
        logFilterKey: logFilter.filter.key,
        blockNumberToCacheFrom
      }
    });
    await this.updateHistoricalCheckpoint({ logFilter });
  };
  updateHistoricalCheckpoint = async ({
    logFilter
  }) => {
    const { startingRangeEndTimestamp } = await this.eventStore.mergeLogFilterCachedRanges({
      logFilterKey: logFilter.filter.key,
      logFilterStartBlockNumber: logFilter.filter.startBlock
    });
    this.logFilterCheckpoints[logFilter.name] = Math.max(
      this.logFilterCheckpoints[logFilter.name],
      startingRangeEndTimestamp
    );
    const historicalCheckpoint = Math.min(
      ...Object.values(this.logFilterCheckpoints)
    );
    if (historicalCheckpoint > this.minimumLogFilterCheckpoint) {
      this.minimumLogFilterCheckpoint = historicalCheckpoint;
      this.emit("historicalCheckpoint", {
        timestamp: this.minimumLogFilterCheckpoint
      });
    }
  };
  getCompletionStats = async () => {
    const cachedBlocksMetric = (await this.resources.metrics.ponder_historical_cached_blocks.get()).values;
    const totalBlocksMetric = (await this.resources.metrics.ponder_historical_total_blocks.get()).values;
    const completedBlocksMetric = (await this.resources.metrics.ponder_historical_completed_blocks.get()).values;
    return this.logFilters.map(({ name }) => {
      const totalBlocks = totalBlocksMetric.find(
        (m) => m.labels.logFilter === name
      )?.value;
      const cachedBlocks = cachedBlocksMetric.find(
        (m) => m.labels.logFilter === name
      )?.value;
      const completedBlocks = completedBlocksMetric.find((m) => m.labels.logFilter === name)?.value ?? 0;
      if (totalBlocks === void 0 || totalBlocks === 0 || cachedBlocks === void 0 || !this.startTimestamp) {
        return { logFilter: name, rate: 0 };
      }
      const rate = (cachedBlocks + completedBlocks) / totalBlocks;
      if (completedBlocks < 3)
        return { logFilter: name, rate };
      if (rate === 1)
        return { logFilter: name, rate, eta: 0 };
      const elapsed = hrTimeToMs(process.hrtime(this.startTimestamp));
      const estimatedTotalDuration = elapsed / (completedBlocks / (totalBlocks - cachedBlocks));
      const estimatedTimeRemaining = estimatedTotalDuration - elapsed;
      return { logFilter: name, rate, eta: estimatedTimeRemaining };
    });
  };
  registerMetricCollectMethods = async () => {
    this.resources.metrics.ponder_historical_completion_rate.collect = async () => {
      const completionStats = await this.getCompletionStats();
      completionStats.forEach(({ logFilter, rate }) => {
        this.resources.metrics.ponder_historical_completion_rate.set(
          { logFilter, network: this.network.name },
          rate
        );
      });
    };
    this.resources.metrics.ponder_historical_completion_eta.collect = async () => {
      const completionStats = await this.getCompletionStats();
      completionStats.forEach(({ logFilter, eta }) => {
        if (eta) {
          this.resources.metrics.ponder_historical_completion_eta.set(
            { logFilter, network: this.network.name },
            eta
          );
        }
      });
    };
  };
};

// src/logs/service.ts
var import_node_path7 = __toESM(require("path"));
var import_picocolors = __toESM(require("picocolors"));
var import_pino = __toESM(require("pino"));
var LoggerService = class {
  logger;
  constructor({
    level = "info",
    dir
  } = {}) {
    const streams = [];
    if (level !== "silent") {
      streams.push({
        level,
        stream: {
          write(logString) {
            const log = JSON.parse(logString);
            const prettyLog = formatMessage(log);
            console.log(prettyLog);
            if (log.error?.stack)
              console.log(log.error.stack);
            if (log.error?.meta)
              console.log(log.error.meta);
          }
        }
      });
    }
    if (dir) {
      const logFile = import_node_path7.default.join(dir, `${(/* @__PURE__ */ new Date()).toISOString()}.log`);
      streams.push({
        level: "trace",
        stream: import_pino.default.destination({ dest: logFile, sync: false, mkdir: true })
      });
    }
    this.logger = (0, import_pino.default)(
      {
        level: "trace",
        serializers: { error: import_pino.default.stdSerializers.errWithCause }
      },
      import_pino.default.multistream(streams)
    );
  }
  fatal = (options) => {
    this.logger.fatal(options);
  };
  error = (options) => {
    this.logger.error(options);
  };
  warn = (options) => {
    this.logger.warn(options);
  };
  info = (options) => {
    this.logger.info(options);
  };
  debug = (options) => {
    this.logger.debug(options);
  };
  trace = (options) => {
    this.logger.trace(options);
  };
};
var levels = {
  60: { label: "FATAL", colorize: (s) => import_picocolors.default.bgRed(s) },
  50: { label: "ERROR", colorize: (s) => import_picocolors.default.red(s) },
  40: { label: "WARN ", colorize: (s) => import_picocolors.default.yellow(s) },
  30: { label: "INFO ", colorize: (s) => import_picocolors.default.green(s) },
  20: { label: "DEBUG", colorize: (s) => import_picocolors.default.blue(s) },
  10: { label: "TRACE", colorize: (s) => import_picocolors.default.gray(s) }
};
var formatMessage = (log) => {
  let result = "";
  const timestamp = log.time;
  const level = levels[log.level ?? 30];
  const msg = log.msg;
  const errorMessage = log.error?.message;
  const message = msg ?? errorMessage;
  const service = log.service;
  const date = new Date(timestamp);
  const hours = String(date.getUTCHours()).padStart(2, "0");
  const minutes = String(date.getUTCMinutes()).padStart(2, "0");
  const seconds = String(date.getUTCSeconds()).padStart(2, "0");
  const millis = String(date.getUTCMilliseconds()).padStart(3, "0");
  const time = `${hours}:${minutes}:${seconds}.${millis} `;
  result += import_picocolors.default.isColorSupported ? import_picocolors.default.gray(time) : time;
  result += import_picocolors.default.isColorSupported ? level.colorize(level.label) : level.label;
  if (service) {
    result += import_picocolors.default.isColorSupported ? " " + import_picocolors.default.cyan(service.padEnd(10, " ")) : " " + service.padEnd(10, " ");
  }
  result += import_picocolors.default.reset(` ${message}`);
  return result;
};

// src/metrics/service.ts
var import_prom_client = __toESM(require("prom-client"));
var httpRequestBucketsInMs = [
  5,
  10,
  20,
  30,
  40,
  50,
  60,
  70,
  80,
  90,
  100,
  150,
  200,
  250,
  300,
  350,
  400,
  450,
  500,
  750,
  1e3,
  2e3,
  1e4
];
var httpRequestSizeInBytes = [
  10,
  50,
  100,
  250,
  500,
  1e3,
  2500,
  5e3,
  1e4,
  5e4,
  1e5,
  25e4,
  5e5,
  1e6,
  5e6,
  1e7
];
var MetricsService = class {
  registry;
  ponder_historical_scheduled_tasks;
  ponder_historical_completed_tasks;
  ponder_historical_rpc_request_duration;
  ponder_historical_total_blocks;
  ponder_historical_cached_blocks;
  ponder_historical_completed_blocks;
  ponder_historical_completion_rate;
  ponder_historical_completion_eta;
  ponder_realtime_is_connected;
  ponder_realtime_latest_block_number;
  ponder_realtime_latest_block_timestamp;
  ponder_realtime_rpc_request_duration;
  ponder_handlers_matched_events;
  ponder_handlers_handled_events;
  ponder_handlers_processed_events;
  ponder_handlers_has_error;
  ponder_handlers_latest_processed_timestamp;
  ponder_server_port;
  ponder_server_request_size;
  ponder_server_response_size;
  ponder_server_response_duration;
  constructor() {
    this.registry = new import_prom_client.default.Registry();
    import_prom_client.default.collectDefaultMetrics({
      register: this.registry,
      prefix: "ponder_default_"
    });
    this.ponder_historical_scheduled_tasks = new import_prom_client.default.Counter({
      name: "ponder_historical_scheduled_tasks",
      help: "Number of historical sync tasks that have been scheduled",
      labelNames: ["network", "kind"],
      registers: [this.registry]
    });
    this.ponder_historical_completed_tasks = new import_prom_client.default.Counter({
      name: "ponder_historical_completed_tasks",
      help: "Number of historical sync tasks that have been processed",
      labelNames: ["network", "kind", "status"],
      registers: [this.registry]
    });
    this.ponder_historical_rpc_request_duration = new import_prom_client.default.Histogram({
      name: "ponder_historical_rpc_request_duration",
      help: "Duration of RPC requests completed during the historical sync",
      labelNames: ["network", "method"],
      buckets: httpRequestBucketsInMs,
      registers: [this.registry]
    });
    this.ponder_historical_total_blocks = new import_prom_client.default.Gauge({
      name: "ponder_historical_total_blocks",
      help: "Number of blocks required for the historical sync",
      labelNames: ["network", "logFilter"],
      registers: [this.registry]
    });
    this.ponder_historical_cached_blocks = new import_prom_client.default.Gauge({
      name: "ponder_historical_cached_blocks",
      help: "Number of blocks that were found in the cache for the historical sync",
      labelNames: ["network", "logFilter"],
      registers: [this.registry]
    });
    this.ponder_historical_completed_blocks = new import_prom_client.default.Gauge({
      name: "ponder_historical_completed_blocks",
      help: "Number of blocks that have been processed for the historical sync",
      labelNames: ["network", "logFilter"],
      registers: [this.registry]
    });
    this.ponder_historical_completion_rate = new import_prom_client.default.Gauge({
      name: "ponder_historical_completion_rate",
      help: "Completion rate (0 to 1) of the historical sync",
      labelNames: ["network", "logFilter"],
      registers: [this.registry]
    });
    this.ponder_historical_completion_eta = new import_prom_client.default.Gauge({
      name: "ponder_historical_completion_eta",
      help: "Estimated number of milliseconds remaining to complete the historical sync",
      labelNames: ["network", "logFilter"],
      registers: [this.registry]
    });
    this.ponder_realtime_is_connected = new import_prom_client.default.Gauge({
      name: "ponder_realtime_is_connected",
      help: "Boolean (0 or 1) indicating if the historical sync service is connected",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_realtime_latest_block_number = new import_prom_client.default.Gauge({
      name: "ponder_realtime_latest_block_number",
      help: "Block number of the latest synced block",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_realtime_latest_block_timestamp = new import_prom_client.default.Gauge({
      name: "ponder_realtime_latest_block_timestamp",
      help: "Block timestamp of the latest synced block",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_realtime_rpc_request_duration = new import_prom_client.default.Histogram({
      name: "ponder_realtime_rpc_request_duration",
      help: "Duration of RPC requests completed during the realtime sync",
      labelNames: ["network", "method"],
      buckets: httpRequestBucketsInMs,
      registers: [this.registry]
    });
    this.ponder_handlers_matched_events = new import_prom_client.default.Gauge({
      name: "ponder_handlers_matched_events",
      help: "Number of available events for all log filters",
      labelNames: ["eventName"],
      registers: [this.registry]
    });
    this.ponder_handlers_handled_events = new import_prom_client.default.Gauge({
      name: "ponder_handlers_handled_events",
      help: "Number of available events for which there is a handler function registered",
      labelNames: ["eventName"],
      registers: [this.registry]
    });
    this.ponder_handlers_processed_events = new import_prom_client.default.Gauge({
      name: "ponder_handlers_processed_events",
      help: "Number of available events that have been processed",
      labelNames: ["eventName"],
      registers: [this.registry]
    });
    this.ponder_handlers_has_error = new import_prom_client.default.Gauge({
      name: "ponder_handlers_has_error",
      help: "Boolean (0 or 1) indicating if an error was encountered while running handlers",
      registers: [this.registry]
    });
    this.ponder_handlers_latest_processed_timestamp = new import_prom_client.default.Gauge({
      name: "ponder_handlers_latest_processed_timestamp",
      help: "Block timestamp of the latest processed event",
      registers: [this.registry]
    });
    this.ponder_server_port = new import_prom_client.default.Gauge({
      name: "ponder_server_port",
      help: "Port that the server is listening on",
      registers: [this.registry]
    });
    this.ponder_server_request_size = new import_prom_client.default.Histogram({
      name: "ponder_server_request_size",
      help: "Size of HTTP requests received by the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestSizeInBytes,
      registers: [this.registry]
    });
    this.ponder_server_response_size = new import_prom_client.default.Histogram({
      name: "ponder_server_response_size",
      help: "Size of HTTP responses served the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestSizeInBytes,
      registers: [this.registry]
    });
    this.ponder_server_response_duration = new import_prom_client.default.Histogram({
      name: "ponder_server_response_duration",
      help: "Duration of HTTP responses served the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestSizeInBytes,
      registers: [this.registry]
    });
  }
  /**
   * Get string representation for all metrics.
   * @returns Metrics encoded using Prometheus v0.0.4 format.
   */
  async getMetrics() {
    return await this.registry.metrics();
  }
};

// src/realtime-sync/service.ts
var import_emittery6 = __toESM(require("emittery"));
var import_p_limit = __toESM(require("p-limit"));
var import_viem8 = require("viem");

// src/utils/wait.ts
async function wait(milliseconds) {
  return new Promise((res) => setTimeout(res, milliseconds));
}

// src/utils/poll.ts
function poll(fn, { emitOnBegin, interval }) {
  let active = true;
  const unwatch = () => active = false;
  const watch = async () => {
    if (emitOnBegin)
      await fn({ unpoll: unwatch });
    await wait(interval);
    const poll2 = async () => {
      if (!active)
        return;
      await fn({ unpoll: unwatch });
      await wait(interval);
      poll2();
    };
    poll2();
  };
  watch();
  return unwatch;
}

// src/utils/range.ts
var range = (start, stop) => Array.from({ length: stop - start }, (_, i) => start + i);

// src/realtime-sync/bloom.ts
var import_ethereum_bloom_filters = require("ethereum-bloom-filters");
function isMatchedLogInBloomFilter({
  bloom,
  logFilters
}) {
  const allAddresses = [];
  logFilters.forEach((logFilter) => {
    const address = logFilter.address === void 0 ? [] : Array.isArray(logFilter.address) ? logFilter.address : [logFilter.address];
    allAddresses.push(...address);
  });
  if (allAddresses.some((a) => (0, import_ethereum_bloom_filters.isContractAddressInBloom)(bloom, a))) {
    return true;
  }
  const allTopics = [];
  logFilters.forEach((logFilter) => {
    logFilter.topics?.forEach((topic) => {
      if (topic === null)
        return;
      if (Array.isArray(topic))
        allTopics.push(...topic);
      else
        allTopics.push(topic);
    });
  });
  if (allTopics.some((a) => (0, import_ethereum_bloom_filters.isTopicInBloom)(bloom, a))) {
    return true;
  }
  return false;
}

// src/realtime-sync/filter.ts
function filterLogs({
  logs,
  logFilters
}) {
  return logs.filter((log) => {
    for (const { address, topics } of logFilters) {
      if (!isLogMatchedByFilter({ log, address, topics }))
        return false;
    }
    return true;
  });
}
function isLogMatchedByFilter({
  log,
  address,
  topics
}) {
  if (address) {
    if (Array.isArray(address)) {
      if (!address.includes(log.address))
        return false;
    } else {
      if (log.address !== address)
        return false;
    }
  }
  if (topics) {
    for (const [index, topic] of topics.entries()) {
      if (topic === null)
        continue;
      if (Array.isArray(topic)) {
        if (!topic.includes(log.topics[index]))
          return false;
      } else {
        if (log.topics[index] !== topic)
          return false;
      }
    }
  }
  return true;
}

// src/realtime-sync/format.ts
var import_viem7 = require("viem");
function rpcBlockToLightBlock(block) {
  return {
    hash: block.hash,
    parentHash: block.parentHash,
    number: (0, import_viem7.hexToNumber)(block.number),
    timestamp: (0, import_viem7.hexToNumber)(block.timestamp)
  };
}

// src/realtime-sync/service.ts
var RealtimeSyncService = class extends import_emittery6.default {
  resources;
  eventStore;
  logFilters;
  network;
  stats;
  // Queue of unprocessed blocks.
  queue;
  // Block number of the current finalized block.
  finalizedBlockNumber = 0;
  // Local representation of the unfinalized portion of the chain.
  blocks = [];
  // Function to stop polling for new blocks.
  unpoll;
  constructor({
    resources,
    eventStore,
    logFilters,
    network
  }) {
    super();
    this.resources = resources;
    this.eventStore = eventStore;
    this.logFilters = logFilters;
    this.network = network;
    this.queue = this.buildQueue();
    this.stats = { blocks: {} };
  }
  setup = async () => {
    const latestBlock = await this.getLatestBlock();
    this.resources.logger.info({
      service: "realtime",
      msg: `Fetched latest block at ${(0, import_viem8.hexToNumber)(
        latestBlock.number
      )} (network=${this.network.name})`
    });
    this.resources.metrics.ponder_realtime_is_connected.set(
      { network: this.network.name },
      1
    );
    const finalizedBlockNumber = Math.max(
      0,
      (0, import_viem8.hexToNumber)(latestBlock.number) - this.network.finalityBlockCount
    );
    this.finalizedBlockNumber = finalizedBlockNumber;
    const priority = Number.MAX_SAFE_INTEGER - (0, import_viem8.hexToNumber)(latestBlock.number);
    this.queue.addTask(latestBlock, { priority });
    return { finalizedBlockNumber };
  };
  start = async () => {
    const endBlocks = this.logFilters.map((f) => f.filter.endBlock);
    if (endBlocks.every(
      (endBlock) => endBlock !== void 0 && endBlock < this.finalizedBlockNumber
    )) {
      this.resources.logger.warn({
        service: "realtime",
        msg: `No realtime log filters found (network=${this.network.name})`
      });
      this.resources.metrics.ponder_realtime_is_connected.set(
        { network: this.network.name },
        0
      );
      return;
    }
    if (this.queue.size === 0) {
      throw new Error(
        `Unable to start. Must call setup() method before start().`
      );
    }
    const stopClock = startClock();
    const finalizedBlock = await this.network.client.request({
      method: "eth_getBlockByNumber",
      params: [(0, import_viem8.numberToHex)(this.finalizedBlockNumber), false]
    });
    if (!finalizedBlock)
      throw new Error(`Unable to fetch finalized block`);
    this.resources.metrics.ponder_realtime_rpc_request_duration.observe(
      {
        method: "eth_getBlockByNumber",
        network: this.network.name
      },
      stopClock()
    );
    this.resources.logger.info({
      service: "realtime",
      msg: `Fetched finalized block at ${(0, import_viem8.hexToNumber)(
        finalizedBlock.number
      )} (network=${this.network.name})`
    });
    this.blocks.push(rpcBlockToLightBlock(finalizedBlock));
    this.queue.start();
    this.unpoll = poll(
      async () => {
        await this.addNewLatestBlock();
      },
      {
        emitOnBegin: false,
        interval: this.network.pollingInterval
      }
    );
  };
  kill = async () => {
    this.unpoll?.();
    this.queue.pause();
    this.queue.clear();
    this.resources.logger.debug({
      service: "realtime",
      msg: `Killed realtime sync service (network=${this.network.name})`
    });
  };
  onIdle = async () => {
    await this.queue.onIdle();
  };
  getLatestBlock = async () => {
    const stopClock = startClock();
    const latestBlock_ = await this.network.client.request({
      method: "eth_getBlockByNumber",
      params: ["latest", true]
    });
    if (!latestBlock_)
      throw new Error(`Unable to fetch latest block`);
    this.resources.metrics.ponder_realtime_rpc_request_duration.observe(
      {
        method: "eth_getBlockByNumber",
        network: this.network.name
      },
      stopClock()
    );
    return latestBlock_;
  };
  addNewLatestBlock = async () => {
    const block = await this.getLatestBlock();
    const priority = Number.MAX_SAFE_INTEGER - (0, import_viem8.hexToNumber)(block.number);
    this.queue.addTask(block, { priority });
  };
  buildQueue = () => {
    const queue = createQueue({
      worker: async ({ task }) => {
        await this.blockTaskWorker(task);
      },
      options: { concurrency: 1, autoStart: false },
      onError: ({ error, task }) => {
        const queueError = new QueueError({
          queueName: "Realtime sync queue",
          task: {
            hash: task.hash,
            parentHash: task.parentHash,
            number: task.number,
            timestamp: task.timestamp,
            transactionCount: task.transactions.length
          },
          cause: error
        });
        this.emit("error", { error: queueError });
      }
    });
    return queue;
  };
  blockTaskWorker = async (block) => {
    const previousHeadBlock = this.blocks[this.blocks.length - 1];
    const newBlockWithTransactions = block;
    const newBlock = rpcBlockToLightBlock(newBlockWithTransactions);
    if (this.blocks.find((b) => b.hash === newBlock.hash)) {
      this.resources.logger.debug({
        service: "realtime",
        msg: `Already processed block at ${newBlock.number} (network=${this.network.name})`
      });
      return;
    }
    if (newBlock.number == previousHeadBlock.number + 1 && newBlock.parentHash == previousHeadBlock.hash) {
      const isMatchedLogPresentInBlock = isMatchedLogInBloomFilter({
        bloom: newBlockWithTransactions.logsBloom,
        logFilters: this.logFilters.map((l) => l.filter)
      });
      let matchedLogCount = 0;
      if (isMatchedLogPresentInBlock) {
        const stopClock = startClock();
        const logs = await this.network.client.request({
          method: "eth_getLogs",
          params: [
            {
              blockHash: newBlock.hash
            }
          ]
        });
        this.resources.metrics.ponder_realtime_rpc_request_duration.observe(
          {
            method: "eth_getLogs",
            network: this.network.name
          },
          stopClock()
        );
        const filteredLogs = filterLogs({
          logs,
          logFilters: this.logFilters.map((l) => l.filter)
        });
        matchedLogCount = filteredLogs.length;
        const requiredTransactionHashes = new Set(
          filteredLogs.map((l) => l.transactionHash)
        );
        const filteredTransactions = newBlockWithTransactions.transactions.filter(
          (t) => requiredTransactionHashes.has(t.hash)
        );
        if (filteredLogs.length > 0) {
          await this.eventStore.insertUnfinalizedBlock({
            chainId: this.network.chainId,
            block: newBlockWithTransactions,
            transactions: filteredTransactions,
            logs: filteredLogs
          });
        }
      }
      this.emit("realtimeCheckpoint", {
        timestamp: (0, import_viem8.hexToNumber)(newBlockWithTransactions.timestamp)
      });
      this.blocks.push(newBlock);
      this.resources.metrics.ponder_realtime_latest_block_number.set(
        { network: this.network.name },
        newBlock.number
      );
      this.resources.metrics.ponder_realtime_latest_block_timestamp.set(
        { network: this.network.name },
        newBlock.timestamp
      );
      this.resources.logger.debug({
        service: "realtime",
        msg: `Processed new head block at ${newBlock.number} (network=${this.network.name})`
      });
      if (matchedLogCount > 0) {
        this.resources.logger.info({
          service: "realtime",
          msg: `Found ${matchedLogCount === 1 ? "1 matched log" : `${matchedLogCount} matched logs`} in new head block ${newBlock.number} (network=${this.network.name})`
        });
      }
      this.stats.blocks[newBlock.number] = {
        bloom: {
          hit: isMatchedLogPresentInBlock,
          falsePositive: isMatchedLogPresentInBlock && matchedLogCount === 0
        },
        matchedLogCount
      };
      if (newBlock.number > this.finalizedBlockNumber + 2 * this.network.finalityBlockCount) {
        const newFinalizedBlock = this.blocks.find(
          (block2) => block2.number === this.finalizedBlockNumber + this.network.finalityBlockCount
        );
        this.blocks = this.blocks.filter(
          (block2) => block2.number >= newFinalizedBlock.number
        );
        for (const blockNumber in this.stats.blocks) {
          if (Number(blockNumber) < newFinalizedBlock.number) {
            delete this.stats.blocks[blockNumber];
          }
        }
        await this.eventStore.finalizeData({
          chainId: this.network.chainId,
          toBlockNumber: newFinalizedBlock.number
        });
        this.finalizedBlockNumber = newFinalizedBlock.number;
        this.emit("finalityCheckpoint", {
          timestamp: newFinalizedBlock.timestamp
        });
        this.resources.logger.debug({
          service: "realtime",
          msg: `Updated finality checkpoint to ${newFinalizedBlock.number} (network=${this.network.name})`,
          matchedLogCount
        });
      }
      return;
    }
    if (newBlock.number > previousHeadBlock.number + 1) {
      const missingBlockNumbers = range(
        previousHeadBlock.number + 1,
        newBlock.number
      );
      const limit = (0, import_p_limit.default)(10);
      const missingBlockRequests = missingBlockNumbers.map((number) => {
        return limit(async () => {
          const stopClock = startClock();
          const block2 = await this.network.client.request({
            method: "eth_getBlockByNumber",
            params: [(0, import_viem8.numberToHex)(number), true]
          });
          if (!block2) {
            throw new Error(`Failed to fetch block number: ${number}`);
          }
          this.resources.metrics.ponder_realtime_rpc_request_duration.observe(
            {
              method: "eth_getBlockByNumber",
              network: this.network.name
            },
            stopClock()
          );
          return block2;
        });
      });
      const missingBlocks = await Promise.all(missingBlockRequests);
      for (const block2 of [...missingBlocks, newBlockWithTransactions]) {
        const priority = Number.MAX_SAFE_INTEGER - (0, import_viem8.hexToNumber)(block2.number);
        this.queue.addTask(block2, { priority });
      }
      this.resources.logger.info({
        service: "realtime",
        msg: `Fetched unfinalized block range [${missingBlockNumbers[0]}, ${missingBlockNumbers[missingBlockNumbers.length - 1]}] (network=${this.network.name})`
      });
      return;
    }
    const canonicalBlocksWithTransactions = [newBlockWithTransactions];
    let canonicalBlock = newBlock;
    let depth = 0;
    while (canonicalBlock.number > this.finalizedBlockNumber) {
      const commonAncestorBlock = this.blocks.find(
        (b) => b.hash === canonicalBlock.parentHash
      );
      if (commonAncestorBlock) {
        this.blocks = this.blocks.filter(
          (block2) => block2.number <= commonAncestorBlock.number
        );
        await this.eventStore.deleteUnfinalizedData({
          chainId: this.network.chainId,
          fromBlockNumber: commonAncestorBlock.number + 1
        });
        this.queue.clear();
        for (const block2 of canonicalBlocksWithTransactions) {
          const priority = Number.MAX_SAFE_INTEGER - (0, import_viem8.hexToNumber)(block2.number);
          this.queue.addTask(block2, { priority });
        }
        await this.addNewLatestBlock();
        this.emit("shallowReorg", {
          commonAncestorTimestamp: commonAncestorBlock.timestamp
        });
        this.resources.logger.info({
          service: "realtime",
          msg: `Reconciled ${depth}-block reorg with common ancestor block ${commonAncestorBlock.number} (network=${this.network.name})`
        });
        return;
      }
      const stopClock = startClock();
      const parentBlock_ = await this.network.client.request({
        method: "eth_getBlockByHash",
        params: [canonicalBlock.parentHash, true]
      });
      this.resources.metrics.ponder_realtime_rpc_request_duration.observe(
        {
          method: "eth_getBlockByHash",
          network: this.network.name
        },
        stopClock()
      );
      if (!parentBlock_)
        throw new Error(
          `Failed to fetch parent block with hash: ${canonicalBlock.parentHash}`
        );
      canonicalBlocksWithTransactions.unshift(
        parentBlock_
      );
      depth += 1;
      canonicalBlock = rpcBlockToLightBlock(parentBlock_);
    }
    this.emit("deepReorg", {
      detectedAtBlockNumber: newBlock.number,
      minimumDepth: depth
    });
    this.resources.logger.warn({
      service: "realtime",
      msg: `Unable to reconcile >${depth}-block reorg (network=${this.network.name})`
    });
  };
};

// src/server/service.ts
var import_cors = __toESM(require("cors"));
var import_express = __toESM(require("express"));
var import_express_graphql = require("express-graphql");
var import_http_terminator = require("http-terminator");
var import_node_http = require("http");
var ServerService = class {
  resources;
  userStore;
  port;
  app;
  terminate;
  graphqlMiddleware;
  isHistoricalEventProcessingComplete = false;
  constructor({
    resources,
    userStore
  }) {
    this.resources = resources;
    this.userStore = userStore;
    this.port = this.resources.options.port;
  }
  async start() {
    this.app = (0, import_express.default)();
    this.app.use((0, import_cors.default)());
    this.app.use((req, res, next) => {
      const endClock = startClock();
      res.on("finish", () => {
        const responseDuration = endClock();
        const method = req.method;
        const path10 = new URL(req.url, `http://${req.get("host")}`).pathname;
        const status = res.statusCode >= 200 && res.statusCode < 300 ? "2XX" : res.statusCode >= 300 && res.statusCode < 400 ? "3XX" : res.statusCode >= 400 && res.statusCode < 500 ? "4XX" : "5XX";
        const requestSize = Number(req.get("Content-Length") ?? 0);
        this.resources.metrics.ponder_server_request_size.observe(
          { method, path: path10, status },
          Number(requestSize)
        );
        const responseSize = Number(res.get("Content-Length") ?? 0);
        this.resources.metrics.ponder_server_response_size.observe(
          { method, path: path10, status },
          Number(responseSize)
        );
        this.resources.metrics.ponder_server_response_duration.observe(
          { method, path: path10, status },
          responseDuration
        );
      });
      next();
    });
    const server = await new Promise((resolve, reject) => {
      const server2 = (0, import_node_http.createServer)(this.app).on("error", (error) => {
        if (error.code === "EADDRINUSE") {
          this.resources.logger.warn({
            service: "server",
            msg: `Port ${this.port} was in use, trying port ${this.port + 1}`
          });
          this.port += 1;
          setTimeout(() => {
            server2.close();
            server2.listen(this.port);
          }, 5);
        } else {
          reject(error);
        }
      }).on("listening", () => {
        this.resources.metrics.ponder_server_port.set(this.port);
        resolve(server2);
      }).listen(this.port);
    });
    const terminator = (0, import_http_terminator.createHttpTerminator)({ server });
    this.terminate = () => terminator.terminate();
    this.resources.logger.info({
      service: "server",
      msg: `Started listening on port ${this.port}`
    });
    this.app.post("/metrics", async (_, res) => {
      try {
        res.set("Content-Type", "text/plain; version=0.0.4; charset=utf-8");
        res.end(await this.resources.metrics.getMetrics());
      } catch (error) {
        res.status(500).end(error);
      }
    });
    this.app.get("/metrics", async (_, res) => {
      try {
        res.set("Content-Type", "text/plain; version=0.0.4; charset=utf-8");
        res.end(await this.resources.metrics.getMetrics());
      } catch (error) {
        res.status(500).end(error);
      }
    });
    this.app.get("/health", (_, res) => {
      if (this.isHistoricalEventProcessingComplete) {
        return res.status(200).send();
      }
      const max = this.resources.options.maxHealthcheckDuration;
      const elapsed = Math.floor(process.uptime());
      if (elapsed > max) {
        this.resources.logger.warn({
          service: "server",
          msg: `Historical sync duration has exceeded the max healthcheck duration of ${max} seconds (current: ${elapsed}). Sevice is now responding as healthy and may serve incomplete data.`
        });
        return res.status(200).send();
      }
      return res.status(503).send();
    });
  }
  reload({ graphqlSchema }) {
    const graphqlMiddleware = (0, import_express_graphql.graphqlHTTP)({
      schema: graphqlSchema,
      context: { store: this.userStore },
      graphiql: true
    });
    this.app?.use("/", graphqlMiddleware);
  }
  async kill() {
    await this.terminate?.();
    this.resources.logger.debug({
      service: "server",
      msg: `Stopped listening on port ${this.port}`
    });
  }
  setIsHistoricalEventProcessingComplete() {
    this.isHistoricalEventProcessingComplete = true;
    this.resources.logger.info({
      service: "server",
      msg: `Started responding as healthy`
    });
  }
};

// src/ui/app.tsx
var import_ink4 = require("ink");
var import_react4 = __toESM(require("react"));

// src/ui/HandlersBar.tsx
var import_ink2 = require("ink");
var import_react2 = __toESM(require("react"));

// src/ui/ProgressBar.tsx
var import_ink = require("ink");
var import_react = __toESM(require("react"));
var ProgressBar = ({ current = 5, end = 10, width = 36 }) => {
  const maxCount = width || process.stdout.columns || 80;
  const fraction = current / end;
  const count = Math.min(Math.floor(maxCount * fraction), maxCount);
  return /* @__PURE__ */ import_react.default.createElement(import_ink.Text, null, /* @__PURE__ */ import_react.default.createElement(import_ink.Text, null, "\u2588".repeat(count)), /* @__PURE__ */ import_react.default.createElement(import_ink.Text, null, "\u2591".repeat(maxCount - count)));
};

// src/ui/HandlersBar.tsx
var HandlersBar = ({ ui }) => {
  const completionRate = ui.handlersCurrent / Math.max(ui.handlersHandledTotal, 1);
  const completionDecimal = Math.round(completionRate * 1e3) / 10;
  const completionText = Number.isInteger(completionDecimal) && completionDecimal < 100 ? `${completionDecimal}.0%` : `${completionDecimal}%`;
  const date = new Date(ui.handlersToTimestamp * 1e3);
  const year = date.getFullYear();
  const months = [
    "Jan",
    "Feb",
    "Mar",
    "Apr",
    "May",
    "Jun",
    "Jul",
    "Aug",
    "Sep",
    "Oct",
    "Nov",
    "Dec"
  ];
  const month = months[date.getMonth()];
  const day = date.getDate();
  const dateText = `${month} ${day}, ${year}`;
  const isUpToDate = ui.isHistoricalSyncComplete && ui.handlersCurrent === ui.handlersHandledTotal;
  const isStarted = ui.handlersToTimestamp > 0;
  const titleText = () => {
    if (isUpToDate)
      return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, { color: "green" }, "(up to date)");
    if (isStarted)
      return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, { color: "yellow" }, "(up to ", ui.handlersToTimestamp === 0 ? "" : dateText, ")");
    return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, "(not started)");
  };
  const countText = () => {
    if (isUpToDate)
      return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, " ", "| ", ui.handlersCurrent, "/", ui.handlersHandledTotal, " events (", ui.handlersTotal, " total)");
    if (isStarted)
      return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, " ", "| ", ui.handlersCurrent, "/", "?".repeat(ui.handlersCurrent.toString().length), " events (", ui.handlersTotal, " total)");
    return null;
  };
  return /* @__PURE__ */ import_react2.default.createElement(import_ink2.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react2.default.createElement(import_ink2.Box, { flexDirection: "row" }, /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, { bold: true }, "Event handlers "), /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, titleText())), /* @__PURE__ */ import_react2.default.createElement(import_ink2.Box, { flexDirection: "row" }, /* @__PURE__ */ import_react2.default.createElement(
    ProgressBar,
    {
      current: ui.handlersCurrent,
      end: Math.max(ui.handlersHandledTotal, 1)
    }
  ), /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, " ", completionText, countText())), /* @__PURE__ */ import_react2.default.createElement(import_ink2.Text, null, " "));
};

// src/ui/HistoricalBar.tsx
var import_ink3 = require("ink");
var import_react3 = __toESM(require("react"));
var HistoricalBar = ({
  title,
  stat
}) => {
  const { rate, eta } = stat;
  const etaText = eta ? ` | ~${formatEta(eta)}` : null;
  const rateText = formatPercentage(rate);
  return /* @__PURE__ */ import_react3.default.createElement(import_ink3.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react3.default.createElement(import_ink3.Text, null, title), /* @__PURE__ */ import_react3.default.createElement(import_ink3.Box, { flexDirection: "row" }, /* @__PURE__ */ import_react3.default.createElement(ProgressBar, { current: rate, end: 1 }), /* @__PURE__ */ import_react3.default.createElement(import_ink3.Text, null, " ", rateText, etaText)));
};

// src/ui/app.tsx
var buildUiState = ({ logFilters }) => {
  const ui = {
    port: 0,
    historicalSyncLogFilterStats: {},
    isHistoricalSyncComplete: false,
    handlerError: false,
    handlersCurrent: 0,
    handlersTotal: 0,
    handlersHandledTotal: 0,
    handlersToTimestamp: 0,
    networks: []
  };
  logFilters.forEach((logFilter) => {
    ui.historicalSyncLogFilterStats[logFilter.name] = {
      rate: 0
    };
  });
  return ui;
};
var App = (ui) => {
  const {
    port,
    historicalSyncLogFilterStats,
    isHistoricalSyncComplete,
    handlersCurrent,
    handlerError,
    networks
  } = ui;
  if (handlerError) {
    return /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, " "), /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { color: "cyan" }, "Resolve the error and save your changes to reload the server."));
  }
  return /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, " "), /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "row" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { bold: true }, "Historical sync "), isHistoricalSyncComplete ? /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { color: "green" }, "(complete)", /* @__PURE__ */ import_react4.default.createElement(import_ink4.Newline, null)) : /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { color: "yellow" }, "(in progress)")), !isHistoricalSyncComplete && /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "column" }, Object.entries(historicalSyncLogFilterStats).map(
    ([logFilterName, stat]) => /* @__PURE__ */ import_react4.default.createElement(
      HistoricalBar,
      {
        key: logFilterName,
        title: logFilterName,
        stat
      }
    )
  ), /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, " ")), /* @__PURE__ */ import_react4.default.createElement(HandlersBar, { ui }), networks.length > 0 && /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { bold: true }, "Networks"), networks.map((network) => /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "row", key: network }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, network.slice(0, 1).toUpperCase() + network.slice(1), " (live)"))), /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, " ")), handlersCurrent > 0 && /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "column" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, { bold: true }, "GraphQL "), /* @__PURE__ */ import_react4.default.createElement(import_ink4.Box, { flexDirection: "row" }, /* @__PURE__ */ import_react4.default.createElement(import_ink4.Text, null, "Server live at http://localhost:", port))));
};
var setupInkApp = (ui) => {
  const { rerender, unmount: inkUnmount, clear } = (0, import_ink4.render)(/* @__PURE__ */ import_react4.default.createElement(App, { ...ui }));
  const render = (ui2) => {
    rerender(/* @__PURE__ */ import_react4.default.createElement(App, { ...ui2 }));
  };
  const unmount = () => {
    clear();
    inkUnmount();
  };
  return { render, unmount };
};

// src/ui/service.ts
var UiService = class {
  resources;
  logFilters;
  ui;
  renderInterval;
  render;
  unmount;
  constructor({
    resources,
    logFilters
  }) {
    this.resources = resources;
    this.logFilters = logFilters;
    this.ui = buildUiState({ logFilters: this.logFilters });
    if (this.resources.options.uiEnabled) {
      const { render, unmount } = setupInkApp(this.ui);
      this.render = () => render(this.ui);
      this.unmount = unmount;
    } else {
      this.render = () => void 0;
      this.unmount = () => void 0;
    }
    this.renderInterval = setInterval(async () => {
      const logFilterNames = Object.keys(this.ui.historicalSyncLogFilterStats);
      const rateMetric = (await this.resources.metrics.ponder_historical_completion_rate.get()).values;
      const etaMetric = (await this.resources.metrics.ponder_historical_completion_eta.get()).values;
      logFilterNames.forEach((name) => {
        const rate = rateMetric.find((m) => m.labels.logFilter === name)?.value;
        const eta = etaMetric.find((m) => m.labels.logFilter === name)?.value;
        if (rate !== void 0) {
          this.ui.historicalSyncLogFilterStats[name].rate = rate;
        }
        this.ui.historicalSyncLogFilterStats[name].eta = eta;
      });
      const minRate = Math.min(
        ...logFilterNames.map(
          (name) => this.ui.historicalSyncLogFilterStats[name].rate
        )
      );
      if (!this.ui.isHistoricalSyncComplete && minRate === 1) {
        this.ui.isHistoricalSyncComplete = true;
      }
      const connectedNetworks = (await this.resources.metrics.ponder_realtime_is_connected.get()).values.filter((m) => m.value === 1).map((m) => m.labels.network).filter((n) => typeof n === "string");
      this.ui.networks = connectedNetworks;
      const matchedEvents = (await this.resources.metrics.ponder_handlers_matched_events.get()).values.reduce((a, v) => a + v.value, 0);
      const handledEvents = (await this.resources.metrics.ponder_handlers_handled_events.get()).values.reduce((a, v) => a + v.value, 0);
      const processedEvents = (await this.resources.metrics.ponder_handlers_processed_events.get()).values.reduce((a, v) => a + v.value, 0);
      const latestProcessedTimestamp = (await this.resources.metrics.ponder_handlers_latest_processed_timestamp.get()).values[0].value ?? 0;
      this.ui.handlersTotal = matchedEvents;
      this.ui.handlersHandledTotal = handledEvents;
      this.ui.handlersCurrent = processedEvents;
      this.ui.handlersToTimestamp = latestProcessedTimestamp;
      this.ui.handlerError = this.resources.errors.hasUserError;
      const port = (await this.resources.metrics.ponder_server_port.get()).values[0].value;
      this.ui.port = port;
      this.render();
    }, 17);
  }
  kill() {
    clearInterval(this.renderInterval);
    this.unmount();
  }
};

// src/user-handlers/service.ts
var import_async_mutex = require("async-mutex");
var import_emittery7 = __toESM(require("emittery"));
var import_viem10 = require("viem");

// src/user-handlers/contract.ts
var import_viem9 = require("viem");
function buildReadOnlyContracts({
  contracts,
  eventStore,
  getCurrentBlockNumber
}) {
  return contracts.reduce((acc, { name, abi, address, network }) => {
    const { chainId, client: publicClient } = network;
    const readOnlyContract = (0, import_viem9.getContract)({ abi, address, publicClient });
    readOnlyContract.read = new Proxy(
      {},
      {
        get(_, functionName) {
          return async (...parameters) => {
            const { args, options } = getFunctionParameters(parameters);
            if (options?.blockTag) {
              return publicClient.readContract({
                abi,
                address,
                functionName,
                args,
                ...options
              });
            }
            const blockNumber = options?.blockNumber ?? getCurrentBlockNumber();
            const calldata = (0, import_viem9.encodeFunctionData)({ abi, args, functionName });
            const decodeRawResult = (rawResult2) => {
              try {
                return (0, import_viem9.decodeFunctionResult)({
                  abi,
                  args,
                  functionName,
                  data: rawResult2
                });
              } catch (err) {
                throw (0, import_viem9.getContractError)(err, {
                  abi,
                  address,
                  args,
                  docsPath: "/docs/contract/readContract",
                  functionName
                });
              }
            };
            const cachedContractReadResult = await eventStore.getContractReadResult({
              address,
              blockNumber,
              chainId,
              data: calldata
            });
            if (cachedContractReadResult) {
              return decodeRawResult(cachedContractReadResult.result);
            }
            let rawResult;
            try {
              const { data } = await publicClient.call({
                data: calldata,
                to: address,
                ...{
                  ...options,
                  blockNumber
                }
              });
              rawResult = data || "0x";
            } catch (err) {
              throw (0, import_viem9.getContractError)(err, {
                abi,
                address,
                args,
                docsPath: "/docs/contract/readContract",
                functionName
              });
            }
            await eventStore.insertContractReadResult({
              address,
              blockNumber,
              chainId,
              data: calldata,
              finalized: false,
              result: rawResult
            });
            return decodeRawResult(rawResult);
          };
        }
      }
    );
    acc[name] = readOnlyContract;
    return acc;
  }, {});
}
function getFunctionParameters(values) {
  const hasArgs = values.length && Array.isArray(values[0]);
  const args = hasArgs ? values[0] : [];
  const options = (hasArgs ? values[1] : values[0]) ?? {};
  return { args, options };
}

// src/user-handlers/model.ts
function buildModels({
  userStore,
  schema,
  getCurrentEventTimestamp
}) {
  return schema.entities.reduce(
    (acc, { name: modelName }) => {
      acc[modelName] = {
        findUnique: ({ id }) => userStore.findUnique({
          modelName,
          timestamp: getCurrentEventTimestamp(),
          id
        }),
        create: ({ id, data }) => userStore.create({
          modelName,
          timestamp: getCurrentEventTimestamp(),
          id,
          data
        }),
        update: ({ id, data }) => userStore.update({
          modelName,
          timestamp: getCurrentEventTimestamp(),
          id,
          data
        }),
        upsert: ({ id, create, update }) => userStore.upsert({
          modelName,
          timestamp: getCurrentEventTimestamp(),
          id,
          create,
          update
        }),
        delete: ({ id }) => userStore.delete({
          modelName,
          timestamp: getCurrentEventTimestamp(),
          id
        })
      };
      return acc;
    },
    {}
  );
}

// src/user-handlers/trace.ts
var import_code_frame = require("@babel/code-frame");
var import_trace_mapping = require("@jridgewell/trace-mapping");
var import_data_uri_to_buffer = __toESM(require("data-uri-to-buffer"));
var import_node_fs7 = require("fs");
var import_node_path8 = __toESM(require("path"));
var import_stacktrace_parser = require("stacktrace-parser");
var getStackTrace = (error, options) => {
  if (!error.stack)
    return void 0;
  const buildDir = import_node_path8.default.join(options.ponderDir, "out");
  const stackTrace = (0, import_stacktrace_parser.parse)(error.stack);
  let codeFrame;
  const sourceMappedStackTrace = stackTrace.map((frame) => {
    if (!frame.file || !frame.lineNumber)
      return;
    const sourceMappedStackFrame = getSourceMappedStackFrame(
      frame.file,
      frame.lineNumber,
      frame.column
    );
    if (!sourceMappedStackFrame)
      return;
    const {
      sourceFile,
      sourceLineNumber,
      sourceColumnNumber,
      sourceContent
    } = sourceMappedStackFrame;
    if (frame.file.includes(buildDir) && codeFrame == null && sourceContent !== null) {
      codeFrame = (0, import_code_frame.codeFrameColumns)(
        sourceContent,
        {
          start: {
            line: sourceLineNumber,
            column: sourceColumnNumber ?? void 0
          }
        },
        {
          highlightCode: true
        }
      );
    }
    return {
      ...frame,
      file: sourceFile,
      lineNumber: sourceLineNumber,
      column: sourceColumnNumber
    };
  }).filter((f) => !!f);
  if (sourceMappedStackTrace.length === 0 || !codeFrame) {
    return void 0;
  }
  const formattedStackTrace = [
    ...sourceMappedStackTrace.map((frame) => {
      let result = "  at";
      result += ` ${frame.methodName === "<unknown>" ? "(anonymous)" : frame.methodName}`;
      result += ` (${frame.file}:${frame.lineNumber}${frame.column !== null ? `:${frame.column}` : ""})`;
      return result;
    }),
    codeFrame
  ].join("\n");
  return formattedStackTrace;
};
function getSourceMappedStackFrame(file, lineNumber, columnNumber) {
  let fileContents;
  try {
    fileContents = (0, import_node_fs7.readFileSync)(file, { encoding: "utf-8" });
  } catch (_) {
    return null;
  }
  const sourceMap = getRawSourceMap(fileContents);
  if (!sourceMap)
    return null;
  const result = getSourcePositionAndContent(
    sourceMap,
    lineNumber,
    columnNumber
  );
  const sourceFileRelative = result?.sourcePosition?.source;
  const sourceLineNumber = result?.sourcePosition?.line;
  const sourceColumnNumber = result?.sourcePosition?.column ?? null;
  const sourceContent = result?.sourceContent ?? null;
  if (!sourceFileRelative || !sourceLineNumber)
    return null;
  const sourceFile = import_node_path8.default.resolve(import_node_path8.default.dirname(file), sourceFileRelative);
  return {
    sourceFile,
    sourceLineNumber,
    sourceColumnNumber,
    sourceContent
  };
}
function getSourceMapUrl(fileContents) {
  const regex = /\/\/[#@] ?sourceMappingURL=([^\s'"]+)\s*$/gm;
  let match = null;
  for (; ; ) {
    const next = regex.exec(fileContents);
    if (next == null) {
      break;
    }
    match = next;
  }
  if (!(match && match[1])) {
    return null;
  }
  return match[1].toString();
}
function getRawSourceMap(fileContents) {
  const sourceUrl = getSourceMapUrl(fileContents);
  if (!sourceUrl?.startsWith("data:")) {
    return null;
  }
  let buffer;
  try {
    buffer = (0, import_data_uri_to_buffer.default)(sourceUrl);
  } catch (err) {
    console.error("Failed to parse source map URL:", err);
    return null;
  }
  if (buffer.type !== "application/json") {
    console.error(`Unknown source map type: ${buffer.typeFull}.`);
    return null;
  }
  try {
    return JSON.parse(buffer.toString());
  } catch {
    console.error("Failed to parse source map.");
    return null;
  }
}
function getSourcePositionAndContent(rawSourceMap, lineNumber, columnNumber) {
  const tracer = new import_trace_mapping.TraceMap(rawSourceMap);
  const sourcePosition = (0, import_trace_mapping.originalPositionFor)(tracer, {
    line: lineNumber,
    column: columnNumber ?? 0
  });
  if (!sourcePosition.source) {
    return null;
  }
  const sourceContent = (0, import_trace_mapping.sourceContentFor)(tracer, sourcePosition.source) ?? null;
  return {
    sourcePosition,
    sourceContent
  };
}

// src/user-handlers/service.ts
var EventHandlerService = class extends import_emittery7.default {
  resources;
  userStore;
  eventAggregatorService;
  logFilters;
  readOnlyContracts = {};
  schema;
  models = {};
  handlers;
  handledLogFilters = {};
  eventProcessingMutex;
  queue;
  eventsProcessedToTimestamp = 0;
  hasError = false;
  currentEventBlockNumber = 0n;
  currentEventTimestamp = 0;
  constructor({
    resources,
    eventStore,
    userStore,
    eventAggregatorService,
    contracts,
    logFilters
  }) {
    super();
    this.resources = resources;
    this.userStore = userStore;
    this.eventAggregatorService = eventAggregatorService;
    this.logFilters = logFilters;
    this.readOnlyContracts = buildReadOnlyContracts({
      contracts,
      getCurrentBlockNumber: () => this.currentEventBlockNumber,
      eventStore
    });
    this.eventProcessingMutex = new import_async_mutex.Mutex();
  }
  kill = () => {
    this.queue?.clear();
    this.eventProcessingMutex.cancel();
    this.resources.logger.debug({
      service: "handlers",
      msg: `Killed user handler service`
    });
  };
  /**
   * Registers a new set of handler functions and/or a new schema, cancels
   * the current event processing mutex & event queue, drops and re-creates
   * all tables from the user store, and resets eventsProcessedToTimestamp to zero.
   *
   * Note: Caller should (probably) immediately call processEvents after this method.
   */
  reset = async ({
    handlers: newHandlers,
    schema: newSchema
  } = {}) => {
    if (newSchema) {
      this.schema = newSchema;
      this.models = buildModels({
        userStore: this.userStore,
        schema: this.schema,
        getCurrentEventTimestamp: () => this.currentEventTimestamp
      });
    }
    if (newHandlers) {
      this.handlers = newHandlers;
      this.handledLogFilters = {};
      this.logFilters.forEach((logFilter) => {
        const handledEventSignatureTopics = Object.keys(
          (this.handlers ?? {})[logFilter.name] ?? {}
        ).map((eventName) => {
          const topics = (0, import_viem10.encodeEventTopics)({
            abi: logFilter.abi,
            eventName
          });
          const abiItem = (0, import_viem10.getAbiItem)({
            abi: logFilter.abi,
            name: eventName
          });
          return { eventName, topic0: topics[0], abiItem };
        });
        this.handledLogFilters[logFilter.name] = handledEventSignatureTopics;
      });
    }
    if (!this.schema || !this.handlers)
      return;
    this.eventProcessingMutex.cancel();
    this.eventProcessingMutex = new import_async_mutex.Mutex();
    this.queue = this.createEventQueue({ handlers: this.handlers });
    this.hasError = false;
    this.resources.metrics.ponder_handlers_has_error.set(0);
    this.resources.metrics.ponder_handlers_matched_events.reset();
    this.resources.metrics.ponder_handlers_handled_events.reset();
    this.resources.metrics.ponder_handlers_processed_events.reset();
    await this.userStore.reload({ schema: this.schema });
    this.resources.logger.debug({
      service: "handlers",
      msg: `Reset user store (versionId=${this.userStore.versionId})`
    });
    this.eventsProcessedToTimestamp = 0;
    this.resources.metrics.ponder_handlers_latest_processed_timestamp.set(0);
  };
  /**
   * This method is triggered by the realtime sync service detecting a reorg,
   * which can happen at any time. The event queue and the user store can be
   * in one of several different states that we need to keep in mind:
   *
   * 1) No events have been added to the queue yet.
   * 2) No unsafe events have been processed (eventsProcessedToTimestamp <= commonAncestorTimestamp).
   * 3) Unsafe events may have been processed (eventsProcessedToTimestamp > commonAncestorTimestamp).
   * 4) The queue has encountered a user error and is waiting for a reload.
   *
   * Note: It's crucial that we acquire a mutex lock while handling the reorg.
   * This will only ever run while the queue is idle, so we can be confident
   * that eventsProcessedToTimestamp matches the current state of the user store,
   * and that no unsafe events will get processed after handling the reorg.
   *
   * Note: Caller should (probably) immediately call processEvents after this method.
   */
  handleReorg = async ({
    commonAncestorTimestamp
  }) => {
    try {
      await this.eventProcessingMutex.runExclusive(async () => {
        if (this.hasError)
          return;
        if (this.eventsProcessedToTimestamp <= commonAncestorTimestamp) {
          this.resources.logger.debug({
            service: "handlers",
            msg: `No unsafe events were detected while reconciling a reorg, no-op`
          });
        } else {
          await this.userStore.revert({
            safeTimestamp: commonAncestorTimestamp
          });
          this.eventsProcessedToTimestamp = commonAncestorTimestamp;
          this.resources.metrics.ponder_handlers_latest_processed_timestamp.set(
            commonAncestorTimestamp
          );
          this.resources.logger.debug({
            service: "handlers",
            msg: `Reverted user store to safe timestamp ${commonAncestorTimestamp}`
          });
        }
      });
    } catch (error) {
      if (error !== import_async_mutex.E_CANCELED)
        throw error;
    }
  };
  /**
   * Processes all newly available events.
   *
   * Acquires a lock on the event processing mutex, then gets the latest checkpoint
   * from the event aggregator service. Fetches events between previous checkpoint
   * and the new checkpoint, adds them to the queue, then processes them.
   */
  processEvents = async () => {
    try {
      await this.eventProcessingMutex.runExclusive(async () => {
        if (this.hasError || !this.queue)
          return;
        const eventsAvailableTo = this.eventAggregatorService.checkpoint;
        if (this.eventsProcessedToTimestamp >= eventsAvailableTo) {
          return;
        }
        const fromTimestamp = this.eventsProcessedToTimestamp === 0 ? 0 : this.eventsProcessedToTimestamp + 1;
        const toTimestamp = eventsAvailableTo;
        const { events, totalEventCount } = await this.eventAggregatorService.getEvents({
          fromTimestamp,
          toTimestamp,
          handledLogFilters: this.handledLogFilters
        });
        this.resources.metrics.ponder_handlers_matched_events.inc(
          totalEventCount
        );
        if (this.eventsProcessedToTimestamp === 0 && this.handlers?.setup) {
          this.queue.addTask({ kind: "SETUP" });
          this.resources.metrics.ponder_handlers_handled_events.inc({
            eventName: "setup"
          });
        }
        for (const event of events) {
          this.queue.addTask({
            kind: "LOG",
            event
          });
          this.resources.metrics.ponder_handlers_handled_events.inc({
            eventName: `${event.logFilterName}:${event.eventName}`
          });
        }
        this.queue.start();
        await this.queue.onIdle();
        this.queue.pause();
        this.eventsProcessedToTimestamp = toTimestamp;
        this.emit("eventsProcessed", { toTimestamp });
        this.resources.metrics.ponder_handlers_latest_processed_timestamp.set(
          toTimestamp
        );
        if (events.length > 0) {
          this.resources.logger.info({
            service: "handlers",
            msg: `Processed ${events.length === 1 ? "1 event" : `${events.length} events`}`
          });
        }
      });
    } catch (error) {
      if (error !== import_async_mutex.E_CANCELED)
        throw error;
    }
  };
  createEventQueue = ({ handlers }) => {
    const context = {
      contracts: this.readOnlyContracts,
      entities: this.models
    };
    const eventHandlerWorker = async ({
      task,
      queue: queue2
    }) => {
      switch (task.kind) {
        case "SETUP": {
          const setupHandler = handlers["setup"];
          if (!setupHandler)
            return;
          try {
            await setupHandler({ context });
            this.resources.metrics.ponder_handlers_processed_events.inc({
              eventName: "setup"
            });
          } catch (error_) {
            queue2.clear();
            this.hasError = true;
            this.resources.metrics.ponder_handlers_has_error.set(1);
            const error = error_;
            const trace = getStackTrace(error, this.resources.options);
            const message = `Error while handling "setup" event: ${error.message}`;
            const userError = new UserError(message, {
              stack: trace,
              cause: error
            });
            this.resources.logger.error({
              service: "handlers",
              error: userError
            });
            this.resources.errors.submitUserError({ error: userError });
          }
          break;
        }
        case "LOG": {
          const event = task.event;
          const handler = handlers[event.logFilterName]?.[event.eventName];
          if (!handler)
            return;
          this.currentEventBlockNumber = event.block.number;
          this.currentEventTimestamp = Number(event.block.timestamp);
          try {
            await handler({
              event: {
                ...event,
                name: event.eventName
              },
              context
            });
            this.resources.metrics.ponder_handlers_processed_events.inc({
              eventName: `${event.logFilterName}:${event.eventName}`
            });
          } catch (error_) {
            queue2.clear();
            this.hasError = true;
            this.resources.metrics.ponder_handlers_has_error.set(1);
            const error = error_;
            const trace = getStackTrace(error, this.resources.options);
            const message = `Error while handling "${event.logFilterName}:${event.eventName}" event at block ${Number(event.block.number)}: ${error.message}`;
            const metaMessage = `Event params:
${prettyPrint(event.params)}`;
            const userError = new UserError(message, {
              stack: trace,
              meta: metaMessage,
              cause: error
            });
            this.resources.logger.error({
              service: "handlers",
              error: userError
            });
            this.resources.errors.submitUserError({ error: userError });
          }
          break;
        }
      }
    };
    const queue = createQueue({
      worker: eventHandlerWorker,
      context: void 0,
      options: {
        concurrency: 1,
        autoStart: false
      }
    });
    return queue;
  };
};

// src/user-store/postgres/store.ts
var import_crypto = require("crypto");
var import_kysely4 = require("kysely");

// src/user-store/utils.ts
var filterTypes = {
  // universal
  "": { operator: "=", patternPrefix: void 0, patternSuffix: void 0 },
  not: { operator: "!=", patternPrefix: void 0, patternSuffix: void 0 },
  // singular
  in: { operator: "in", patternPrefix: void 0, patternSuffix: void 0 },
  not_in: {
    operator: "not in",
    patternPrefix: void 0,
    patternSuffix: void 0
  },
  // plural
  contains: { operator: "like", patternPrefix: "%", patternSuffix: "%" },
  not_contains: {
    operator: "not like",
    patternPrefix: "%",
    patternSuffix: "%"
  },
  // numeric
  gt: { operator: ">", patternPrefix: void 0, patternSuffix: void 0 },
  lt: { operator: "<", patternPrefix: void 0, patternSuffix: void 0 },
  gte: { operator: ">=", patternPrefix: void 0, patternSuffix: void 0 },
  lte: { operator: "<=", patternPrefix: void 0, patternSuffix: void 0 },
  // string
  starts_with: {
    operator: "like",
    patternPrefix: void 0,
    patternSuffix: "%"
  },
  ends_with: { operator: "like", patternPrefix: "%", patternSuffix: void 0 },
  not_starts_with: {
    operator: "not like",
    patternPrefix: void 0,
    patternSuffix: "%"
  },
  not_ends_with: {
    operator: "not like",
    patternPrefix: "%",
    patternSuffix: void 0
  }
};
function getWhereOperatorAndValue({
  filterType,
  value
}) {
  const { operator, patternPrefix, patternSuffix } = filterTypes[filterType];
  if (value === null || value === void 0) {
    return {
      operator: operator === "=" ? "is" : operator === "!=" ? "is not" : operator,
      value: null
    };
  }
  if (Array.isArray(value)) {
    if (filterType === "" || filterType === "not") {
      return { operator, value: JSON.stringify(value) };
    }
    return {
      operator,
      value: value.map((v) => {
        if (typeof v === "boolean") {
          return v ? 1 : 0;
        } else if (typeof v === "bigint") {
          return intToBlob(v);
        } else {
          return v;
        }
      })
    };
  }
  if (typeof value === "boolean") {
    return { operator, value: value ? 1 : 0 };
  }
  if (typeof value === "bigint") {
    return { operator, value: intToBlob(value) };
  }
  let finalValue = value;
  if (patternPrefix)
    finalValue = `${patternPrefix}${finalValue}`;
  if (patternSuffix)
    finalValue = `${finalValue}${patternSuffix}`;
  return { operator, value: finalValue };
}
function formatModelFieldValue({ value }) {
  if (typeof value === "boolean") {
    return value ? 1 : 0;
  } else if (typeof value === "bigint") {
    return intToBlob(value);
  } else if (typeof value === "undefined") {
    return null;
  } else if (Array.isArray(value)) {
    if (typeof value[0] === "bigint") {
      return JSON.stringify(value.map(String));
    } else {
      return JSON.stringify(value);
    }
  } else {
    return value;
  }
}
function formatModelInstance({
  id,
  data
}) {
  const instance = {};
  instance["id"] = formatModelFieldValue({ value: id });
  Object.entries(data).forEach(([key, value]) => {
    instance[key] = formatModelFieldValue({ value });
  });
  return instance;
}
function parseModelFilter(filter = {}) {
  const parsedFilter = {};
  if (filter.first) {
    if (filter.first > MAX_LIMIT) {
      throw new BaseError("Cannot query more than 1000 rows.");
    }
    parsedFilter.first = filter.first;
  } else {
    parsedFilter.first = DEFAULT_LIMIT;
  }
  if (filter.skip) {
    if (filter.skip > MAX_SKIP)
      throw new BaseError("Cannot skip more than 5000 rows.");
    parsedFilter.skip = filter.skip;
  }
  parsedFilter.orderBy = filter.orderBy || "id";
  parsedFilter.orderDirection = filter.orderDirection || "asc";
  parsedFilter.where = filter.where;
  return parsedFilter;
}
var DEFAULT_LIMIT = 100;
var MAX_LIMIT = 1e3;
var MAX_SKIP = 5e3;

// src/user-store/postgres/store.ts
var gqlScalarToSqlType = {
  Boolean: "integer",
  Int: "integer",
  String: "text",
  BigInt: import_kysely4.sql`bytea`,
  Bytes: "text",
  Float: "text"
};
var MAX_INTEGER = 2147483647;
var PostgresUserStore = class {
  db;
  schema;
  versionId;
  constructor({
    pool,
    databaseSchema
  }) {
    this.db = new import_kysely4.Kysely({
      dialect: new import_kysely4.PostgresDialect({
        pool,
        onCreateConnection: databaseSchema ? async (connection) => {
          await connection.executeQuery(
            import_kysely4.CompiledQuery.raw(
              `CREATE SCHEMA IF NOT EXISTS ${databaseSchema}`
            )
          );
          await connection.executeQuery(
            import_kysely4.CompiledQuery.raw(`SET search_path = ${databaseSchema}`)
          );
        } : void 0
      })
    });
  }
  /**
   * Resets the database by dropping existing tables and creating new tables.
   * If no new schema is provided, the existing schema is used.
   *
   * @param options.schema New schema to be used.
   */
  reload = async ({ schema } = {}) => {
    if (!this.schema && !schema)
      return;
    await this.db.transaction().execute(async (tx) => {
      if (this.schema) {
        await Promise.all(
          this.schema.entities.map((model) => {
            const tableName = `${model.name}_${this.versionId}`;
            tx.schema.dropTable(tableName);
          })
        );
      }
      if (schema)
        this.schema = schema;
      this.versionId = (0, import_crypto.randomBytes)(4).toString("hex");
      await Promise.all(
        this.schema.entities.map(async (model) => {
          const tableName = `${model.name}_${this.versionId}`;
          let tableBuilder = tx.schema.createTable(tableName);
          model.fields.forEach((field) => {
            switch (field.kind) {
              case "SCALAR": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  gqlScalarToSqlType[field.scalarTypeName],
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
              case "ENUM": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  "text",
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    col = col.check(
                      import_kysely4.sql`${import_kysely4.sql.ref(field.name)} in (${import_kysely4.sql.join(
                        field.enumValues.map((v) => import_kysely4.sql.lit(v))
                      )})`
                    );
                    return col;
                  }
                );
                break;
              }
              case "LIST": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  "text",
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
              case "RELATIONSHIP": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  gqlScalarToSqlType[field.relatedEntityIdType.name],
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
            }
          });
          tableBuilder = tableBuilder.addColumn(
            "effectiveFrom",
            "integer",
            (col) => col.notNull()
          );
          tableBuilder = tableBuilder.addColumn(
            "effectiveTo",
            "integer",
            (col) => col.notNull()
          );
          tableBuilder = tableBuilder.addPrimaryKeyConstraint(
            `${tableName}_id_effectiveTo_unique`,
            // eslint-disable-next-line @typescript-eslint/ban-ts-comment
            // @ts-ignore
            ["id", "effectiveTo"]
          );
          await tableBuilder.execute();
        })
      );
    });
  };
  /**
   * Tears down the store by dropping all tables for the current schema.
   */
  teardown = async () => {
    if (!this.schema)
      return;
    await this.db.transaction().execute(async (tx) => {
      await Promise.all(
        this.schema.entities.map((model) => {
          const tableName = `${model.name}_${this.versionId}`;
          tx.schema.dropTable(tableName);
        })
      );
    });
  };
  findUnique = async ({
    modelName,
    timestamp = MAX_INTEGER,
    id
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const instances = await this.db.selectFrom(tableName).selectAll().where("id", "=", formattedId).where("effectiveFrom", "<=", timestamp).where("effectiveTo", ">=", timestamp).execute();
    if (instances.length > 1) {
      throw new Error(`Expected 1 instance, found ${instances.length}`);
    }
    return instances[0] ? this.deserializeInstance({ modelName, instance: instances[0] }) : null;
  };
  create = async ({
    modelName,
    timestamp,
    id,
    data = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const createInstance = formatModelInstance({ id, data });
    const instance = await this.db.insertInto(tableName).values({
      ...createInstance,
      effectiveFrom: timestamp,
      effectiveTo: MAX_INTEGER
    }).returningAll().executeTakeFirstOrThrow();
    return this.deserializeInstance({ modelName, instance });
  };
  update = async ({
    modelName,
    timestamp,
    id,
    data = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const updateInstance = formatModelInstance({ id, data });
    const instance = await this.db.transaction().execute(async (tx) => {
      const latestInstance = await tx.selectFrom(tableName).selectAll().where("id", "=", formattedId).orderBy("effectiveTo", "desc").executeTakeFirstOrThrow();
      if (latestInstance.effectiveFrom === timestamp) {
        return await tx.updateTable(tableName).set(updateInstance).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom > timestamp) {
        throw new Error(`Cannot update an instance in the past`);
      }
      await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER).execute();
      return await tx.insertInto(tableName).values({
        ...latestInstance,
        ...updateInstance,
        effectiveFrom: timestamp,
        effectiveTo: MAX_INTEGER
      }).returningAll().executeTakeFirstOrThrow();
    });
    return this.deserializeInstance({ modelName, instance });
  };
  upsert = async ({
    modelName,
    timestamp,
    id,
    create = {},
    update = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const createInstance = formatModelInstance({ id, data: create });
    const updateInstance = formatModelInstance({ id, data: update });
    const instance = await this.db.transaction().execute(async (tx) => {
      const latestInstance = await tx.selectFrom(tableName).selectAll().where("id", "=", formattedId).orderBy("effectiveTo", "desc").executeTakeFirst();
      if (!latestInstance) {
        return await tx.insertInto(tableName).values({
          ...createInstance,
          effectiveFrom: timestamp,
          effectiveTo: MAX_INTEGER
        }).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom === timestamp) {
        return await tx.updateTable(tableName).set(updateInstance).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom > timestamp) {
        throw new Error(`Cannot update an instance in the past`);
      }
      await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER).execute();
      return await tx.insertInto(tableName).values({
        ...latestInstance,
        ...updateInstance,
        effectiveFrom: timestamp,
        effectiveTo: MAX_INTEGER
      }).returningAll().executeTakeFirstOrThrow();
    });
    return this.deserializeInstance({ modelName, instance });
  };
  delete = async ({
    modelName,
    timestamp,
    id
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const instance = await this.db.transaction().execute(async (tx) => {
      const deletedInstance = await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER).returning(["id", "effectiveFrom"]).executeTakeFirst();
      if (deletedInstance?.effectiveFrom === timestamp) {
        await tx.deleteFrom(tableName).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returning(["id"]).executeTakeFirst();
      }
      return !!deletedInstance;
    });
    return instance;
  };
  findMany = async ({
    modelName,
    timestamp = MAX_INTEGER,
    filter = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    let query2 = this.db.selectFrom(tableName).selectAll().where("effectiveFrom", "<=", timestamp).where("effectiveTo", ">=", timestamp);
    const { where, first, skip, orderBy, orderDirection } = parseModelFilter(filter);
    if (where) {
      Object.entries(where).forEach(([whereKey, rawValue]) => {
        const [fieldName, rawFilterType] = whereKey.split(/_(.*)/s);
        const filterType = rawFilterType === void 0 ? "" : rawFilterType;
        const { operator, value } = getWhereOperatorAndValue({
          filterType,
          value: rawValue
        });
        query2 = query2.where(fieldName, operator, value);
      });
    }
    if (skip) {
      query2 = query2.offset(skip);
    }
    if (first) {
      query2 = query2.limit(first);
    }
    if (orderBy) {
      query2 = query2.orderBy(
        orderBy,
        orderDirection === "asc" || orderDirection === void 0 ? import_kysely4.sql`asc nulls first` : import_kysely4.sql`desc nulls last`
      );
    }
    const instances = await query2.execute();
    return instances.map(
      (instance) => this.deserializeInstance({ modelName, instance })
    );
  };
  revert = async ({ safeTimestamp }) => {
    await this.db.transaction().execute(async (tx) => {
      await Promise.all(
        (this.schema?.entities ?? []).map(async (entity) => {
          const modelName = entity.name;
          const tableName = `${modelName}_${this.versionId}`;
          await tx.deleteFrom(tableName).where("effectiveFrom", ">", safeTimestamp).execute();
          await tx.updateTable(tableName).where("effectiveTo", ">=", safeTimestamp).set({ effectiveTo: MAX_INTEGER }).execute();
        })
      );
    });
  };
  deserializeInstance = ({
    modelName,
    instance
  }) => {
    const entity = this.schema.entities.find((e) => e.name === modelName);
    const deserializedInstance = {};
    entity.fields.forEach((field) => {
      const value = instance[field.name];
      if (value === null || value === void 0) {
        deserializedInstance[field.name] = null;
        return;
      }
      if (field.kind === "SCALAR" && field.scalarTypeName === "Boolean") {
        deserializedInstance[field.name] = value === 1 ? true : false;
        return;
      }
      if (field.kind === "SCALAR" && field.scalarTypeName === "BigInt") {
        deserializedInstance[field.name] = blobToBigInt(
          value
        );
        return;
      }
      if (field.kind === "RELATIONSHIP" && field.relatedEntityIdType.name === "BigInt") {
        deserializedInstance[field.name] = blobToBigInt(
          value
        );
        return;
      }
      if (field.kind === "LIST") {
        let parsedValue = JSON.parse(value);
        if (field.baseGqlType.name === "BigInt")
          parsedValue = parsedValue.map(BigInt);
        deserializedInstance[field.name] = parsedValue;
        return;
      }
      deserializedInstance[field.name] = value;
    });
    return deserializedInstance;
  };
};

// src/user-store/sqlite/store.ts
var import_crypto2 = require("crypto");
var import_kysely5 = require("kysely");
var gqlScalarToSqlType2 = {
  Boolean: "integer",
  Int: "integer",
  String: "text",
  BigInt: "blob",
  Bytes: "text",
  Float: "text"
};
var MAX_INTEGER2 = 2147483647;
var SqliteUserStore = class {
  db;
  schema;
  versionId;
  constructor({ db }) {
    this.db = new import_kysely5.Kysely({
      dialect: new import_kysely5.SqliteDialect({ database: db })
    });
  }
  /**
   * Resets the database by dropping existing tables and creating new tables.
   * If no new schema is provided, the existing schema is used.
   *
   * @param options.schema New schema to be used.
   */
  reload = async ({ schema } = {}) => {
    if (!this.schema && !schema)
      return;
    await this.db.transaction().execute(async (tx) => {
      if (this.schema) {
        await Promise.all(
          this.schema.entities.map((model) => {
            const tableName = `${model.name}_${this.versionId}`;
            tx.schema.dropTable(tableName);
          })
        );
      }
      if (schema)
        this.schema = schema;
      this.versionId = (0, import_crypto2.randomBytes)(4).toString("hex");
      await Promise.all(
        this.schema.entities.map(async (model) => {
          const tableName = `${model.name}_${this.versionId}`;
          let tableBuilder = tx.schema.createTable(tableName);
          model.fields.forEach((field) => {
            switch (field.kind) {
              case "SCALAR": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  gqlScalarToSqlType2[field.scalarTypeName],
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
              case "ENUM": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  "text",
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    col = col.check(
                      import_kysely5.sql`${import_kysely5.sql.ref(field.name)} in (${import_kysely5.sql.join(
                        field.enumValues.map((v) => import_kysely5.sql.lit(v))
                      )})`
                    );
                    return col;
                  }
                );
                break;
              }
              case "LIST": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  "text",
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
              case "RELATIONSHIP": {
                tableBuilder = tableBuilder.addColumn(
                  field.name,
                  gqlScalarToSqlType2[field.relatedEntityIdType.name],
                  (col) => {
                    if (field.notNull)
                      col = col.notNull();
                    return col;
                  }
                );
                break;
              }
            }
          });
          tableBuilder = tableBuilder.addColumn(
            "effectiveFrom",
            "integer",
            (col) => col.notNull()
          );
          tableBuilder = tableBuilder.addColumn(
            "effectiveTo",
            "integer",
            (col) => col.notNull()
          );
          tableBuilder = tableBuilder.addPrimaryKeyConstraint(
            `${tableName}_id_effectiveTo_unique`,
            // eslint-disable-next-line @typescript-eslint/ban-ts-comment
            // @ts-ignore
            ["id", "effectiveTo"]
          );
          await tableBuilder.execute();
        })
      );
    });
  };
  /**
   * Tears down the store by dropping all tables for the current schema.
   */
  teardown = async () => {
    if (!this.schema)
      return;
    await this.db.transaction().execute(async (tx) => {
      await Promise.all(
        this.schema.entities.map((model) => {
          const tableName = `${model.name}_${this.versionId}`;
          tx.schema.dropTable(tableName);
        })
      );
    });
  };
  findUnique = async ({
    modelName,
    timestamp = MAX_INTEGER2,
    id
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const instances = await this.db.selectFrom(tableName).selectAll().where("id", "=", formattedId).where("effectiveFrom", "<=", timestamp).where("effectiveTo", ">=", timestamp).execute();
    if (instances.length > 1) {
      throw new Error(`Expected 1 instance, found ${instances.length}`);
    }
    return instances[0] ? this.deserializeInstance({ modelName, instance: instances[0] }) : null;
  };
  create = async ({
    modelName,
    timestamp,
    id,
    data = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const createInstance = formatModelInstance({ id, data });
    const instance = await this.db.insertInto(tableName).values({
      ...createInstance,
      effectiveFrom: timestamp,
      effectiveTo: MAX_INTEGER2
    }).returningAll().executeTakeFirstOrThrow();
    return this.deserializeInstance({ modelName, instance });
  };
  update = async ({
    modelName,
    timestamp,
    id,
    data = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const updateInstance = formatModelInstance({ id, data });
    const instance = await this.db.transaction().execute(async (tx) => {
      const latestInstance = await tx.selectFrom(tableName).selectAll().where("id", "=", formattedId).orderBy("effectiveTo", "desc").executeTakeFirstOrThrow();
      if (latestInstance.effectiveFrom === timestamp) {
        return await tx.updateTable(tableName).set(updateInstance).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom > timestamp) {
        throw new Error(`Cannot update an instance in the past`);
      }
      await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER2).execute();
      return await tx.insertInto(tableName).values({
        ...latestInstance,
        ...updateInstance,
        effectiveFrom: timestamp,
        effectiveTo: MAX_INTEGER2
      }).returningAll().executeTakeFirstOrThrow();
    });
    return this.deserializeInstance({ modelName, instance });
  };
  upsert = async ({
    modelName,
    timestamp,
    id,
    create = {},
    update = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const createInstance = formatModelInstance({ id, data: create });
    const updateInstance = formatModelInstance({ id, data: update });
    const instance = await this.db.transaction().execute(async (tx) => {
      const latestInstance = await tx.selectFrom(tableName).selectAll().where("id", "=", formattedId).orderBy("effectiveTo", "desc").executeTakeFirst();
      if (!latestInstance) {
        return await tx.insertInto(tableName).values({
          ...createInstance,
          effectiveFrom: timestamp,
          effectiveTo: MAX_INTEGER2
        }).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom === timestamp) {
        return await tx.updateTable(tableName).set(updateInstance).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returningAll().executeTakeFirstOrThrow();
      }
      if (latestInstance.effectiveFrom > timestamp) {
        throw new Error(`Cannot update an instance in the past`);
      }
      await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER2).execute();
      return await tx.insertInto(tableName).values({
        ...latestInstance,
        ...updateInstance,
        effectiveFrom: timestamp,
        effectiveTo: MAX_INTEGER2
      }).returningAll().executeTakeFirstOrThrow();
    });
    return this.deserializeInstance({ modelName, instance });
  };
  delete = async ({
    modelName,
    timestamp,
    id
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    const formattedId = formatModelFieldValue({ value: id });
    const instance = await this.db.transaction().execute(async (tx) => {
      const deletedInstance = await tx.updateTable(tableName).set({ effectiveTo: timestamp - 1 }).where("id", "=", formattedId).where("effectiveTo", "=", MAX_INTEGER2).returning(["id", "effectiveFrom"]).executeTakeFirst();
      if (deletedInstance?.effectiveFrom === timestamp) {
        await tx.deleteFrom(tableName).where("id", "=", formattedId).where("effectiveFrom", "=", timestamp).returning(["id"]).executeTakeFirst();
      }
      return !!deletedInstance;
    });
    return instance;
  };
  findMany = async ({
    modelName,
    timestamp = MAX_INTEGER2,
    filter = {}
  }) => {
    const tableName = `${modelName}_${this.versionId}`;
    let query2 = this.db.selectFrom(tableName).selectAll().where("effectiveFrom", "<=", timestamp).where("effectiveTo", ">=", timestamp);
    const { where, first, skip, orderBy, orderDirection } = parseModelFilter(filter);
    if (where) {
      Object.entries(where).forEach(([whereKey, rawValue]) => {
        const [fieldName, rawFilterType] = whereKey.split(/_(.*)/s);
        const filterType = rawFilterType === void 0 ? "" : rawFilterType;
        const { operator, value } = getWhereOperatorAndValue({
          filterType,
          value: rawValue
        });
        query2 = query2.where(fieldName, operator, value);
      });
    }
    if (skip) {
      query2 = query2.offset(skip);
    }
    if (first) {
      query2 = query2.limit(first);
    }
    if (orderBy) {
      query2 = query2.orderBy(orderBy, orderDirection);
    }
    const instances = await query2.execute();
    return instances.map(
      (instance) => this.deserializeInstance({ modelName, instance })
    );
  };
  revert = async ({ safeTimestamp }) => {
    await this.db.transaction().execute(async (tx) => {
      await Promise.all(
        (this.schema?.entities ?? []).map(async (entity) => {
          const modelName = entity.name;
          const tableName = `${modelName}_${this.versionId}`;
          await tx.deleteFrom(tableName).where("effectiveFrom", ">", safeTimestamp).execute();
          await tx.updateTable(tableName).where("effectiveTo", ">=", safeTimestamp).set({ effectiveTo: MAX_INTEGER2 }).execute();
        })
      );
    });
  };
  deserializeInstance = ({
    modelName,
    instance
  }) => {
    const entity = this.schema.entities.find((e) => e.name === modelName);
    const deserializedInstance = {};
    entity.fields.forEach((field) => {
      const value = instance[field.name];
      if (value === null || value === void 0) {
        deserializedInstance[field.name] = null;
        return;
      }
      if (field.kind === "SCALAR" && field.scalarTypeName === "Boolean") {
        deserializedInstance[field.name] = value === 1 ? true : false;
        return;
      }
      if (field.kind === "SCALAR" && field.scalarTypeName === "BigInt") {
        deserializedInstance[field.name] = blobToBigInt(
          value
        );
        return;
      }
      if (field.kind === "RELATIONSHIP" && field.relatedEntityIdType.name === "BigInt") {
        deserializedInstance[field.name] = blobToBigInt(
          value
        );
        return;
      }
      if (field.kind === "LIST") {
        let parsedValue = JSON.parse(value);
        if (field.baseGqlType.name === "BigInt")
          parsedValue = parsedValue.map(BigInt);
        deserializedInstance[field.name] = parsedValue;
        return;
      }
      deserializedInstance[field.name] = value;
    });
    return deserializedInstance;
  };
};

// src/Ponder.ts
var Ponder = class {
  resources;
  logFilters;
  eventStore;
  userStore;
  // List of indexing-related services. One per configured network.
  networkSyncServices = [];
  eventAggregatorService;
  eventHandlerService;
  serverService;
  buildService;
  codegenService;
  uiService;
  constructor({
    options,
    config,
    eventStore,
    userStore
  }) {
    const logger = new LoggerService({
      level: options.logLevel,
      dir: options.logDir
    });
    const errors = new UserErrorService();
    const metrics = new MetricsService();
    const resources = { options, logger, errors, metrics };
    this.resources = resources;
    const logFilters = buildLogFilters({ options, config });
    this.logFilters = logFilters;
    const contracts = buildContracts({ options, config });
    const networks = config.networks.map(
      (network) => buildNetwork({ network })
    );
    const database = buildDatabase({ options, config });
    this.eventStore = eventStore ?? (database.kind === "sqlite" ? new SqliteEventStore({ db: database.db }) : new PostgresEventStore({ pool: database.pool }));
    this.userStore = userStore ?? (database.kind === "sqlite" ? new SqliteUserStore({ db: database.db }) : new PostgresUserStore({ pool: database.pool }));
    networks.forEach((network) => {
      const logFiltersForNetwork = logFilters.filter(
        (logFilter) => logFilter.network === network.name
      );
      this.networkSyncServices.push({
        network,
        logFilters: logFiltersForNetwork,
        historicalSyncService: new HistoricalSyncService({
          resources,
          eventStore: this.eventStore,
          network,
          logFilters: logFiltersForNetwork
        }),
        realtimeSyncService: new RealtimeSyncService({
          resources,
          eventStore: this.eventStore,
          network,
          logFilters: logFiltersForNetwork
        })
      });
    });
    this.eventAggregatorService = new EventAggregatorService({
      eventStore: this.eventStore,
      networks,
      logFilters
    });
    this.eventHandlerService = new EventHandlerService({
      resources,
      eventStore: this.eventStore,
      userStore: this.userStore,
      eventAggregatorService: this.eventAggregatorService,
      contracts,
      logFilters: this.logFilters
    });
    this.serverService = new ServerService({
      resources,
      userStore: this.userStore
    });
    this.buildService = new BuildService({ resources });
    this.codegenService = new CodegenService({
      resources,
      contracts,
      logFilters
    });
    this.uiService = new UiService({ resources, logFilters });
  }
  async setup() {
    this.resources.logger.debug({
      service: "app",
      msg: `Started using config file: ${import_node_path9.default.relative(
        this.resources.options.rootDir,
        this.resources.options.configFile
      )}`
    });
    this.registerServiceDependencies();
    const networksMissingRpcUrl = [];
    this.networkSyncServices.forEach(({ network }) => {
      if (!network.rpcUrl) {
        networksMissingRpcUrl.push(network);
      }
    });
    if (networksMissingRpcUrl.length > 0) {
      return new Error(
        `missing RPC URL for networks (${networksMissingRpcUrl.map(
          (n) => `"${n.name}"`
        )}). Did you forget to add an RPC URL in .env.local?`
      );
    }
    await this.serverService.start();
    this.codegenService.generateAppFile();
    await this.eventStore.migrateUp();
    this.buildService.buildSchema();
    await this.buildService.buildHandlers();
  }
  async dev() {
    const setupError = await this.setup();
    if (setupError) {
      this.resources.logger.error({
        service: "app",
        msg: setupError.message,
        error: setupError
      });
      return await this.kill();
    }
    await Promise.all(
      this.networkSyncServices.map(
        async ({ historicalSyncService, realtimeSyncService }) => {
          const { finalizedBlockNumber } = await realtimeSyncService.setup();
          await historicalSyncService.setup({ finalizedBlockNumber });
          historicalSyncService.start();
          realtimeSyncService.start();
        }
      )
    );
    this.buildService.watch();
  }
  async start() {
    const setupError = await this.setup();
    if (setupError) {
      return await this.kill();
    }
    await Promise.all(
      this.networkSyncServices.map(
        async ({ historicalSyncService, realtimeSyncService }) => {
          const { finalizedBlockNumber } = await realtimeSyncService.setup();
          await historicalSyncService.setup({ finalizedBlockNumber });
          historicalSyncService.start();
          realtimeSyncService.start();
        }
      )
    );
  }
  async codegen() {
    this.codegenService.generateAppFile();
    const result = this.buildService.buildSchema();
    if (result) {
      const { schema, graphqlSchema } = result;
      this.codegenService.generateAppFile({ schema });
      this.codegenService.generateSchemaFile({ graphqlSchema });
    }
    await this.kill();
  }
  async kill() {
    this.eventAggregatorService.clearListeners();
    await Promise.all(
      this.networkSyncServices.map(
        async ({ realtimeSyncService, historicalSyncService }) => {
          await realtimeSyncService.kill();
          await historicalSyncService.kill();
        }
      )
    );
    await this.buildService.kill?.();
    this.uiService.kill();
    this.eventHandlerService.kill();
    await this.serverService.kill();
    await this.userStore.teardown();
    this.resources.logger.debug({
      service: "app",
      msg: `Finished shutdown sequence`
    });
  }
  registerServiceDependencies() {
    this.buildService.on("newConfig", async () => {
      this.resources.logger.fatal({
        service: "build",
        msg: "Detected change in ponder.config.ts"
      });
      await this.kill();
    });
    this.buildService.on("newSchema", async ({ schema, graphqlSchema }) => {
      this.codegenService.generateAppFile({ schema });
      this.codegenService.generateSchemaFile({ graphqlSchema });
      this.serverService.reload({ graphqlSchema });
      await this.eventHandlerService.reset({ schema });
      await this.eventHandlerService.processEvents();
    });
    this.buildService.on("newHandlers", async ({ handlers }) => {
      await this.eventHandlerService.reset({ handlers });
      await this.eventHandlerService.processEvents();
    });
    this.networkSyncServices.forEach((networkSyncService) => {
      const { chainId } = networkSyncService.network;
      const { historicalSyncService, realtimeSyncService } = networkSyncService;
      historicalSyncService.on("historicalCheckpoint", ({ timestamp }) => {
        this.eventAggregatorService.handleNewHistoricalCheckpoint({
          chainId,
          timestamp
        });
      });
      historicalSyncService.on("syncComplete", () => {
        this.eventAggregatorService.handleHistoricalSyncComplete({
          chainId
        });
      });
      realtimeSyncService.on("realtimeCheckpoint", ({ timestamp }) => {
        this.eventAggregatorService.handleNewRealtimeCheckpoint({
          chainId,
          timestamp
        });
      });
      realtimeSyncService.on("finalityCheckpoint", ({ timestamp }) => {
        this.eventAggregatorService.handleNewFinalityCheckpoint({
          chainId,
          timestamp
        });
      });
      realtimeSyncService.on("shallowReorg", ({ commonAncestorTimestamp }) => {
        this.eventAggregatorService.handleReorg({ commonAncestorTimestamp });
      });
    });
    this.eventAggregatorService.on("newCheckpoint", async () => {
      await this.eventHandlerService.processEvents();
    });
    this.eventAggregatorService.on(
      "reorg",
      async ({ commonAncestorTimestamp }) => {
        await this.eventHandlerService.handleReorg({ commonAncestorTimestamp });
        await this.eventHandlerService.processEvents();
      }
    );
    this.eventHandlerService.on("eventsProcessed", ({ toTimestamp }) => {
      if (this.serverService.isHistoricalEventProcessingComplete)
        return;
      if (this.eventAggregatorService.historicalSyncCompletedAt && toTimestamp >= this.eventAggregatorService.historicalSyncCompletedAt) {
        this.serverService.setIsHistoricalEventProcessingComplete();
      }
    });
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  Ponder,
  PonderApp
});
//# sourceMappingURL=index.js.map